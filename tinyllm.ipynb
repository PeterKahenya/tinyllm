{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tinyllm\n",
    "import dataclasses\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset_path = \"datasets/wikipedia\"\n",
    "model_path = \"models/tinyllm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(dataset_path, exist_ok=True)\n",
    "# dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "# dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775c9fb15c9344feb6e396b136ea5332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 6,407,814\n",
      "Doc Keys: dict_keys(['id', 'url', 'title', 'text'])\n",
      "Doc Text: Anarchism is a political philosophy and movement that is skeptical of all justifications for authori\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(dataset_path)[\"train\"]\n",
    "print(f\"Length of dataset: {len(dataset):,}\")\n",
    "print(f\"Doc Keys: {dataset[0].keys()}\")\n",
    "print(f\"Doc Text: {dataset[0]['text'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a bpe tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 20_000\n",
    "# word_counts = Counter()\n",
    "# for i in tqdm(range(0, len(dataset), chunk_size)):\n",
    "#     chunk = dataset[i:i + chunk_size]\n",
    "#     word_counts += tinyllm.count_words_in_documents(chunk[\"text\"], processes=32)\n",
    "# with open(f\"{dataset_path}/word_counts.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(word_counts, f)\n",
    "# word_counts_str = {bytes(word).decode(\"utf-8\"): count for word, count in word_counts.items()}\n",
    "# with open(f\"{dataset_path}/word_counts.json\", \"w\") as f:\n",
    "#     json.dump(word_counts_str, f)\n",
    "# with open(f\"{dataset_path}/word_counts.pkl\", \"rb\") as f:\n",
    "#     word_counts = pickle.load(f)\n",
    "# with open(f\"{dataset_path}/word_counts.json\", \"r\") as f:\n",
    "#     word_counts_str = json.load(f)\n",
    "# vocab_size = 2**8 + 500 # 756 is the default vocab size in tinyllm\n",
    "# text_encoder = tinyllm.TextEncoder()\n",
    "# text_encoder.train(word_counts, vocab_size=vocab_size, processes=32)\n",
    "# text_encoder.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and validate the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tinyllm.TextEncoder.load(model_path)\n",
    "assert tokenizer.decode(tokenizer.encode(\"Hello universe, there is a lot to learn from you\")) == \"Hello universe, there is a lot to learn from you\", \"Tokenizer is not working correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset in shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf {dataset_path}/shards\n",
    "# shard_size = 50_000_000\n",
    "# nprocs = 32\n",
    "# total_tokens = tinyllm.tokenize_and_write_shards(\n",
    "#     ds=dataset,\n",
    "#     shard_size=shard_size,\n",
    "#     nprocs=nprocs,\n",
    "#     shards_dir=f\"{dataset_path}/shards\",\n",
    "#     model_path=model_path,\n",
    "# )\n",
    "# total_train_tokens = total_tokens - shard_size # first shard is for validation\n",
    "# print(f\"Total tokens: {total_train_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: {'context_length': 512, 'vocab_size': 757, 'num_blocks': 12, 'num_heads': 12, 'd_model': 768, 'head_dim': 64, 'dropout_rate': 0.1, 'num_of_hidden_units': 3072, 'device': 'cuda'}\n",
      "Number of parameters: 86,031,349\n",
      "found 5 shards for split train\n",
      "[GPU0] | Batchsize: 8 | Steps: 55,081\n",
      "Step 8/55081 | Loss: 6.813 | Norm: 0.222 | LR: 3.3333e-05 | Tokens/s: 125062\n",
      "Step 16/55081 | Loss: 6.643 | Norm: 0.305 | LR: 6.6667e-05 | Tokens/s: 150173\n",
      "Step 24/55081 | Loss: 6.329 | Norm: 0.307 | LR: 1.0000e-04 | Tokens/s: 150490\n",
      "Step 32/55081 | Loss: 6.130 | Norm: 0.219 | LR: 1.3333e-04 | Tokens/s: 151079\n",
      "Step 40/55081 | Loss: 5.904 | Norm: 0.402 | LR: 1.6667e-04 | Tokens/s: 148949\n",
      "Step 48/55081 | Loss: 5.827 | Norm: 0.280 | LR: 2.0000e-04 | Tokens/s: 149014\n",
      "Step 56/55081 | Loss: 5.687 | Norm: 0.234 | LR: 2.3333e-04 | Tokens/s: 149323\n",
      "Step 64/55081 | Loss: 5.579 | Norm: 0.156 | LR: 2.6667e-04 | Tokens/s: 149059\n",
      "Step 72/55081 | Loss: 5.547 | Norm: 0.204 | LR: 3.0000e-04 | Tokens/s: 150076\n",
      "Step 80/55081 | Loss: 5.501 | Norm: 0.159 | LR: 3.3333e-04 | Tokens/s: 149314\n",
      "Step 88/55081 | Loss: 5.413 | Norm: 0.192 | LR: 3.6667e-04 | Tokens/s: 150561\n",
      "Step 96/55081 | Loss: 5.337 | Norm: 0.144 | LR: 4.0000e-04 | Tokens/s: 149132\n",
      "Step 104/55081 | Loss: 5.284 | Norm: 0.123 | LR: 4.3333e-04 | Tokens/s: 149377\n",
      "Step 112/55081 | Loss: 5.399 | Norm: 0.210 | LR: 4.6667e-04 | Tokens/s: 149841\n",
      "Step 120/55081 | Loss: 5.288 | Norm: 0.104 | LR: 5.0000e-04 | Tokens/s: 149670\n",
      "Step 128/55081 | Loss: 5.275 | Norm: 0.111 | LR: 5.3333e-04 | Tokens/s: 149461\n",
      "Step 136/55081 | Loss: 5.231 | Norm: 0.201 | LR: 5.6667e-04 | Tokens/s: 149109\n",
      "Step 144/55081 | Loss: 5.129 | Norm: 0.123 | LR: 6.0000e-04 | Tokens/s: 149508\n",
      "Step 152/55081 | Loss: 5.149 | Norm: 0.128 | LR: 6.3333e-04 | Tokens/s: 148448\n",
      "Step 160/55081 | Loss: 5.171 | Norm: 0.129 | LR: 6.6667e-04 | Tokens/s: 150167\n",
      "Step 168/55081 | Loss: 5.074 | Norm: 0.107 | LR: 7.0000e-04 | Tokens/s: 149193\n",
      "Step 176/55081 | Loss: 5.633 | Norm: 0.209 | LR: 7.3333e-04 | Tokens/s: 150353\n",
      "Step 184/55081 | Loss: 5.538 | Norm: 0.190 | LR: 7.6667e-04 | Tokens/s: 149482\n",
      "Step 192/55081 | Loss: 5.249 | Norm: 0.186 | LR: 8.0000e-04 | Tokens/s: 149054\n",
      "Step 200/55081 | Loss: 5.236 | Norm: 0.187 | LR: 8.3333e-04 | Tokens/s: 148860\n",
      "Step 208/55081 | Loss: 5.284 | Norm: 0.165 | LR: 8.6667e-04 | Tokens/s: 148975\n",
      "Step 216/55081 | Loss: 5.180 | Norm: 0.129 | LR: 9.0000e-04 | Tokens/s: 149501\n",
      "Step 224/55081 | Loss: 5.126 | Norm: 0.125 | LR: 9.3333e-04 | Tokens/s: 149258\n",
      "Step 232/55081 | Loss: 5.335 | Norm: 0.120 | LR: 9.6667e-04 | Tokens/s: 149791\n",
      "Step 240/55081 | Loss: 5.301 | Norm: 0.104 | LR: 1.0000e-03 | Tokens/s: 149703\n",
      "Step 248/55081 | Loss: 5.209 | Norm: 0.110 | LR: 1.0000e-03 | Tokens/s: 149498\n",
      "Step 256/55081 | Loss: 5.098 | Norm: 0.129 | LR: 1.0000e-03 | Tokens/s: 149249\n",
      "Step 264/55081 | Loss: 5.256 | Norm: 0.114 | LR: 9.9999e-04 | Tokens/s: 150566\n",
      "Step 272/55081 | Loss: 5.279 | Norm: 0.122 | LR: 9.9998e-04 | Tokens/s: 149790\n",
      "Step 280/55081 | Loss: 5.065 | Norm: 0.107 | LR: 9.9996e-04 | Tokens/s: 149633\n",
      "Step 288/55081 | Loss: 5.109 | Norm: 0.097 | LR: 9.9994e-04 | Tokens/s: 149015\n",
      "Step 296/55081 | Loss: 5.126 | Norm: 0.104 | LR: 9.9991e-04 | Tokens/s: 149213\n",
      "Step 304/55081 | Loss: 5.149 | Norm: 0.126 | LR: 9.9988e-04 | Tokens/s: 149366\n",
      "Step 312/55081 | Loss: 5.142 | Norm: 0.092 | LR: 9.9984e-04 | Tokens/s: 149534\n",
      "Step 320/55081 | Loss: 5.067 | Norm: 0.106 | LR: 9.9980e-04 | Tokens/s: 149195\n",
      "Step 328/55081 | Loss: 5.123 | Norm: 0.128 | LR: 9.9975e-04 | Tokens/s: 148980\n",
      "Step 336/55081 | Loss: 5.270 | Norm: 0.094 | LR: 9.9970e-04 | Tokens/s: 149018\n",
      "Step 344/55081 | Loss: 5.108 | Norm: 0.118 | LR: 9.9964e-04 | Tokens/s: 149196\n",
      "Step 352/55081 | Loss: 5.125 | Norm: 0.097 | LR: 9.9958e-04 | Tokens/s: 149046\n",
      "Step 360/55081 | Loss: 5.063 | Norm: 0.116 | LR: 9.9951e-04 | Tokens/s: 148943\n",
      "Step 368/55081 | Loss: 5.128 | Norm: 0.097 | LR: 9.9944e-04 | Tokens/s: 149946\n",
      "Step 376/55081 | Loss: 5.070 | Norm: 0.120 | LR: 9.9936e-04 | Tokens/s: 148976\n",
      "Step 384/55081 | Loss: 5.137 | Norm: 0.119 | LR: 9.9928e-04 | Tokens/s: 150065\n",
      "Step 392/55081 | Loss: 5.156 | Norm: 0.103 | LR: 9.9919e-04 | Tokens/s: 149018\n",
      "Step 400/55081 | Loss: 5.219 | Norm: 0.125 | LR: 9.9910e-04 | Tokens/s: 148735\n",
      "Step 408/55081 | Loss: 5.099 | Norm: 0.092 | LR: 9.9900e-04 | Tokens/s: 148900\n",
      "Step 416/55081 | Loss: 5.195 | Norm: 0.119 | LR: 9.9890e-04 | Tokens/s: 148430\n",
      "Step 424/55081 | Loss: 5.132 | Norm: 0.091 | LR: 9.9879e-04 | Tokens/s: 149233\n",
      "Step 432/55081 | Loss: 5.075 | Norm: 0.120 | LR: 9.9868e-04 | Tokens/s: 148876\n",
      "Step 440/55081 | Loss: 5.143 | Norm: 0.095 | LR: 9.9857e-04 | Tokens/s: 149859\n",
      "Step 448/55081 | Loss: 5.175 | Norm: 0.087 | LR: 9.9844e-04 | Tokens/s: 149219\n",
      "Step 456/55081 | Loss: 5.700 | Norm: 0.204 | LR: 9.9832e-04 | Tokens/s: 150453\n",
      "Step 464/55081 | Loss: 5.280 | Norm: 0.089 | LR: 9.9819e-04 | Tokens/s: 149299\n",
      "Step 472/55081 | Loss: 5.107 | Norm: 0.133 | LR: 9.9805e-04 | Tokens/s: 148956\n",
      "Step 480/55081 | Loss: 5.021 | Norm: 0.107 | LR: 9.9791e-04 | Tokens/s: 149225\n",
      "Step 488/55081 | Loss: 5.022 | Norm: 0.108 | LR: 9.9776e-04 | Tokens/s: 149870\n",
      "Step 496/55081 | Loss: 5.010 | Norm: 0.104 | LR: 9.9761e-04 | Tokens/s: 149142\n",
      "Epoch 500 | Training checkpoint saved at models/tinyllm/checkpoint_500.pt\n",
      "Step 504/55081 | Loss: 5.080 | Norm: 0.124 | LR: 9.9745e-04 | Tokens/s: 149518\n",
      "Step 512/55081 | Loss: 5.087 | Norm: 0.180 | LR: 9.9729e-04 | Tokens/s: 148902\n",
      "Step 520/55081 | Loss: 5.056 | Norm: 0.112 | LR: 9.9712e-04 | Tokens/s: 149141\n",
      "Step 528/55081 | Loss: 5.083 | Norm: 0.089 | LR: 9.9695e-04 | Tokens/s: 149092\n",
      "Step 536/55081 | Loss: 5.064 | Norm: 0.147 | LR: 9.9677e-04 | Tokens/s: 149729\n",
      "Step 544/55081 | Loss: 5.055 | Norm: 0.162 | LR: 9.9659e-04 | Tokens/s: 148210\n",
      "Step 552/55081 | Loss: 5.073 | Norm: 0.108 | LR: 9.9641e-04 | Tokens/s: 150160\n",
      "Step 560/55081 | Loss: 5.069 | Norm: 0.073 | LR: 9.9622e-04 | Tokens/s: 149215\n",
      "Step 568/55081 | Loss: 5.100 | Norm: 0.084 | LR: 9.9602e-04 | Tokens/s: 148895\n",
      "Step 576/55081 | Loss: 5.068 | Norm: 0.108 | LR: 9.9582e-04 | Tokens/s: 149393\n",
      "Step 584/55081 | Loss: 5.157 | Norm: 0.107 | LR: 9.9561e-04 | Tokens/s: 149320\n",
      "Step 592/55081 | Loss: 5.080 | Norm: 0.096 | LR: 9.9540e-04 | Tokens/s: 149167\n",
      "Step 600/55081 | Loss: 5.133 | Norm: 0.092 | LR: 9.9519e-04 | Tokens/s: 148388\n",
      "Step 608/55081 | Loss: 5.077 | Norm: 0.079 | LR: 9.9496e-04 | Tokens/s: 149021\n",
      "Step 616/55081 | Loss: 5.157 | Norm: 0.078 | LR: 9.9474e-04 | Tokens/s: 149046\n",
      "Step 624/55081 | Loss: 5.398 | Norm: 0.295 | LR: 9.9451e-04 | Tokens/s: 149452\n",
      "Step 632/55081 | Loss: 5.140 | Norm: 0.115 | LR: 9.9427e-04 | Tokens/s: 149621\n",
      "Step 640/55081 | Loss: 5.077 | Norm: 0.198 | LR: 9.9403e-04 | Tokens/s: 150033\n",
      "Step 648/55081 | Loss: 5.331 | Norm: 0.124 | LR: 9.9379e-04 | Tokens/s: 149222\n",
      "Step 656/55081 | Loss: 5.335 | Norm: 0.112 | LR: 9.9354e-04 | Tokens/s: 148956\n",
      "Step 664/55081 | Loss: 5.162 | Norm: 0.090 | LR: 9.9328e-04 | Tokens/s: 149021\n",
      "Step 672/55081 | Loss: 5.039 | Norm: 0.098 | LR: 9.9302e-04 | Tokens/s: 149445\n",
      "Step 680/55081 | Loss: 5.026 | Norm: 0.105 | LR: 9.9275e-04 | Tokens/s: 148776\n",
      "Step 688/55081 | Loss: 4.939 | Norm: 0.122 | LR: 9.9248e-04 | Tokens/s: 150252\n",
      "Step 696/55081 | Loss: 5.017 | Norm: 0.088 | LR: 9.9221e-04 | Tokens/s: 149040\n",
      "Step 704/55081 | Loss: 5.098 | Norm: 0.100 | LR: 9.9193e-04 | Tokens/s: 149109\n",
      "Step 712/55081 | Loss: 5.053 | Norm: 0.103 | LR: 9.9164e-04 | Tokens/s: 149067\n",
      "Step 720/55081 | Loss: 5.037 | Norm: 0.115 | LR: 9.9135e-04 | Tokens/s: 149181\n",
      "Step 728/55081 | Loss: 5.025 | Norm: 0.082 | LR: 9.9106e-04 | Tokens/s: 149427\n",
      "Step 736/55081 | Loss: 5.073 | Norm: 0.096 | LR: 9.9076e-04 | Tokens/s: 148474\n",
      "Step 744/55081 | Loss: 5.058 | Norm: 0.086 | LR: 9.9046e-04 | Tokens/s: 149377\n",
      "Step 752/55081 | Loss: 5.035 | Norm: 0.154 | LR: 9.9015e-04 | Tokens/s: 148933\n",
      "Step 760/55081 | Loss: 4.964 | Norm: 0.120 | LR: 9.8983e-04 | Tokens/s: 149774\n",
      "Step 768/55081 | Loss: 4.937 | Norm: 0.121 | LR: 9.8951e-04 | Tokens/s: 148903\n",
      "Step 776/55081 | Loss: 5.029 | Norm: 0.116 | LR: 9.8919e-04 | Tokens/s: 149098\n",
      "Step 784/55081 | Loss: 5.092 | Norm: 0.127 | LR: 9.8886e-04 | Tokens/s: 149241\n",
      "Step 792/55081 | Loss: 5.061 | Norm: 0.142 | LR: 9.8853e-04 | Tokens/s: 148980\n",
      "Step 800/55081 | Loss: 5.035 | Norm: 0.113 | LR: 9.8819e-04 | Tokens/s: 149225\n",
      "Step 808/55081 | Loss: 5.006 | Norm: 0.095 | LR: 9.8785e-04 | Tokens/s: 149334\n",
      "Step 816/55081 | Loss: 5.035 | Norm: 0.080 | LR: 9.8750e-04 | Tokens/s: 149007\n",
      "Step 824/55081 | Loss: 5.141 | Norm: 0.099 | LR: 9.8714e-04 | Tokens/s: 148730\n",
      "Step 832/55081 | Loss: 5.054 | Norm: 0.087 | LR: 9.8679e-04 | Tokens/s: 149142\n",
      "Step 840/55081 | Loss: 5.047 | Norm: 0.103 | LR: 9.8642e-04 | Tokens/s: 148849\n",
      "Step 848/55081 | Loss: 5.068 | Norm: 0.149 | LR: 9.8606e-04 | Tokens/s: 148785\n",
      "Step 856/55081 | Loss: 5.070 | Norm: 0.109 | LR: 9.8568e-04 | Tokens/s: 149111\n",
      "Step 864/55081 | Loss: 5.150 | Norm: 0.179 | LR: 9.8531e-04 | Tokens/s: 150878\n",
      "Step 872/55081 | Loss: 5.210 | Norm: 0.139 | LR: 9.8492e-04 | Tokens/s: 149348\n",
      "Step 880/55081 | Loss: 4.928 | Norm: 0.127 | LR: 9.8454e-04 | Tokens/s: 150097\n",
      "Step 888/55081 | Loss: 5.107 | Norm: 0.107 | LR: 9.8414e-04 | Tokens/s: 149059\n",
      "Step 896/55081 | Loss: 5.243 | Norm: 0.093 | LR: 9.8375e-04 | Tokens/s: 149430\n",
      "Step 904/55081 | Loss: 5.235 | Norm: 0.102 | LR: 9.8335e-04 | Tokens/s: 62711\n",
      "Step 912/55081 | Loss: 5.067 | Norm: 0.093 | LR: 9.8294e-04 | Tokens/s: 150437\n",
      "Step 920/55081 | Loss: 5.227 | Norm: 0.116 | LR: 9.8253e-04 | Tokens/s: 149381\n",
      "Step 928/55081 | Loss: 5.349 | Norm: 0.152 | LR: 9.8211e-04 | Tokens/s: 149994\n",
      "Step 936/55081 | Loss: 5.301 | Norm: 0.131 | LR: 9.8169e-04 | Tokens/s: 148923\n",
      "Step 944/55081 | Loss: 5.178 | Norm: 0.092 | LR: 9.8127e-04 | Tokens/s: 149873\n",
      "Step 952/55081 | Loss: 5.092 | Norm: 0.162 | LR: 9.8084e-04 | Tokens/s: 149458\n",
      "Step 960/55081 | Loss: 5.301 | Norm: 0.107 | LR: 9.8040e-04 | Tokens/s: 149204\n",
      "Step 968/55081 | Loss: 5.190 | Norm: 0.116 | LR: 9.7996e-04 | Tokens/s: 149501\n",
      "Step 976/55081 | Loss: 5.069 | Norm: 0.184 | LR: 9.7952e-04 | Tokens/s: 149113\n",
      "Step 984/55081 | Loss: 5.142 | Norm: 0.147 | LR: 9.7907e-04 | Tokens/s: 149339\n",
      "Step 992/55081 | Loss: 5.071 | Norm: 0.098 | LR: 9.7862e-04 | Tokens/s: 149046\n",
      "Step 1000/55081 | Loss: 5.018 | Norm: 0.084 | LR: 9.7816e-04 | Tokens/s: 149602\n",
      "Epoch 1000 | Training checkpoint saved at models/tinyllm/checkpoint_1000.pt\n",
      "Step 1008/55081 | Loss: 5.128 | Norm: 0.110 | LR: 9.7769e-04 | Tokens/s: 149484\n",
      "Step 1016/55081 | Loss: 5.188 | Norm: 0.166 | LR: 9.7722e-04 | Tokens/s: 149101\n",
      "Step 1024/55081 | Loss: 5.263 | Norm: 0.183 | LR: 9.7675e-04 | Tokens/s: 149414\n",
      "Step 1032/55081 | Loss: 5.093 | Norm: 0.121 | LR: 9.7627e-04 | Tokens/s: 149567\n",
      "Step 1040/55081 | Loss: 5.217 | Norm: 0.183 | LR: 9.7579e-04 | Tokens/s: 149403\n",
      "Step 1048/55081 | Loss: 5.175 | Norm: 0.089 | LR: 9.7530e-04 | Tokens/s: 149874\n",
      "Step 1056/55081 | Loss: 5.194 | Norm: 0.102 | LR: 9.7481e-04 | Tokens/s: 149011\n",
      "Step 1064/55081 | Loss: 5.135 | Norm: 0.111 | LR: 9.7432e-04 | Tokens/s: 150024\n",
      "Step 1072/55081 | Loss: 5.025 | Norm: 0.178 | LR: 9.7381e-04 | Tokens/s: 149264\n",
      "Step 1080/55081 | Loss: 5.138 | Norm: 0.136 | LR: 9.7331e-04 | Tokens/s: 149964\n",
      "Step 1088/55081 | Loss: 5.149 | Norm: 0.129 | LR: 9.7280e-04 | Tokens/s: 149073\n",
      "Step 1096/55081 | Loss: 5.053 | Norm: 0.091 | LR: 9.7228e-04 | Tokens/s: 148766\n",
      "Step 1104/55081 | Loss: 5.083 | Norm: 0.079 | LR: 9.7176e-04 | Tokens/s: 148907\n",
      "Step 1112/55081 | Loss: 5.245 | Norm: 0.284 | LR: 9.7124e-04 | Tokens/s: 149146\n",
      "Step 1120/55081 | Loss: 5.133 | Norm: 0.101 | LR: 9.7071e-04 | Tokens/s: 149069\n",
      "Step 1128/55081 | Loss: 5.112 | Norm: 0.068 | LR: 9.7017e-04 | Tokens/s: 149614\n",
      "Step 1136/55081 | Loss: 5.064 | Norm: 0.082 | LR: 9.6963e-04 | Tokens/s: 148856\n",
      "Step 1144/55081 | Loss: 5.161 | Norm: 0.077 | LR: 9.6909e-04 | Tokens/s: 149126\n",
      "Step 1152/55081 | Loss: 5.085 | Norm: 0.085 | LR: 9.6854e-04 | Tokens/s: 149161\n",
      "Step 1160/55081 | Loss: 5.029 | Norm: 0.085 | LR: 9.6799e-04 | Tokens/s: 149160\n",
      "Step 1168/55081 | Loss: 5.093 | Norm: 0.089 | LR: 9.6743e-04 | Tokens/s: 148826\n",
      "Step 1176/55081 | Loss: 5.093 | Norm: 0.094 | LR: 9.6687e-04 | Tokens/s: 149289\n",
      "Step 1184/55081 | Loss: 5.172 | Norm: 0.080 | LR: 9.6630e-04 | Tokens/s: 149084\n",
      "Step 1192/55081 | Loss: 5.243 | Norm: 0.138 | LR: 9.6573e-04 | Tokens/s: 149479\n",
      "Step 1200/55081 | Loss: 5.195 | Norm: 0.173 | LR: 9.6516e-04 | Tokens/s: 150818\n",
      "Step 1208/55081 | Loss: 5.136 | Norm: 0.206 | LR: 9.6458e-04 | Tokens/s: 149439\n",
      "Step 1216/55081 | Loss: 5.435 | Norm: 0.303 | LR: 9.6399e-04 | Tokens/s: 150221\n",
      "Step 1224/55081 | Loss: 5.237 | Norm: 0.093 | LR: 9.6340e-04 | Tokens/s: 149004\n",
      "Step 1232/55081 | Loss: 5.105 | Norm: 0.096 | LR: 9.6281e-04 | Tokens/s: 148804\n",
      "Step 1240/55081 | Loss: 5.088 | Norm: 0.079 | LR: 9.6221e-04 | Tokens/s: 149165\n",
      "Step 1248/55081 | Loss: 5.105 | Norm: 0.175 | LR: 9.6160e-04 | Tokens/s: 148955\n",
      "Step 1256/55081 | Loss: 5.075 | Norm: 0.120 | LR: 9.6099e-04 | Tokens/s: 149142\n",
      "Step 1264/55081 | Loss: 5.185 | Norm: 0.141 | LR: 9.6038e-04 | Tokens/s: 148989\n",
      "Step 1272/55081 | Loss: 5.051 | Norm: 0.093 | LR: 9.5976e-04 | Tokens/s: 149145\n",
      "Step 1280/55081 | Loss: 5.230 | Norm: 0.098 | LR: 9.5914e-04 | Tokens/s: 149409\n",
      "Step 1288/55081 | Loss: 5.197 | Norm: 0.113 | LR: 9.5852e-04 | Tokens/s: 149976\n",
      "Step 1296/55081 | Loss: 5.032 | Norm: 0.091 | LR: 9.5788e-04 | Tokens/s: 69422\n",
      "Step 1304/55081 | Loss: 5.083 | Norm: 0.094 | LR: 9.5725e-04 | Tokens/s: 149311\n",
      "Step 1312/55081 | Loss: 5.071 | Norm: 0.113 | LR: 9.5661e-04 | Tokens/s: 148610\n",
      "Step 1320/55081 | Loss: 5.079 | Norm: 0.095 | LR: 9.5596e-04 | Tokens/s: 149365\n",
      "Step 1328/55081 | Loss: 5.074 | Norm: 0.086 | LR: 9.5532e-04 | Tokens/s: 149469\n",
      "Step 1336/55081 | Loss: 5.058 | Norm: 0.083 | LR: 9.5466e-04 | Tokens/s: 150016\n",
      "Step 1344/55081 | Loss: 5.099 | Norm: 0.075 | LR: 9.5400e-04 | Tokens/s: 149364\n",
      "Step 1352/55081 | Loss: 5.041 | Norm: 0.087 | LR: 9.5334e-04 | Tokens/s: 150307\n",
      "Step 1360/55081 | Loss: 5.126 | Norm: 0.099 | LR: 9.5267e-04 | Tokens/s: 149384\n",
      "Step 1368/55081 | Loss: 5.046 | Norm: 0.087 | LR: 9.5200e-04 | Tokens/s: 149409\n",
      "Step 1376/55081 | Loss: 5.055 | Norm: 0.082 | LR: 9.5133e-04 | Tokens/s: 149063\n",
      "Step 1384/55081 | Loss: 5.315 | Norm: 0.172 | LR: 9.5065e-04 | Tokens/s: 149313\n",
      "Step 1392/55081 | Loss: 5.274 | Norm: 0.116 | LR: 9.4996e-04 | Tokens/s: 149198\n",
      "Step 1400/55081 | Loss: 5.056 | Norm: 0.086 | LR: 9.4927e-04 | Tokens/s: 149356\n",
      "Step 1408/55081 | Loss: 4.887 | Norm: 0.095 | LR: 9.4858e-04 | Tokens/s: 149493\n",
      "Step 1416/55081 | Loss: 5.101 | Norm: 0.102 | LR: 9.4788e-04 | Tokens/s: 148838\n",
      "Step 1424/55081 | Loss: 5.114 | Norm: 0.082 | LR: 9.4718e-04 | Tokens/s: 149447\n",
      "Step 1432/55081 | Loss: 5.064 | Norm: 0.110 | LR: 9.4647e-04 | Tokens/s: 149422\n",
      "Step 1440/55081 | Loss: 4.943 | Norm: 0.088 | LR: 9.4576e-04 | Tokens/s: 149785\n",
      "Step 1448/55081 | Loss: 5.064 | Norm: 0.086 | LR: 9.4504e-04 | Tokens/s: 149443\n",
      "Step 1456/55081 | Loss: 5.239 | Norm: 0.081 | LR: 9.4432e-04 | Tokens/s: 148968\n",
      "Step 1464/55081 | Loss: 5.034 | Norm: 0.081 | LR: 9.4359e-04 | Tokens/s: 149310\n",
      "Step 1472/55081 | Loss: 5.085 | Norm: 0.069 | LR: 9.4287e-04 | Tokens/s: 148745\n",
      "Step 1480/55081 | Loss: 5.080 | Norm: 0.096 | LR: 9.4213e-04 | Tokens/s: 148802\n",
      "Step 1488/55081 | Loss: 5.357 | Norm: 0.286 | LR: 9.4139e-04 | Tokens/s: 149100\n",
      "Step 1496/55081 | Loss: 5.119 | Norm: 0.129 | LR: 9.4065e-04 | Tokens/s: 149206\n",
      "Epoch 1500 | Training checkpoint saved at models/tinyllm/checkpoint_1500.pt\n",
      "Step 1504/55081 | Loss: 5.154 | Norm: 0.140 | LR: 9.3990e-04 | Tokens/s: 149498\n",
      "Step 1512/55081 | Loss: 5.164 | Norm: 0.211 | LR: 9.3915e-04 | Tokens/s: 149370\n",
      "Step 1520/55081 | Loss: 5.049 | Norm: 0.108 | LR: 9.3840e-04 | Tokens/s: 148725\n",
      "Step 1528/55081 | Loss: 5.077 | Norm: 0.159 | LR: 9.3764e-04 | Tokens/s: 147851\n",
      "Step 1536/55081 | Loss: 5.280 | Norm: 0.245 | LR: 9.3687e-04 | Tokens/s: 149883\n",
      "Step 1544/55081 | Loss: 5.238 | Norm: 0.086 | LR: 9.3611e-04 | Tokens/s: 149392\n",
      "Step 1552/55081 | Loss: 5.103 | Norm: 0.112 | LR: 9.3533e-04 | Tokens/s: 149208\n",
      "Step 1560/55081 | Loss: 5.141 | Norm: 0.107 | LR: 9.3456e-04 | Tokens/s: 149233\n",
      "Step 1568/55081 | Loss: 5.108 | Norm: 0.104 | LR: 9.3377e-04 | Tokens/s: 148418\n",
      "Step 1576/55081 | Loss: 5.097 | Norm: 0.137 | LR: 9.3299e-04 | Tokens/s: 149175\n",
      "Step 1584/55081 | Loss: 5.142 | Norm: 0.120 | LR: 9.3220e-04 | Tokens/s: 148999\n",
      "Step 1592/55081 | Loss: 5.054 | Norm: 0.101 | LR: 9.3140e-04 | Tokens/s: 149758\n",
      "Step 1600/55081 | Loss: 5.018 | Norm: 0.140 | LR: 9.3061e-04 | Tokens/s: 149252\n",
      "Step 1608/55081 | Loss: 4.986 | Norm: 0.101 | LR: 9.2980e-04 | Tokens/s: 150051\n",
      "Step 1616/55081 | Loss: 5.339 | Norm: 0.282 | LR: 9.2900e-04 | Tokens/s: 148671\n",
      "Step 1624/55081 | Loss: 5.242 | Norm: 0.071 | LR: 9.2818e-04 | Tokens/s: 149938\n",
      "Step 1632/55081 | Loss: 4.993 | Norm: 0.115 | LR: 9.2737e-04 | Tokens/s: 148686\n",
      "Step 1640/55081 | Loss: 5.071 | Norm: 0.120 | LR: 9.2655e-04 | Tokens/s: 149250\n",
      "Step 1648/55081 | Loss: 5.091 | Norm: 0.354 | LR: 9.2573e-04 | Tokens/s: 149242\n",
      "Step 1656/55081 | Loss: 5.103 | Norm: 0.130 | LR: 9.2490e-04 | Tokens/s: 149642\n",
      "Step 1664/55081 | Loss: 5.172 | Norm: 0.136 | LR: 9.2407e-04 | Tokens/s: 148685\n",
      "Step 1672/55081 | Loss: 5.109 | Norm: 0.163 | LR: 9.2323e-04 | Tokens/s: 149128\n",
      "Step 1680/55081 | Loss: 5.071 | Norm: 0.165 | LR: 9.2239e-04 | Tokens/s: 150433\n",
      "Step 1688/55081 | Loss: 5.024 | Norm: 0.127 | LR: 9.2154e-04 | Tokens/s: 149325\n",
      "Step 1696/55081 | Loss: 4.983 | Norm: 0.144 | LR: 9.2069e-04 | Tokens/s: 149737\n",
      "Step 1704/55081 | Loss: 5.038 | Norm: 0.132 | LR: 9.1984e-04 | Tokens/s: 149122\n",
      "Step 1712/55081 | Loss: 5.041 | Norm: 0.087 | LR: 9.1898e-04 | Tokens/s: 149413\n",
      "Step 1720/55081 | Loss: 5.079 | Norm: 0.082 | LR: 9.1812e-04 | Tokens/s: 148867\n",
      "Step 1728/55081 | Loss: 5.039 | Norm: 0.087 | LR: 9.1726e-04 | Tokens/s: 149247\n",
      "Step 1736/55081 | Loss: 5.033 | Norm: 0.107 | LR: 9.1639e-04 | Tokens/s: 149227\n",
      "Step 1744/55081 | Loss: 5.153 | Norm: 0.163 | LR: 9.1551e-04 | Tokens/s: 149033\n",
      "Step 1752/55081 | Loss: 5.015 | Norm: 0.087 | LR: 9.1464e-04 | Tokens/s: 149031\n",
      "Step 1760/55081 | Loss: 4.992 | Norm: 0.114 | LR: 9.1375e-04 | Tokens/s: 149538\n",
      "Step 1768/55081 | Loss: 5.023 | Norm: 0.087 | LR: 9.1287e-04 | Tokens/s: 150237\n",
      "Step 1776/55081 | Loss: 4.981 | Norm: 0.069 | LR: 9.1198e-04 | Tokens/s: 70677\n",
      "Step 1784/55081 | Loss: 5.106 | Norm: 0.130 | LR: 9.1108e-04 | Tokens/s: 149154\n",
      "Step 1792/55081 | Loss: 5.004 | Norm: 0.104 | LR: 9.1019e-04 | Tokens/s: 148753\n",
      "Step 1800/55081 | Loss: 5.059 | Norm: 0.134 | LR: 9.0928e-04 | Tokens/s: 149158\n",
      "Step 1808/55081 | Loss: 5.069 | Norm: 0.087 | LR: 9.0838e-04 | Tokens/s: 148186\n",
      "Step 1816/55081 | Loss: 5.064 | Norm: 0.105 | LR: 9.0747e-04 | Tokens/s: 149707\n",
      "Step 1824/55081 | Loss: 5.072 | Norm: 0.102 | LR: 9.0655e-04 | Tokens/s: 149152\n",
      "Step 1832/55081 | Loss: 5.043 | Norm: 0.094 | LR: 9.0563e-04 | Tokens/s: 149789\n",
      "Step 1840/55081 | Loss: 5.107 | Norm: 0.118 | LR: 9.0471e-04 | Tokens/s: 149458\n",
      "Step 1848/55081 | Loss: 5.029 | Norm: 0.076 | LR: 9.0379e-04 | Tokens/s: 149396\n",
      "Step 1856/55081 | Loss: 5.079 | Norm: 0.071 | LR: 9.0286e-04 | Tokens/s: 149296\n",
      "Step 1864/55081 | Loss: 5.163 | Norm: 0.079 | LR: 9.0192e-04 | Tokens/s: 148267\n",
      "Step 1872/55081 | Loss: 4.987 | Norm: 0.077 | LR: 9.0098e-04 | Tokens/s: 148959\n",
      "Step 1880/55081 | Loss: 5.167 | Norm: 0.085 | LR: 9.0004e-04 | Tokens/s: 148760\n",
      "Step 1888/55081 | Loss: 4.999 | Norm: 0.088 | LR: 8.9909e-04 | Tokens/s: 148828\n",
      "Step 1896/55081 | Loss: 4.958 | Norm: 0.083 | LR: 8.9814e-04 | Tokens/s: 148808\n",
      "Step 1904/55081 | Loss: 5.078 | Norm: 0.102 | LR: 8.9719e-04 | Tokens/s: 150031\n",
      "Step 1912/55081 | Loss: 5.071 | Norm: 0.085 | LR: 8.9623e-04 | Tokens/s: 148594\n",
      "Step 1920/55081 | Loss: 5.127 | Norm: 0.080 | LR: 8.9527e-04 | Tokens/s: 149816\n",
      "Step 1928/55081 | Loss: 5.092 | Norm: 0.120 | LR: 8.9431e-04 | Tokens/s: 149029\n",
      "Step 1936/55081 | Loss: 5.049 | Norm: 0.071 | LR: 8.9334e-04 | Tokens/s: 148276\n",
      "Step 1944/55081 | Loss: 5.044 | Norm: 0.109 | LR: 8.9236e-04 | Tokens/s: 148846\n",
      "Step 1952/55081 | Loss: 5.008 | Norm: 0.115 | LR: 8.9139e-04 | Tokens/s: 148979\n",
      "Step 1960/55081 | Loss: 5.072 | Norm: 0.102 | LR: 8.9040e-04 | Tokens/s: 64477\n",
      "Step 1968/55081 | Loss: 4.971 | Norm: 0.059 | LR: 8.8942e-04 | Tokens/s: 149684\n",
      "Step 1976/55081 | Loss: 5.023 | Norm: 0.093 | LR: 8.8843e-04 | Tokens/s: 149559\n",
      "Step 1984/55081 | Loss: 5.109 | Norm: 0.078 | LR: 8.8744e-04 | Tokens/s: 149209\n",
      "Step 1992/55081 | Loss: 5.065 | Norm: 0.084 | LR: 8.8644e-04 | Tokens/s: 149205\n",
      "Step 2000/55081 | Loss: 5.026 | Norm: 0.083 | LR: 8.8544e-04 | Tokens/s: 149184\n",
      "Epoch 2000 | Training checkpoint saved at models/tinyllm/checkpoint_2000.pt\n",
      "Step 2008/55081 | Loss: 5.051 | Norm: 0.076 | LR: 8.8444e-04 | Tokens/s: 149196\n",
      "Step 2016/55081 | Loss: 5.079 | Norm: 0.115 | LR: 8.8343e-04 | Tokens/s: 150758\n",
      "Step 2024/55081 | Loss: 5.098 | Norm: 0.075 | LR: 8.8242e-04 | Tokens/s: 149168\n",
      "Step 2032/55081 | Loss: 5.063 | Norm: 0.101 | LR: 8.8140e-04 | Tokens/s: 149373\n",
      "Step 2040/55081 | Loss: 5.030 | Norm: 0.104 | LR: 8.8039e-04 | Tokens/s: 148701\n",
      "Step 2048/55081 | Loss: 5.089 | Norm: 0.080 | LR: 8.7936e-04 | Tokens/s: 149186\n",
      "Step 2056/55081 | Loss: 5.052 | Norm: 0.116 | LR: 8.7834e-04 | Tokens/s: 148810\n",
      "Step 2064/55081 | Loss: 5.072 | Norm: 0.105 | LR: 8.7731e-04 | Tokens/s: 149203\n",
      "Step 2072/55081 | Loss: 5.165 | Norm: 0.114 | LR: 8.7627e-04 | Tokens/s: 149658\n",
      "Step 2080/55081 | Loss: 5.225 | Norm: 0.158 | LR: 8.7524e-04 | Tokens/s: 149556\n",
      "Step 2088/55081 | Loss: 5.221 | Norm: 0.118 | LR: 8.7419e-04 | Tokens/s: 149067\n",
      "Step 2096/55081 | Loss: 5.080 | Norm: 0.107 | LR: 8.7315e-04 | Tokens/s: 148812\n",
      "Step 2104/55081 | Loss: 5.080 | Norm: 0.125 | LR: 8.7210e-04 | Tokens/s: 149609\n",
      "Step 2112/55081 | Loss: 5.106 | Norm: 0.114 | LR: 8.7105e-04 | Tokens/s: 149415\n",
      "Step 2120/55081 | Loss: 5.037 | Norm: 0.081 | LR: 8.6999e-04 | Tokens/s: 149968\n",
      "Step 2128/55081 | Loss: 5.016 | Norm: 0.077 | LR: 8.6893e-04 | Tokens/s: 149040\n",
      "Step 2136/55081 | Loss: 5.189 | Norm: 0.125 | LR: 8.6787e-04 | Tokens/s: 149228\n",
      "Step 2144/55081 | Loss: 5.086 | Norm: 0.087 | LR: 8.6681e-04 | Tokens/s: 65282\n",
      "Step 2152/55081 | Loss: 4.952 | Norm: 0.115 | LR: 8.6573e-04 | Tokens/s: 149845\n",
      "Step 2160/55081 | Loss: 5.023 | Norm: 0.078 | LR: 8.6466e-04 | Tokens/s: 149260\n",
      "Step 2168/55081 | Loss: 5.068 | Norm: 0.083 | LR: 8.6358e-04 | Tokens/s: 150493\n",
      "Step 2176/55081 | Loss: 5.083 | Norm: 0.075 | LR: 8.6250e-04 | Tokens/s: 149193\n",
      "Step 2184/55081 | Loss: 4.997 | Norm: 0.107 | LR: 8.6142e-04 | Tokens/s: 149520\n",
      "Step 2192/55081 | Loss: 5.075 | Norm: 0.089 | LR: 8.6033e-04 | Tokens/s: 149324\n",
      "Step 2200/55081 | Loss: 5.175 | Norm: 0.100 | LR: 8.5924e-04 | Tokens/s: 148824\n",
      "Step 2208/55081 | Loss: 5.022 | Norm: 0.103 | LR: 8.5815e-04 | Tokens/s: 149371\n",
      "Step 2216/55081 | Loss: 4.995 | Norm: 0.110 | LR: 8.5705e-04 | Tokens/s: 147886\n",
      "Step 2224/55081 | Loss: 5.136 | Norm: 0.150 | LR: 8.5594e-04 | Tokens/s: 148605\n",
      "Step 2232/55081 | Loss: 5.029 | Norm: 0.078 | LR: 8.5484e-04 | Tokens/s: 149261\n",
      "Step 2240/55081 | Loss: 4.986 | Norm: 0.074 | LR: 8.5373e-04 | Tokens/s: 150266\n",
      "Step 2248/55081 | Loss: 4.997 | Norm: 0.147 | LR: 8.5262e-04 | Tokens/s: 149021\n",
      "Step 2256/55081 | Loss: 5.049 | Norm: 0.085 | LR: 8.5150e-04 | Tokens/s: 149512\n",
      "Step 2264/55081 | Loss: 5.049 | Norm: 0.083 | LR: 8.5038e-04 | Tokens/s: 149089\n",
      "Step 2272/55081 | Loss: 5.096 | Norm: 0.091 | LR: 8.4926e-04 | Tokens/s: 149231\n",
      "Step 2280/55081 | Loss: 5.052 | Norm: 0.088 | LR: 8.4814e-04 | Tokens/s: 149463\n",
      "Step 2288/55081 | Loss: 5.114 | Norm: 0.077 | LR: 8.4701e-04 | Tokens/s: 149183\n",
      "Step 2296/55081 | Loss: 5.098 | Norm: 0.087 | LR: 8.4587e-04 | Tokens/s: 149298\n",
      "Step 2304/55081 | Loss: 5.016 | Norm: 0.107 | LR: 8.4474e-04 | Tokens/s: 148980\n",
      "Step 2312/55081 | Loss: 4.989 | Norm: 0.081 | LR: 8.4360e-04 | Tokens/s: 149000\n",
      "Step 2320/55081 | Loss: 4.969 | Norm: 0.086 | LR: 8.4246e-04 | Tokens/s: 149336\n",
      "Step 2328/55081 | Loss: 4.963 | Norm: 0.103 | LR: 8.4131e-04 | Tokens/s: 150141\n",
      "Step 2336/55081 | Loss: 5.094 | Norm: 0.139 | LR: 8.4016e-04 | Tokens/s: 149374\n",
      "Step 2344/55081 | Loss: 5.467 | Norm: 0.095 | LR: 8.3901e-04 | Tokens/s: 149291\n",
      "Step 2352/55081 | Loss: 4.943 | Norm: 0.086 | LR: 8.3785e-04 | Tokens/s: 148938\n",
      "Step 2360/55081 | Loss: 5.146 | Norm: 0.072 | LR: 8.3669e-04 | Tokens/s: 149113\n",
      "Step 2368/55081 | Loss: 5.044 | Norm: 0.090 | LR: 8.3553e-04 | Tokens/s: 148989\n",
      "Step 2376/55081 | Loss: 4.967 | Norm: 0.079 | LR: 8.3436e-04 | Tokens/s: 149356\n",
      "Step 2384/55081 | Loss: 5.010 | Norm: 0.099 | LR: 8.3320e-04 | Tokens/s: 149486\n",
      "Step 2392/55081 | Loss: 5.012 | Norm: 0.071 | LR: 8.3202e-04 | Tokens/s: 149388\n",
      "Step 2400/55081 | Loss: 5.002 | Norm: 0.098 | LR: 8.3085e-04 | Tokens/s: 148926\n",
      "Step 2408/55081 | Loss: 4.999 | Norm: 0.088 | LR: 8.2967e-04 | Tokens/s: 149322\n",
      "Step 2416/55081 | Loss: 5.006 | Norm: 0.111 | LR: 8.2849e-04 | Tokens/s: 148085\n",
      "Step 2424/55081 | Loss: 5.034 | Norm: 0.086 | LR: 8.2730e-04 | Tokens/s: 149098\n",
      "Step 2432/55081 | Loss: 4.960 | Norm: 0.084 | LR: 8.2611e-04 | Tokens/s: 149291\n",
      "Step 2440/55081 | Loss: 5.012 | Norm: 0.076 | LR: 8.2492e-04 | Tokens/s: 149245\n",
      "Step 2448/55081 | Loss: 5.039 | Norm: 0.135 | LR: 8.2373e-04 | Tokens/s: 149045\n",
      "Step 2456/55081 | Loss: 5.171 | Norm: 0.070 | LR: 8.2253e-04 | Tokens/s: 149497\n",
      "Step 2464/55081 | Loss: 5.348 | Norm: 0.074 | LR: 8.2133e-04 | Tokens/s: 149852\n",
      "Step 2472/55081 | Loss: 5.228 | Norm: 0.069 | LR: 8.2013e-04 | Tokens/s: 149061\n",
      "Step 2480/55081 | Loss: 4.970 | Norm: 0.077 | LR: 8.1892e-04 | Tokens/s: 149709\n",
      "Step 2488/55081 | Loss: 5.083 | Norm: 0.069 | LR: 8.1771e-04 | Tokens/s: 149204\n",
      "Step 2496/55081 | Loss: 5.020 | Norm: 0.062 | LR: 8.1650e-04 | Tokens/s: 148936\n",
      "Epoch 2500 | Training checkpoint saved at models/tinyllm/checkpoint_2500.pt\n",
      "Step 2504/55081 | Loss: 5.198 | Norm: 0.070 | LR: 8.1528e-04 | Tokens/s: 149515\n",
      "Step 2512/55081 | Loss: 5.036 | Norm: 0.132 | LR: 8.1406e-04 | Tokens/s: 150195\n",
      "Step 2520/55081 | Loss: 5.112 | Norm: 0.089 | LR: 8.1284e-04 | Tokens/s: 149107\n",
      "Step 2528/55081 | Loss: 4.967 | Norm: 0.100 | LR: 8.1161e-04 | Tokens/s: 150363\n",
      "Step 2536/55081 | Loss: 5.019 | Norm: 0.066 | LR: 8.1038e-04 | Tokens/s: 149309\n",
      "Step 2544/55081 | Loss: 4.980 | Norm: 0.070 | LR: 8.0915e-04 | Tokens/s: 149246\n",
      "Step 2552/55081 | Loss: 5.014 | Norm: 0.064 | LR: 8.0792e-04 | Tokens/s: 149158\n",
      "Step 2560/55081 | Loss: 5.217 | Norm: 0.141 | LR: 8.0668e-04 | Tokens/s: 149043\n",
      "Step 2568/55081 | Loss: 5.143 | Norm: 0.085 | LR: 8.0544e-04 | Tokens/s: 148619\n",
      "Step 2576/55081 | Loss: 5.183 | Norm: 0.104 | LR: 8.0420e-04 | Tokens/s: 148954\n",
      "Step 2584/55081 | Loss: 5.005 | Norm: 0.122 | LR: 8.0295e-04 | Tokens/s: 148550\n",
      "Step 2592/55081 | Loss: 5.077 | Norm: 0.117 | LR: 8.0170e-04 | Tokens/s: 149256\n",
      "Step 2600/55081 | Loss: 5.063 | Norm: 0.098 | LR: 8.0045e-04 | Tokens/s: 149147\n",
      "Step 2608/55081 | Loss: 5.022 | Norm: 0.098 | LR: 7.9920e-04 | Tokens/s: 148886\n",
      "Step 2616/55081 | Loss: 4.972 | Norm: 0.069 | LR: 7.9794e-04 | Tokens/s: 149157\n",
      "Step 2624/55081 | Loss: 5.066 | Norm: 0.141 | LR: 7.9668e-04 | Tokens/s: 149555\n",
      "Step 2632/55081 | Loss: 4.927 | Norm: 0.083 | LR: 7.9541e-04 | Tokens/s: 150282\n",
      "Step 2640/55081 | Loss: 4.975 | Norm: 0.093 | LR: 7.9415e-04 | Tokens/s: 149208\n",
      "Step 2648/55081 | Loss: 5.014 | Norm: 0.102 | LR: 7.9288e-04 | Tokens/s: 149468\n",
      "Step 2656/55081 | Loss: 5.011 | Norm: 0.102 | LR: 7.9161e-04 | Tokens/s: 149130\n",
      "Step 2664/55081 | Loss: 4.967 | Norm: 0.084 | LR: 7.9033e-04 | Tokens/s: 150018\n",
      "Step 2672/55081 | Loss: 5.095 | Norm: 0.102 | LR: 7.8906e-04 | Tokens/s: 149324\n",
      "Step 2680/55081 | Loss: 5.101 | Norm: 0.075 | LR: 7.8778e-04 | Tokens/s: 149097\n",
      "Step 2688/55081 | Loss: 5.023 | Norm: 0.122 | LR: 7.8649e-04 | Tokens/s: 149122\n",
      "Step 2696/55081 | Loss: 5.014 | Norm: 0.138 | LR: 7.8521e-04 | Tokens/s: 149015\n",
      "Step 2704/55081 | Loss: 5.079 | Norm: 0.082 | LR: 7.8392e-04 | Tokens/s: 149276\n",
      "Step 2712/55081 | Loss: 4.953 | Norm: 0.099 | LR: 7.8263e-04 | Tokens/s: 149529\n",
      "Step 2720/55081 | Loss: 5.020 | Norm: 0.094 | LR: 7.8133e-04 | Tokens/s: 149468\n",
      "Step 2728/55081 | Loss: 5.104 | Norm: 0.071 | LR: 7.8004e-04 | Tokens/s: 148883\n",
      "Step 2736/55081 | Loss: 5.087 | Norm: 0.069 | LR: 7.7874e-04 | Tokens/s: 148816\n",
      "Step 2744/55081 | Loss: 5.003 | Norm: 0.067 | LR: 7.7744e-04 | Tokens/s: 149297\n",
      "Step 2752/55081 | Loss: 5.001 | Norm: 0.085 | LR: 7.7613e-04 | Tokens/s: 149581\n",
      "Step 2760/55081 | Loss: 5.177 | Norm: 0.096 | LR: 7.7482e-04 | Tokens/s: 149372\n",
      "Step 2768/55081 | Loss: 5.021 | Norm: 0.076 | LR: 7.7351e-04 | Tokens/s: 66731\n",
      "Step 2776/55081 | Loss: 5.109 | Norm: 0.107 | LR: 7.7220e-04 | Tokens/s: 149661\n",
      "Step 2784/55081 | Loss: 4.980 | Norm: 0.088 | LR: 7.7089e-04 | Tokens/s: 150444\n",
      "Step 2792/55081 | Loss: 5.221 | Norm: 0.064 | LR: 7.6957e-04 | Tokens/s: 149132\n",
      "Step 2800/55081 | Loss: 5.051 | Norm: 0.140 | LR: 7.6825e-04 | Tokens/s: 150127\n",
      "Step 2808/55081 | Loss: 5.164 | Norm: 0.078 | LR: 7.6693e-04 | Tokens/s: 149403\n",
      "Step 2816/55081 | Loss: 5.016 | Norm: 0.081 | LR: 7.6560e-04 | Tokens/s: 148921\n",
      "Step 2824/55081 | Loss: 5.056 | Norm: 0.103 | LR: 7.6427e-04 | Tokens/s: 148996\n",
      "Step 2832/55081 | Loss: 5.043 | Norm: 0.115 | LR: 7.6294e-04 | Tokens/s: 149245\n",
      "Step 2840/55081 | Loss: 5.011 | Norm: 0.071 | LR: 7.6161e-04 | Tokens/s: 149326\n",
      "Step 2848/55081 | Loss: 5.086 | Norm: 0.119 | LR: 7.6028e-04 | Tokens/s: 149137\n",
      "Step 2856/55081 | Loss: 5.071 | Norm: 0.071 | LR: 7.5894e-04 | Tokens/s: 148502\n",
      "Step 2864/55081 | Loss: 5.118 | Norm: 0.078 | LR: 7.5760e-04 | Tokens/s: 149316\n",
      "Step 2872/55081 | Loss: 5.057 | Norm: 0.076 | LR: 7.5626e-04 | Tokens/s: 150575\n",
      "Step 2880/55081 | Loss: 4.996 | Norm: 0.082 | LR: 7.5491e-04 | Tokens/s: 149528\n",
      "Step 2888/55081 | Loss: 5.056 | Norm: 0.068 | LR: 7.5356e-04 | Tokens/s: 150130\n",
      "Step 2896/55081 | Loss: 5.068 | Norm: 0.084 | LR: 7.5221e-04 | Tokens/s: 149179\n",
      "Step 2904/55081 | Loss: 5.050 | Norm: 0.060 | LR: 7.5086e-04 | Tokens/s: 148848\n",
      "Step 2912/55081 | Loss: 5.044 | Norm: 0.101 | LR: 7.4951e-04 | Tokens/s: 149450\n",
      "Step 2920/55081 | Loss: 5.046 | Norm: 0.103 | LR: 7.4815e-04 | Tokens/s: 149348\n",
      "Step 2928/55081 | Loss: 5.072 | Norm: 0.080 | LR: 7.4679e-04 | Tokens/s: 149094\n",
      "Step 2936/55081 | Loss: 5.091 | Norm: 0.186 | LR: 7.4543e-04 | Tokens/s: 148809\n",
      "Step 2944/55081 | Loss: 5.112 | Norm: 0.168 | LR: 7.4407e-04 | Tokens/s: 149195\n",
      "Step 2952/55081 | Loss: 5.033 | Norm: 0.090 | LR: 7.4270e-04 | Tokens/s: 149046\n",
      "Step 2960/55081 | Loss: 5.059 | Norm: 0.128 | LR: 7.4133e-04 | Tokens/s: 149489\n",
      "Step 2968/55081 | Loss: 5.002 | Norm: 0.136 | LR: 7.3996e-04 | Tokens/s: 148864\n",
      "Step 2976/55081 | Loss: 5.035 | Norm: 0.116 | LR: 7.3859e-04 | Tokens/s: 149674\n",
      "Step 2984/55081 | Loss: 4.982 | Norm: 0.165 | LR: 7.3721e-04 | Tokens/s: 148826\n",
      "Step 2992/55081 | Loss: 5.022 | Norm: 0.225 | LR: 7.3583e-04 | Tokens/s: 149853\n",
      "Step 3000/55081 | Loss: 5.035 | Norm: 0.086 | LR: 7.3445e-04 | Tokens/s: 149325\n",
      "Epoch 3000 | Training checkpoint saved at models/tinyllm/checkpoint_3000.pt\n",
      "Step 3008/55081 | Loss: 5.032 | Norm: 0.077 | LR: 7.3307e-04 | Tokens/s: 149571\n",
      "Step 3016/55081 | Loss: 5.031 | Norm: 0.088 | LR: 7.3169e-04 | Tokens/s: 149396\n",
      "Step 3024/55081 | Loss: 5.338 | Norm: 0.081 | LR: 7.3030e-04 | Tokens/s: 149719\n",
      "Step 3032/55081 | Loss: 4.918 | Norm: 0.101 | LR: 7.2891e-04 | Tokens/s: 149523\n",
      "Step 3040/55081 | Loss: 5.035 | Norm: 0.093 | LR: 7.2752e-04 | Tokens/s: 149071\n",
      "Step 3048/55081 | Loss: 5.101 | Norm: 0.097 | LR: 7.2613e-04 | Tokens/s: 148774\n",
      "Step 3056/55081 | Loss: 4.996 | Norm: 0.129 | LR: 7.2474e-04 | Tokens/s: 150274\n",
      "Step 3064/55081 | Loss: 5.055 | Norm: 0.124 | LR: 7.2334e-04 | Tokens/s: 149347\n",
      "Step 3072/55081 | Loss: 5.324 | Norm: 0.091 | LR: 7.2194e-04 | Tokens/s: 150290\n",
      "Step 3080/55081 | Loss: 5.041 | Norm: 0.082 | LR: 7.2054e-04 | Tokens/s: 149109\n",
      "Step 3088/55081 | Loss: 5.040 | Norm: 0.074 | LR: 7.1914e-04 | Tokens/s: 149270\n",
      "Step 3096/55081 | Loss: 5.370 | Norm: 0.069 | LR: 7.1773e-04 | Tokens/s: 148300\n",
      "Step 3104/55081 | Loss: 5.073 | Norm: 0.093 | LR: 7.1632e-04 | Tokens/s: 149131\n",
      "Step 3112/55081 | Loss: 5.003 | Norm: 0.078 | LR: 7.1492e-04 | Tokens/s: 149100\n",
      "Step 3120/55081 | Loss: 4.887 | Norm: 0.077 | LR: 7.1350e-04 | Tokens/s: 149283\n",
      "Step 3128/55081 | Loss: 5.000 | Norm: 0.074 | LR: 7.1209e-04 | Tokens/s: 149411\n",
      "Step 3136/55081 | Loss: 5.261 | Norm: 0.075 | LR: 7.1068e-04 | Tokens/s: 148916\n",
      "Step 3144/55081 | Loss: 4.948 | Norm: 0.070 | LR: 7.0926e-04 | Tokens/s: 151035\n",
      "Step 3152/55081 | Loss: 4.924 | Norm: 0.080 | LR: 7.0784e-04 | Tokens/s: 149408\n",
      "Step 3160/55081 | Loss: 5.045 | Norm: 0.100 | LR: 7.0642e-04 | Tokens/s: 150646\n",
      "Step 3168/55081 | Loss: 5.051 | Norm: 0.068 | LR: 7.0500e-04 | Tokens/s: 149503\n",
      "Step 3176/55081 | Loss: 5.062 | Norm: 0.093 | LR: 7.0357e-04 | Tokens/s: 149176\n",
      "Step 3184/55081 | Loss: 5.074 | Norm: 0.080 | LR: 7.0215e-04 | Tokens/s: 148865\n",
      "Step 3192/55081 | Loss: 5.015 | Norm: 0.062 | LR: 7.0072e-04 | Tokens/s: 149096\n",
      "Step 3200/55081 | Loss: 5.007 | Norm: 0.075 | LR: 6.9929e-04 | Tokens/s: 148812\n",
      "Step 3208/55081 | Loss: 5.156 | Norm: 0.113 | LR: 6.9785e-04 | Tokens/s: 148151\n",
      "Step 3216/55081 | Loss: 5.070 | Norm: 0.107 | LR: 6.9642e-04 | Tokens/s: 149634\n",
      "Step 3224/55081 | Loss: 5.227 | Norm: 0.282 | LR: 6.9499e-04 | Tokens/s: 148981\n",
      "Step 3232/55081 | Loss: 5.494 | Norm: 0.090 | LR: 6.9355e-04 | Tokens/s: 149940\n",
      "Step 3240/55081 | Loss: 4.997 | Norm: 0.088 | LR: 6.9211e-04 | Tokens/s: 149074\n",
      "Step 3248/55081 | Loss: 4.969 | Norm: 0.082 | LR: 6.9067e-04 | Tokens/s: 149418\n",
      "Step 3256/55081 | Loss: 5.048 | Norm: 0.100 | LR: 6.8923e-04 | Tokens/s: 149055\n",
      "Step 3264/55081 | Loss: 4.977 | Norm: 0.134 | LR: 6.8778e-04 | Tokens/s: 150737\n",
      "Step 3272/55081 | Loss: 4.968 | Norm: 0.094 | LR: 6.8633e-04 | Tokens/s: 148918\n",
      "Step 3280/55081 | Loss: 5.074 | Norm: 0.076 | LR: 6.8489e-04 | Tokens/s: 149869\n",
      "Step 3288/55081 | Loss: 5.098 | Norm: 0.100 | LR: 6.8344e-04 | Tokens/s: 149181\n",
      "Step 3296/55081 | Loss: 5.007 | Norm: 0.105 | LR: 6.8199e-04 | Tokens/s: 148538\n",
      "Step 3304/55081 | Loss: 4.995 | Norm: 0.102 | LR: 6.8053e-04 | Tokens/s: 149028\n",
      "Step 3312/55081 | Loss: 5.043 | Norm: 0.079 | LR: 6.7908e-04 | Tokens/s: 149110\n",
      "Step 3320/55081 | Loss: 4.964 | Norm: 0.085 | LR: 6.7762e-04 | Tokens/s: 149494\n",
      "Step 3328/55081 | Loss: 5.005 | Norm: 0.084 | LR: 6.7617e-04 | Tokens/s: 149371\n",
      "Step 3336/55081 | Loss: 4.958 | Norm: 0.077 | LR: 6.7471e-04 | Tokens/s: 149515\n",
      "Step 3344/55081 | Loss: 5.149 | Norm: 0.105 | LR: 6.7325e-04 | Tokens/s: 149420\n",
      "Step 3352/55081 | Loss: 5.106 | Norm: 0.086 | LR: 6.7178e-04 | Tokens/s: 150055\n",
      "Step 3360/55081 | Loss: 4.984 | Norm: 0.076 | LR: 6.7032e-04 | Tokens/s: 149615\n",
      "Step 3368/55081 | Loss: 4.883 | Norm: 0.069 | LR: 6.6885e-04 | Tokens/s: 149456\n",
      "Step 3376/55081 | Loss: 5.532 | Norm: 0.247 | LR: 6.6739e-04 | Tokens/s: 149499\n",
      "Step 3384/55081 | Loss: 5.511 | Norm: 0.104 | LR: 6.6592e-04 | Tokens/s: 148548\n",
      "Step 3392/55081 | Loss: 5.095 | Norm: 0.086 | LR: 6.6445e-04 | Tokens/s: 148813\n",
      "Step 3400/55081 | Loss: 5.119 | Norm: 0.101 | LR: 6.6298e-04 | Tokens/s: 148969\n",
      "Step 3408/55081 | Loss: 5.014 | Norm: 0.094 | LR: 6.6150e-04 | Tokens/s: 149055\n",
      "Step 3416/55081 | Loss: 5.068 | Norm: 0.098 | LR: 6.6003e-04 | Tokens/s: 149588\n",
      "Step 3424/55081 | Loss: 4.981 | Norm: 0.133 | LR: 6.5855e-04 | Tokens/s: 149179\n",
      "Step 3432/55081 | Loss: 4.962 | Norm: 0.144 | LR: 6.5708e-04 | Tokens/s: 149528\n",
      "Step 3440/55081 | Loss: 4.980 | Norm: 0.102 | LR: 6.5560e-04 | Tokens/s: 149002\n",
      "Step 3448/55081 | Loss: 4.989 | Norm: 0.076 | LR: 6.5412e-04 | Tokens/s: 149493\n",
      "Step 3456/55081 | Loss: 5.022 | Norm: 0.074 | LR: 6.5264e-04 | Tokens/s: 148471\n",
      "Step 3464/55081 | Loss: 5.066 | Norm: 0.100 | LR: 6.5116e-04 | Tokens/s: 148914\n",
      "Step 3472/55081 | Loss: 5.142 | Norm: 0.108 | LR: 6.4967e-04 | Tokens/s: 149224\n",
      "Step 3480/55081 | Loss: 5.060 | Norm: 0.103 | LR: 6.4819e-04 | Tokens/s: 149649\n",
      "Step 3488/55081 | Loss: 4.955 | Norm: 0.084 | LR: 6.4670e-04 | Tokens/s: 149547\n",
      "Step 3496/55081 | Loss: 4.901 | Norm: 0.078 | LR: 6.4521e-04 | Tokens/s: 148866\n",
      "Epoch 3500 | Training checkpoint saved at models/tinyllm/checkpoint_3500.pt\n",
      "Step 3504/55081 | Loss: 5.035 | Norm: 0.082 | LR: 6.4372e-04 | Tokens/s: 149486\n",
      "Step 3512/55081 | Loss: 5.040 | Norm: 0.071 | LR: 6.4223e-04 | Tokens/s: 149536\n",
      "Step 3520/55081 | Loss: 4.977 | Norm: 0.093 | LR: 6.4074e-04 | Tokens/s: 148495\n",
      "Step 3528/55081 | Loss: 5.024 | Norm: 0.065 | LR: 6.3925e-04 | Tokens/s: 149595\n",
      "Step 3536/55081 | Loss: 5.087 | Norm: 0.066 | LR: 6.3776e-04 | Tokens/s: 149455\n",
      "Step 3544/55081 | Loss: 4.952 | Norm: 0.073 | LR: 6.3626e-04 | Tokens/s: 149025\n",
      "Step 3552/55081 | Loss: 5.080 | Norm: 0.115 | LR: 6.3476e-04 | Tokens/s: 149365\n",
      "Step 3560/55081 | Loss: 5.124 | Norm: 0.082 | LR: 6.3327e-04 | Tokens/s: 148677\n",
      "Step 3568/55081 | Loss: 5.003 | Norm: 0.086 | LR: 6.3177e-04 | Tokens/s: 148869\n",
      "Step 3576/55081 | Loss: 5.024 | Norm: 0.061 | LR: 6.3027e-04 | Tokens/s: 148044\n",
      "Step 3584/55081 | Loss: 5.011 | Norm: 0.066 | LR: 6.2877e-04 | Tokens/s: 148785\n",
      "Step 3592/55081 | Loss: 5.012 | Norm: 0.075 | LR: 6.2727e-04 | Tokens/s: 148635\n",
      "Step 3600/55081 | Loss: 4.966 | Norm: 0.115 | LR: 6.2576e-04 | Tokens/s: 148247\n",
      "Step 3608/55081 | Loss: 5.076 | Norm: 0.131 | LR: 6.2426e-04 | Tokens/s: 147473\n",
      "Step 3616/55081 | Loss: 5.041 | Norm: 0.070 | LR: 6.2275e-04 | Tokens/s: 148876\n",
      "Step 3624/55081 | Loss: 4.967 | Norm: 0.084 | LR: 6.2125e-04 | Tokens/s: 65240\n",
      "Step 3632/55081 | Loss: 5.046 | Norm: 0.079 | LR: 6.1974e-04 | Tokens/s: 148813\n",
      "Step 3640/55081 | Loss: 5.055 | Norm: 0.092 | LR: 6.1823e-04 | Tokens/s: 148917\n",
      "Step 3648/55081 | Loss: 4.979 | Norm: 0.100 | LR: 6.1672e-04 | Tokens/s: 148738\n",
      "Step 3656/55081 | Loss: 5.037 | Norm: 0.080 | LR: 6.1521e-04 | Tokens/s: 149660\n",
      "Step 3664/55081 | Loss: 4.954 | Norm: 0.102 | LR: 6.1370e-04 | Tokens/s: 149130\n",
      "Step 3672/55081 | Loss: 5.070 | Norm: 0.141 | LR: 6.1219e-04 | Tokens/s: 149542\n",
      "Step 3680/55081 | Loss: 5.107 | Norm: 0.095 | LR: 6.1068e-04 | Tokens/s: 148868\n",
      "Step 3688/55081 | Loss: 5.193 | Norm: 0.189 | LR: 6.0917e-04 | Tokens/s: 149234\n",
      "Step 3696/55081 | Loss: 5.101 | Norm: 0.132 | LR: 6.0765e-04 | Tokens/s: 149038\n",
      "Step 3704/55081 | Loss: 5.117 | Norm: 0.115 | LR: 6.0614e-04 | Tokens/s: 149048\n",
      "Step 3712/55081 | Loss: 4.994 | Norm: 0.118 | LR: 6.0462e-04 | Tokens/s: 149176\n",
      "Step 3720/55081 | Loss: 5.040 | Norm: 0.114 | LR: 6.0310e-04 | Tokens/s: 149524\n",
      "Step 3728/55081 | Loss: 5.001 | Norm: 0.122 | LR: 6.0158e-04 | Tokens/s: 149188\n",
      "Step 3736/55081 | Loss: 5.030 | Norm: 0.111 | LR: 6.0007e-04 | Tokens/s: 148931\n",
      "Step 3744/55081 | Loss: 4.999 | Norm: 0.118 | LR: 5.9855e-04 | Tokens/s: 148768\n",
      "Step 3752/55081 | Loss: 5.008 | Norm: 0.085 | LR: 5.9703e-04 | Tokens/s: 149563\n",
      "Step 3760/55081 | Loss: 5.016 | Norm: 0.092 | LR: 5.9551e-04 | Tokens/s: 149567\n",
      "Step 3768/55081 | Loss: 5.019 | Norm: 0.089 | LR: 5.9398e-04 | Tokens/s: 147673\n",
      "Step 3776/55081 | Loss: 5.023 | Norm: 0.092 | LR: 5.9246e-04 | Tokens/s: 150134\n",
      "Step 3784/55081 | Loss: 5.015 | Norm: 0.124 | LR: 5.9094e-04 | Tokens/s: 149484\n",
      "Step 3792/55081 | Loss: 5.291 | Norm: 0.223 | LR: 5.8941e-04 | Tokens/s: 149962\n",
      "Step 3800/55081 | Loss: 5.269 | Norm: 0.081 | LR: 5.8789e-04 | Tokens/s: 149426\n",
      "Step 3808/55081 | Loss: 5.048 | Norm: 0.099 | LR: 5.8636e-04 | Tokens/s: 148875\n",
      "Step 3816/55081 | Loss: 5.002 | Norm: 0.091 | LR: 5.8484e-04 | Tokens/s: 149122\n",
      "Step 3824/55081 | Loss: 5.069 | Norm: 0.078 | LR: 5.8331e-04 | Tokens/s: 149339\n",
      "Step 3832/55081 | Loss: 4.983 | Norm: 0.120 | LR: 5.8178e-04 | Tokens/s: 149247\n",
      "Step 3840/55081 | Loss: 4.993 | Norm: 0.103 | LR: 5.8026e-04 | Tokens/s: 149166\n",
      "Step 3848/55081 | Loss: 5.055 | Norm: 0.088 | LR: 5.7873e-04 | Tokens/s: 149535\n",
      "Step 3856/55081 | Loss: 5.009 | Norm: 0.106 | LR: 5.7720e-04 | Tokens/s: 149529\n",
      "Step 3864/55081 | Loss: 5.059 | Norm: 0.157 | LR: 5.7567e-04 | Tokens/s: 150132\n",
      "Step 3872/55081 | Loss: 5.022 | Norm: 0.082 | LR: 5.7414e-04 | Tokens/s: 149050\n",
      "Step 3880/55081 | Loss: 5.021 | Norm: 0.069 | LR: 5.7261e-04 | Tokens/s: 149272\n",
      "Step 3888/55081 | Loss: 4.982 | Norm: 0.083 | LR: 5.7108e-04 | Tokens/s: 149267\n",
      "Step 3896/55081 | Loss: 5.000 | Norm: 0.065 | LR: 5.6955e-04 | Tokens/s: 150687\n",
      "Step 3904/55081 | Loss: 4.963 | Norm: 0.074 | LR: 5.6802e-04 | Tokens/s: 148825\n",
      "Step 3912/55081 | Loss: 5.057 | Norm: 0.075 | LR: 5.6648e-04 | Tokens/s: 150269\n",
      "Step 3920/55081 | Loss: 5.011 | Norm: 0.091 | LR: 5.6495e-04 | Tokens/s: 149231\n",
      "Step 3928/55081 | Loss: 4.963 | Norm: 0.088 | LR: 5.6342e-04 | Tokens/s: 148990\n",
      "Step 3936/55081 | Loss: 5.075 | Norm: 0.112 | LR: 5.6188e-04 | Tokens/s: 148858\n",
      "Step 3944/55081 | Loss: 5.029 | Norm: 0.064 | LR: 5.6035e-04 | Tokens/s: 149647\n",
      "Step 3952/55081 | Loss: 4.998 | Norm: 0.072 | LR: 5.5882e-04 | Tokens/s: 148682\n",
      "Step 3960/55081 | Loss: 5.088 | Norm: 0.138 | LR: 5.5728e-04 | Tokens/s: 149380\n",
      "Step 3968/55081 | Loss: 4.954 | Norm: 0.095 | LR: 5.5575e-04 | Tokens/s: 149561\n",
      "Step 3976/55081 | Loss: 5.135 | Norm: 0.093 | LR: 5.5421e-04 | Tokens/s: 148853\n",
      "Step 3984/55081 | Loss: 4.993 | Norm: 0.089 | LR: 5.5268e-04 | Tokens/s: 148570\n",
      "Step 3992/55081 | Loss: 4.983 | Norm: 0.070 | LR: 5.5114e-04 | Tokens/s: 149395\n",
      "Step 4000/55081 | Loss: 5.004 | Norm: 0.076 | LR: 5.4960e-04 | Tokens/s: 149223\n",
      "Epoch 4000 | Training checkpoint saved at models/tinyllm/checkpoint_4000.pt\n",
      "Step 4008/55081 | Loss: 5.264 | Norm: 0.351 | LR: 5.4807e-04 | Tokens/s: 148685\n",
      "Step 4016/55081 | Loss: 5.017 | Norm: 0.086 | LR: 5.4653e-04 | Tokens/s: 149840\n",
      "Step 4024/55081 | Loss: 5.098 | Norm: 0.154 | LR: 5.4499e-04 | Tokens/s: 148867\n",
      "Step 4032/55081 | Loss: 5.054 | Norm: 0.083 | LR: 5.4346e-04 | Tokens/s: 149300\n",
      "Step 4040/55081 | Loss: 5.200 | Norm: 0.079 | LR: 5.4192e-04 | Tokens/s: 149212\n",
      "Step 4048/55081 | Loss: 5.408 | Norm: 0.220 | LR: 5.4038e-04 | Tokens/s: 150250\n",
      "Step 4056/55081 | Loss: 5.395 | Norm: 0.203 | LR: 5.3884e-04 | Tokens/s: 148703\n",
      "Step 4064/55081 | Loss: 5.366 | Norm: 0.220 | LR: 5.3731e-04 | Tokens/s: 149588\n",
      "Step 4072/55081 | Loss: 5.338 | Norm: 0.231 | LR: 5.3577e-04 | Tokens/s: 148939\n",
      "Step 4080/55081 | Loss: 5.203 | Norm: 0.237 | LR: 5.3423e-04 | Tokens/s: 149823\n",
      "Step 4088/55081 | Loss: 5.174 | Norm: 0.228 | LR: 5.3269e-04 | Tokens/s: 149141\n",
      "Step 4096/55081 | Loss: 5.103 | Norm: 0.228 | LR: 5.3115e-04 | Tokens/s: 149459\n",
      "Step 4104/55081 | Loss: 5.110 | Norm: 0.216 | LR: 5.2962e-04 | Tokens/s: 149246\n",
      "Step 4112/55081 | Loss: 5.127 | Norm: 0.212 | LR: 5.2808e-04 | Tokens/s: 148930\n",
      "Step 4120/55081 | Loss: 5.088 | Norm: 0.191 | LR: 5.2654e-04 | Tokens/s: 148508\n",
      "Step 4128/55081 | Loss: 5.028 | Norm: 0.190 | LR: 5.2500e-04 | Tokens/s: 148967\n",
      "Step 4136/55081 | Loss: 5.069 | Norm: 0.154 | LR: 5.2346e-04 | Tokens/s: 149081\n",
      "Step 4144/55081 | Loss: 5.144 | Norm: 0.121 | LR: 5.2192e-04 | Tokens/s: 149490\n",
      "Step 4152/55081 | Loss: 5.307 | Norm: 0.162 | LR: 5.2038e-04 | Tokens/s: 148682\n",
      "Step 4160/55081 | Loss: 5.229 | Norm: 0.227 | LR: 5.1885e-04 | Tokens/s: 147575\n",
      "Step 4168/55081 | Loss: 5.166 | Norm: 0.106 | LR: 5.1731e-04 | Tokens/s: 149789\n",
      "Step 4176/55081 | Loss: 5.164 | Norm: 0.137 | LR: 5.1577e-04 | Tokens/s: 149071\n",
      "Step 4184/55081 | Loss: 5.140 | Norm: 0.132 | LR: 5.1423e-04 | Tokens/s: 149901\n",
      "Step 4192/55081 | Loss: 5.169 | Norm: 0.123 | LR: 5.1269e-04 | Tokens/s: 149059\n",
      "Step 4200/55081 | Loss: 5.114 | Norm: 0.130 | LR: 5.1116e-04 | Tokens/s: 148280\n",
      "Step 4208/55081 | Loss: 5.359 | Norm: 0.297 | LR: 5.0962e-04 | Tokens/s: 148868\n",
      "Step 4216/55081 | Loss: 5.138 | Norm: 0.147 | LR: 5.0808e-04 | Tokens/s: 148806\n",
      "Step 4224/55081 | Loss: 5.068 | Norm: 0.141 | LR: 5.0654e-04 | Tokens/s: 148920\n",
      "Step 4232/55081 | Loss: 5.139 | Norm: 0.187 | LR: 5.0501e-04 | Tokens/s: 148657\n",
      "Step 4240/55081 | Loss: 5.061 | Norm: 0.118 | LR: 5.0347e-04 | Tokens/s: 148827\n",
      "Step 4248/55081 | Loss: 5.069 | Norm: 0.113 | LR: 5.0193e-04 | Tokens/s: 149327\n",
      "Step 4256/55081 | Loss: 5.071 | Norm: 0.104 | LR: 5.0040e-04 | Tokens/s: 149136\n",
      "Step 4264/55081 | Loss: 5.096 | Norm: 0.100 | LR: 4.9886e-04 | Tokens/s: 149300\n",
      "Step 4272/55081 | Loss: 5.022 | Norm: 0.096 | LR: 4.9732e-04 | Tokens/s: 149137\n",
      "Step 4280/55081 | Loss: 5.001 | Norm: 0.112 | LR: 4.9579e-04 | Tokens/s: 149234\n",
      "Step 4288/55081 | Loss: 5.029 | Norm: 0.100 | LR: 4.9425e-04 | Tokens/s: 148607\n",
      "Step 4296/55081 | Loss: 5.013 | Norm: 0.081 | LR: 4.9272e-04 | Tokens/s: 149417\n",
      "Step 4304/55081 | Loss: 5.084 | Norm: 0.101 | LR: 4.9118e-04 | Tokens/s: 149953\n",
      "Step 4312/55081 | Loss: 4.964 | Norm: 0.096 | LR: 4.8965e-04 | Tokens/s: 148441\n",
      "Step 4320/55081 | Loss: 5.018 | Norm: 0.080 | LR: 4.8812e-04 | Tokens/s: 149421\n",
      "Step 4328/55081 | Loss: 5.046 | Norm: 0.118 | LR: 4.8658e-04 | Tokens/s: 148648\n",
      "Step 4336/55081 | Loss: 5.027 | Norm: 0.103 | LR: 4.8505e-04 | Tokens/s: 149555\n",
      "Step 4344/55081 | Loss: 4.999 | Norm: 0.095 | LR: 4.8352e-04 | Tokens/s: 149295\n",
      "Step 4352/55081 | Loss: 5.002 | Norm: 0.093 | LR: 4.8198e-04 | Tokens/s: 148979\n",
      "Step 4360/55081 | Loss: 5.078 | Norm: 0.076 | LR: 4.8045e-04 | Tokens/s: 149066\n",
      "Step 4368/55081 | Loss: 5.087 | Norm: 0.091 | LR: 4.7892e-04 | Tokens/s: 149422\n",
      "Step 4376/55081 | Loss: 4.979 | Norm: 0.079 | LR: 4.7739e-04 | Tokens/s: 149342\n",
      "Step 4384/55081 | Loss: 4.942 | Norm: 0.103 | LR: 4.7586e-04 | Tokens/s: 148696\n",
      "Step 4392/55081 | Loss: 5.270 | Norm: 0.093 | LR: 4.7433e-04 | Tokens/s: 148792\n",
      "Step 4400/55081 | Loss: 4.938 | Norm: 0.072 | LR: 4.7280e-04 | Tokens/s: 148805\n",
      "Step 4408/55081 | Loss: 5.359 | Norm: 0.070 | LR: 4.7127e-04 | Tokens/s: 150457\n",
      "Step 4416/55081 | Loss: 5.074 | Norm: 0.099 | LR: 4.6974e-04 | Tokens/s: 149421\n",
      "Step 4424/55081 | Loss: 5.041 | Norm: 0.078 | LR: 4.6821e-04 | Tokens/s: 150064\n",
      "Step 4432/55081 | Loss: 5.134 | Norm: 0.181 | LR: 4.6669e-04 | Tokens/s: 148814\n",
      "Step 4440/55081 | Loss: 5.197 | Norm: 0.129 | LR: 4.6516e-04 | Tokens/s: 149191\n",
      "Step 4448/55081 | Loss: 5.016 | Norm: 0.073 | LR: 4.6364e-04 | Tokens/s: 149033\n",
      "Step 4456/55081 | Loss: 4.927 | Norm: 0.076 | LR: 4.6211e-04 | Tokens/s: 148453\n",
      "Step 4464/55081 | Loss: 5.019 | Norm: 0.070 | LR: 4.6059e-04 | Tokens/s: 148767\n",
      "Step 4472/55081 | Loss: 4.917 | Norm: 0.104 | LR: 4.5906e-04 | Tokens/s: 148388\n",
      "Step 4480/55081 | Loss: 5.309 | Norm: 0.164 | LR: 4.5754e-04 | Tokens/s: 150450\n",
      "Step 4488/55081 | Loss: 5.300 | Norm: 0.198 | LR: 4.5602e-04 | Tokens/s: 65391\n",
      "Step 4496/55081 | Loss: 4.975 | Norm: 0.087 | LR: 4.5449e-04 | Tokens/s: 148999\n",
      "Epoch 4500 | Training checkpoint saved at models/tinyllm/checkpoint_4500.pt\n",
      "Step 4504/55081 | Loss: 5.067 | Norm: 0.100 | LR: 4.5297e-04 | Tokens/s: 149538\n",
      "Step 4512/55081 | Loss: 5.036 | Norm: 0.153 | LR: 4.5145e-04 | Tokens/s: 148987\n",
      "Step 4520/55081 | Loss: 5.034 | Norm: 0.081 | LR: 4.4993e-04 | Tokens/s: 149930\n",
      "Step 4528/55081 | Loss: 4.895 | Norm: 0.146 | LR: 4.4842e-04 | Tokens/s: 149291\n",
      "Step 4536/55081 | Loss: 4.881 | Norm: 0.095 | LR: 4.4690e-04 | Tokens/s: 149112\n",
      "Step 4544/55081 | Loss: 5.078 | Norm: 0.116 | LR: 4.4538e-04 | Tokens/s: 148158\n",
      "Step 4552/55081 | Loss: 4.951 | Norm: 0.134 | LR: 4.4386e-04 | Tokens/s: 148845\n",
      "Step 4560/55081 | Loss: 4.891 | Norm: 0.083 | LR: 4.4235e-04 | Tokens/s: 148873\n",
      "Step 4568/55081 | Loss: 4.954 | Norm: 0.161 | LR: 4.4083e-04 | Tokens/s: 148992\n",
      "Step 4576/55081 | Loss: 4.978 | Norm: 0.069 | LR: 4.3932e-04 | Tokens/s: 150058\n",
      "Step 4584/55081 | Loss: 4.972 | Norm: 0.085 | LR: 4.3781e-04 | Tokens/s: 149162\n",
      "Step 4592/55081 | Loss: 4.948 | Norm: 0.066 | LR: 4.3630e-04 | Tokens/s: 149384\n",
      "Step 4600/55081 | Loss: 5.071 | Norm: 0.265 | LR: 4.3479e-04 | Tokens/s: 148980\n",
      "Step 4608/55081 | Loss: 5.239 | Norm: 0.118 | LR: 4.3328e-04 | Tokens/s: 149367\n",
      "Step 4616/55081 | Loss: 4.965 | Norm: 0.090 | LR: 4.3177e-04 | Tokens/s: 149200\n",
      "Step 4624/55081 | Loss: 5.175 | Norm: 0.109 | LR: 4.3026e-04 | Tokens/s: 148930\n",
      "Step 4632/55081 | Loss: 5.025 | Norm: 0.186 | LR: 4.2875e-04 | Tokens/s: 148989\n",
      "Step 4640/55081 | Loss: 4.926 | Norm: 0.089 | LR: 4.2725e-04 | Tokens/s: 149571\n",
      "Step 4648/55081 | Loss: 5.011 | Norm: 0.082 | LR: 4.2574e-04 | Tokens/s: 149282\n",
      "Step 4656/55081 | Loss: 5.095 | Norm: 0.102 | LR: 4.2424e-04 | Tokens/s: 148513\n",
      "Step 4664/55081 | Loss: 5.127 | Norm: 0.066 | LR: 4.2273e-04 | Tokens/s: 149646\n",
      "Step 4672/55081 | Loss: 5.091 | Norm: 0.078 | LR: 4.2123e-04 | Tokens/s: 149112\n",
      "Step 4680/55081 | Loss: 5.039 | Norm: 0.179 | LR: 4.1973e-04 | Tokens/s: 150222\n",
      "Step 4688/55081 | Loss: 5.129 | Norm: 0.142 | LR: 4.1823e-04 | Tokens/s: 148692\n",
      "Step 4696/55081 | Loss: 4.990 | Norm: 0.116 | LR: 4.1673e-04 | Tokens/s: 150006\n",
      "Step 4704/55081 | Loss: 4.996 | Norm: 0.076 | LR: 4.1524e-04 | Tokens/s: 148777\n",
      "Step 4712/55081 | Loss: 5.015 | Norm: 0.093 | LR: 4.1374e-04 | Tokens/s: 148851\n",
      "Step 4720/55081 | Loss: 5.330 | Norm: 0.107 | LR: 4.1224e-04 | Tokens/s: 149274\n",
      "Step 4728/55081 | Loss: 5.072 | Norm: 0.090 | LR: 4.1075e-04 | Tokens/s: 149726\n",
      "Step 4736/55081 | Loss: 5.032 | Norm: 0.082 | LR: 4.0926e-04 | Tokens/s: 149295\n",
      "Step 4744/55081 | Loss: 4.966 | Norm: 0.093 | LR: 4.0777e-04 | Tokens/s: 148853\n",
      "Step 4752/55081 | Loss: 5.004 | Norm: 0.082 | LR: 4.0628e-04 | Tokens/s: 149108\n",
      "Step 4760/55081 | Loss: 5.051 | Norm: 0.098 | LR: 4.0479e-04 | Tokens/s: 149040\n",
      "Step 4768/55081 | Loss: 5.053 | Norm: 0.077 | LR: 4.0330e-04 | Tokens/s: 149337\n",
      "Step 4776/55081 | Loss: 5.037 | Norm: 0.111 | LR: 4.0181e-04 | Tokens/s: 149265\n",
      "Step 4784/55081 | Loss: 5.068 | Norm: 0.080 | LR: 4.0033e-04 | Tokens/s: 148130\n",
      "Step 4792/55081 | Loss: 5.430 | Norm: 0.281 | LR: 3.9884e-04 | Tokens/s: 149226\n",
      "Step 4800/55081 | Loss: 5.714 | Norm: 0.616 | LR: 3.9736e-04 | Tokens/s: 150036\n",
      "Step 4808/55081 | Loss: 5.123 | Norm: 0.114 | LR: 3.9588e-04 | Tokens/s: 148210\n",
      "Step 4816/55081 | Loss: 5.081 | Norm: 0.135 | LR: 3.9440e-04 | Tokens/s: 150101\n",
      "Step 4824/55081 | Loss: 5.002 | Norm: 0.096 | LR: 3.9292e-04 | Tokens/s: 149165\n",
      "Step 4832/55081 | Loss: 5.077 | Norm: 0.075 | LR: 3.9145e-04 | Tokens/s: 148784\n",
      "Step 4840/55081 | Loss: 5.137 | Norm: 0.120 | LR: 3.8997e-04 | Tokens/s: 149003\n",
      "Step 4848/55081 | Loss: 4.992 | Norm: 0.110 | LR: 3.8850e-04 | Tokens/s: 148382\n",
      "Step 4856/55081 | Loss: 4.929 | Norm: 0.087 | LR: 3.8702e-04 | Tokens/s: 148943\n",
      "Step 4864/55081 | Loss: 5.040 | Norm: 0.099 | LR: 3.8555e-04 | Tokens/s: 149053\n",
      "Step 4872/55081 | Loss: 4.948 | Norm: 0.086 | LR: 3.8408e-04 | Tokens/s: 149401\n",
      "Step 4880/55081 | Loss: 4.962 | Norm: 0.096 | LR: 3.8261e-04 | Tokens/s: 149087\n",
      "Step 4888/55081 | Loss: 5.010 | Norm: 0.107 | LR: 3.8115e-04 | Tokens/s: 149943\n",
      "Step 4896/55081 | Loss: 5.004 | Norm: 0.090 | LR: 3.7968e-04 | Tokens/s: 149446\n",
      "Step 4904/55081 | Loss: 5.072 | Norm: 0.078 | LR: 3.7822e-04 | Tokens/s: 148891\n",
      "Step 4912/55081 | Loss: 4.967 | Norm: 0.094 | LR: 3.7675e-04 | Tokens/s: 149316\n",
      "Step 4920/55081 | Loss: 4.932 | Norm: 0.075 | LR: 3.7529e-04 | Tokens/s: 149620\n",
      "Step 4928/55081 | Loss: 5.051 | Norm: 0.224 | LR: 3.7383e-04 | Tokens/s: 149152\n",
      "Step 4936/55081 | Loss: 4.886 | Norm: 0.090 | LR: 3.7238e-04 | Tokens/s: 149567\n",
      "Step 4944/55081 | Loss: 5.091 | Norm: 0.299 | LR: 3.7092e-04 | Tokens/s: 148638\n",
      "Step 4952/55081 | Loss: 5.052 | Norm: 0.098 | LR: 3.6947e-04 | Tokens/s: 148417\n",
      "Step 4960/55081 | Loss: 4.963 | Norm: 0.096 | LR: 3.6801e-04 | Tokens/s: 149139\n",
      "Step 4968/55081 | Loss: 4.994 | Norm: 0.092 | LR: 3.6656e-04 | Tokens/s: 149171\n",
      "Step 4976/55081 | Loss: 4.974 | Norm: 0.080 | LR: 3.6511e-04 | Tokens/s: 148944\n",
      "Step 4984/55081 | Loss: 4.971 | Norm: 0.083 | LR: 3.6367e-04 | Tokens/s: 149008\n",
      "Step 4992/55081 | Loss: 5.082 | Norm: 0.095 | LR: 3.6222e-04 | Tokens/s: 149655\n",
      "Step 5000/55081 | Loss: 4.992 | Norm: 0.085 | LR: 3.6077e-04 | Tokens/s: 148750\n",
      "Epoch 5000 | Training checkpoint saved at models/tinyllm/checkpoint_5000.pt\n",
      "Step 5008/55081 | Loss: 4.929 | Norm: 0.078 | LR: 3.5933e-04 | Tokens/s: 148740\n",
      "Step 5016/55081 | Loss: 5.099 | Norm: 0.098 | LR: 3.5789e-04 | Tokens/s: 149292\n",
      "Step 5024/55081 | Loss: 4.928 | Norm: 0.075 | LR: 3.5645e-04 | Tokens/s: 148513\n",
      "Step 5032/55081 | Loss: 5.104 | Norm: 0.111 | LR: 3.5501e-04 | Tokens/s: 149274\n",
      "Step 5040/55081 | Loss: 5.161 | Norm: 0.228 | LR: 3.5358e-04 | Tokens/s: 149817\n",
      "Step 5048/55081 | Loss: 5.134 | Norm: 0.087 | LR: 3.5215e-04 | Tokens/s: 149091\n",
      "Step 5056/55081 | Loss: 4.969 | Norm: 0.103 | LR: 3.5071e-04 | Tokens/s: 150608\n",
      "Step 5064/55081 | Loss: 5.012 | Norm: 0.068 | LR: 3.4928e-04 | Tokens/s: 149304\n",
      "Step 5072/55081 | Loss: 5.040 | Norm: 0.136 | LR: 3.4785e-04 | Tokens/s: 149609\n",
      "Step 5080/55081 | Loss: 4.984 | Norm: 0.118 | LR: 3.4643e-04 | Tokens/s: 149089\n",
      "Step 5088/55081 | Loss: 4.999 | Norm: 0.073 | LR: 3.4500e-04 | Tokens/s: 148698\n",
      "Step 5096/55081 | Loss: 4.931 | Norm: 0.070 | LR: 3.4358e-04 | Tokens/s: 148320\n",
      "Step 5104/55081 | Loss: 4.914 | Norm: 0.078 | LR: 3.4216e-04 | Tokens/s: 150317\n",
      "Step 5112/55081 | Loss: 4.989 | Norm: 0.117 | LR: 3.4074e-04 | Tokens/s: 149143\n",
      "Step 5120/55081 | Loss: 4.966 | Norm: 0.070 | LR: 3.3932e-04 | Tokens/s: 149616\n",
      "Step 5128/55081 | Loss: 5.001 | Norm: 0.080 | LR: 3.3791e-04 | Tokens/s: 148828\n",
      "Step 5136/55081 | Loss: 4.956 | Norm: 0.070 | LR: 3.3650e-04 | Tokens/s: 148316\n",
      "Step 5144/55081 | Loss: 5.087 | Norm: 0.070 | LR: 3.3508e-04 | Tokens/s: 153810\n",
      "Step 5152/55081 | Loss: 5.015 | Norm: 0.063 | LR: 3.3368e-04 | Tokens/s: 153348\n",
      "Step 5160/55081 | Loss: 5.068 | Norm: 0.082 | LR: 3.3227e-04 | Tokens/s: 153484\n",
      "Step 5168/55081 | Loss: 5.056 | Norm: 0.244 | LR: 3.3086e-04 | Tokens/s: 153596\n",
      "Step 5176/55081 | Loss: 4.999 | Norm: 0.088 | LR: 3.2946e-04 | Tokens/s: 153503\n",
      "Step 5184/55081 | Loss: 5.008 | Norm: 0.079 | LR: 3.2806e-04 | Tokens/s: 152890\n",
      "Step 5192/55081 | Loss: 4.991 | Norm: 0.072 | LR: 3.2666e-04 | Tokens/s: 153385\n",
      "Step 5200/55081 | Loss: 5.057 | Norm: 0.066 | LR: 3.2526e-04 | Tokens/s: 152715\n",
      "Step 5208/55081 | Loss: 4.974 | Norm: 0.070 | LR: 3.2387e-04 | Tokens/s: 153479\n",
      "Step 5216/55081 | Loss: 5.065 | Norm: 0.090 | LR: 3.2248e-04 | Tokens/s: 152503\n",
      "Step 5224/55081 | Loss: 4.941 | Norm: 0.085 | LR: 3.2109e-04 | Tokens/s: 153561\n",
      "Step 5232/55081 | Loss: 5.226 | Norm: 0.159 | LR: 3.1970e-04 | Tokens/s: 153666\n",
      "Step 5240/55081 | Loss: 4.992 | Norm: 0.073 | LR: 3.1831e-04 | Tokens/s: 153658\n",
      "Step 5248/55081 | Loss: 4.945 | Norm: 0.118 | LR: 3.1693e-04 | Tokens/s: 153735\n",
      "Step 5256/55081 | Loss: 4.971 | Norm: 0.102 | LR: 3.1555e-04 | Tokens/s: 153588\n",
      "Step 5264/55081 | Loss: 4.962 | Norm: 0.084 | LR: 3.1417e-04 | Tokens/s: 153185\n",
      "Step 5272/55081 | Loss: 4.985 | Norm: 0.065 | LR: 3.1279e-04 | Tokens/s: 153488\n",
      "Step 5280/55081 | Loss: 4.953 | Norm: 0.114 | LR: 3.1141e-04 | Tokens/s: 153463\n",
      "Step 5288/55081 | Loss: 5.361 | Norm: 0.091 | LR: 3.1004e-04 | Tokens/s: 152700\n",
      "Step 5296/55081 | Loss: 5.006 | Norm: 0.078 | LR: 3.0867e-04 | Tokens/s: 153006\n",
      "Step 5304/55081 | Loss: 5.074 | Norm: 0.063 | LR: 3.0730e-04 | Tokens/s: 152508\n",
      "Step 5312/55081 | Loss: 4.969 | Norm: 0.132 | LR: 3.0593e-04 | Tokens/s: 153106\n",
      "Step 5320/55081 | Loss: 5.075 | Norm: 0.079 | LR: 3.0457e-04 | Tokens/s: 152928\n",
      "Step 5328/55081 | Loss: 4.911 | Norm: 0.098 | LR: 3.0321e-04 | Tokens/s: 153703\n",
      "Step 5336/55081 | Loss: 5.043 | Norm: 0.134 | LR: 3.0185e-04 | Tokens/s: 153930\n",
      "Step 5344/55081 | Loss: 4.937 | Norm: 0.090 | LR: 3.0049e-04 | Tokens/s: 153249\n",
      "Step 5352/55081 | Loss: 4.974 | Norm: 0.086 | LR: 2.9914e-04 | Tokens/s: 153875\n",
      "Step 5360/55081 | Loss: 4.955 | Norm: 0.088 | LR: 2.9779e-04 | Tokens/s: 152500\n",
      "Step 5368/55081 | Loss: 4.931 | Norm: 0.099 | LR: 2.9644e-04 | Tokens/s: 154102\n",
      "Step 5376/55081 | Loss: 4.908 | Norm: 0.072 | LR: 2.9509e-04 | Tokens/s: 153900\n",
      "Step 5384/55081 | Loss: 4.977 | Norm: 0.068 | LR: 2.9374e-04 | Tokens/s: 152742\n",
      "Step 5392/55081 | Loss: 5.107 | Norm: 0.095 | LR: 2.9240e-04 | Tokens/s: 153944\n",
      "Step 5400/55081 | Loss: 4.943 | Norm: 0.092 | LR: 2.9106e-04 | Tokens/s: 152511\n",
      "Step 5408/55081 | Loss: 4.967 | Norm: 0.070 | LR: 2.8972e-04 | Tokens/s: 153835\n",
      "Step 5416/55081 | Loss: 5.160 | Norm: 0.067 | LR: 2.8839e-04 | Tokens/s: 152740\n",
      "Step 5424/55081 | Loss: 4.879 | Norm: 0.077 | LR: 2.8706e-04 | Tokens/s: 153200\n",
      "Step 5432/55081 | Loss: 4.942 | Norm: 0.075 | LR: 2.8573e-04 | Tokens/s: 152318\n",
      "Step 5440/55081 | Loss: 4.987 | Norm: 0.074 | LR: 2.8440e-04 | Tokens/s: 153515\n",
      "Step 5448/55081 | Loss: 4.984 | Norm: 0.080 | LR: 2.8307e-04 | Tokens/s: 152918\n",
      "Step 5456/55081 | Loss: 4.968 | Norm: 0.061 | LR: 2.8175e-04 | Tokens/s: 153556\n",
      "Step 5464/55081 | Loss: 5.026 | Norm: 0.105 | LR: 2.8043e-04 | Tokens/s: 153378\n",
      "Step 5472/55081 | Loss: 4.952 | Norm: 0.074 | LR: 2.7911e-04 | Tokens/s: 153297\n",
      "Step 5480/55081 | Loss: 4.920 | Norm: 0.094 | LR: 2.7780e-04 | Tokens/s: 153490\n",
      "Step 5488/55081 | Loss: 4.985 | Norm: 0.095 | LR: 2.7649e-04 | Tokens/s: 153076\n",
      "Step 5496/55081 | Loss: 5.039 | Norm: 0.070 | LR: 2.7518e-04 | Tokens/s: 153905\n",
      "Epoch 5500 | Training checkpoint saved at models/tinyllm/checkpoint_5500.pt\n",
      "Step 5504/55081 | Loss: 4.952 | Norm: 0.085 | LR: 2.7387e-04 | Tokens/s: 152405\n",
      "Step 5512/55081 | Loss: 4.990 | Norm: 0.063 | LR: 2.7256e-04 | Tokens/s: 153780\n",
      "Step 5520/55081 | Loss: 4.956 | Norm: 0.070 | LR: 2.7126e-04 | Tokens/s: 153499\n",
      "Step 5528/55081 | Loss: 5.029 | Norm: 0.077 | LR: 2.6996e-04 | Tokens/s: 153692\n",
      "Step 5536/55081 | Loss: 4.931 | Norm: 0.080 | LR: 2.6867e-04 | Tokens/s: 153466\n",
      "Step 5544/55081 | Loss: 4.964 | Norm: 0.067 | LR: 2.6737e-04 | Tokens/s: 153000\n",
      "Step 5552/55081 | Loss: 4.990 | Norm: 0.071 | LR: 2.6608e-04 | Tokens/s: 153320\n",
      "Step 5560/55081 | Loss: 5.028 | Norm: 0.081 | LR: 2.6479e-04 | Tokens/s: 153619\n",
      "Step 5568/55081 | Loss: 4.968 | Norm: 0.062 | LR: 2.6351e-04 | Tokens/s: 153498\n",
      "Step 5576/55081 | Loss: 4.901 | Norm: 0.089 | LR: 2.6222e-04 | Tokens/s: 153520\n",
      "Step 5584/55081 | Loss: 5.025 | Norm: 0.064 | LR: 2.6094e-04 | Tokens/s: 153088\n",
      "Step 5592/55081 | Loss: 5.075 | Norm: 0.076 | LR: 2.5967e-04 | Tokens/s: 153752\n",
      "Step 5600/55081 | Loss: 5.092 | Norm: 0.064 | LR: 2.5839e-04 | Tokens/s: 153515\n",
      "Step 5608/55081 | Loss: 4.913 | Norm: 0.072 | LR: 2.5712e-04 | Tokens/s: 153891\n",
      "Step 5616/55081 | Loss: 4.907 | Norm: 0.059 | LR: 2.5585e-04 | Tokens/s: 152540\n",
      "Step 5624/55081 | Loss: 4.981 | Norm: 0.069 | LR: 2.5459e-04 | Tokens/s: 153574\n",
      "Step 5632/55081 | Loss: 4.898 | Norm: 0.067 | LR: 2.5332e-04 | Tokens/s: 152361\n",
      "Step 5640/55081 | Loss: 4.927 | Norm: 0.083 | LR: 2.5206e-04 | Tokens/s: 153377\n",
      "Step 5648/55081 | Loss: 4.980 | Norm: 0.110 | LR: 2.5080e-04 | Tokens/s: 152588\n",
      "Step 5656/55081 | Loss: 4.928 | Norm: 0.070 | LR: 2.4955e-04 | Tokens/s: 153311\n",
      "Step 5664/55081 | Loss: 4.830 | Norm: 0.105 | LR: 2.4830e-04 | Tokens/s: 153136\n",
      "Step 5672/55081 | Loss: 4.986 | Norm: 0.117 | LR: 2.4705e-04 | Tokens/s: 153544\n",
      "Step 5680/55081 | Loss: 5.859 | Norm: 0.353 | LR: 2.4580e-04 | Tokens/s: 154032\n",
      "Step 5688/55081 | Loss: 5.018 | Norm: 0.094 | LR: 2.4456e-04 | Tokens/s: 153301\n",
      "Step 5696/55081 | Loss: 5.060 | Norm: 0.111 | LR: 2.4332e-04 | Tokens/s: 153831\n",
      "Step 5704/55081 | Loss: 5.175 | Norm: 0.074 | LR: 2.4208e-04 | Tokens/s: 153660\n",
      "Step 5712/55081 | Loss: 5.107 | Norm: 0.078 | LR: 2.4085e-04 | Tokens/s: 153603\n",
      "Step 5720/55081 | Loss: 5.083 | Norm: 0.087 | LR: 2.3962e-04 | Tokens/s: 153411\n",
      "Step 5728/55081 | Loss: 5.043 | Norm: 0.085 | LR: 2.3839e-04 | Tokens/s: 153413\n",
      "Step 5736/55081 | Loss: 5.262 | Norm: 0.113 | LR: 2.3716e-04 | Tokens/s: 152669\n",
      "Step 5744/55081 | Loss: 5.031 | Norm: 0.078 | LR: 2.3594e-04 | Tokens/s: 152925\n",
      "Step 5752/55081 | Loss: 5.002 | Norm: 0.092 | LR: 2.3472e-04 | Tokens/s: 154194\n",
      "Step 5760/55081 | Loss: 4.979 | Norm: 0.110 | LR: 2.3350e-04 | Tokens/s: 153023\n",
      "Step 5768/55081 | Loss: 5.093 | Norm: 0.112 | LR: 2.3229e-04 | Tokens/s: 154102\n",
      "Step 5776/55081 | Loss: 4.956 | Norm: 0.092 | LR: 2.3108e-04 | Tokens/s: 153055\n",
      "Step 5784/55081 | Loss: 4.886 | Norm: 0.145 | LR: 2.2987e-04 | Tokens/s: 153641\n",
      "Step 5792/55081 | Loss: 4.874 | Norm: 0.067 | LR: 2.2867e-04 | Tokens/s: 153028\n",
      "Step 5800/55081 | Loss: 4.959 | Norm: 0.059 | LR: 2.2747e-04 | Tokens/s: 154137\n",
      "Step 5808/55081 | Loss: 5.248 | Norm: 0.085 | LR: 2.2627e-04 | Tokens/s: 153320\n",
      "Step 5816/55081 | Loss: 5.366 | Norm: 0.348 | LR: 2.2508e-04 | Tokens/s: 153575\n",
      "Step 5824/55081 | Loss: 5.368 | Norm: 0.189 | LR: 2.2389e-04 | Tokens/s: 153361\n",
      "Step 5832/55081 | Loss: 4.960 | Norm: 0.101 | LR: 2.2270e-04 | Tokens/s: 153969\n",
      "Step 5840/55081 | Loss: 4.900 | Norm: 0.084 | LR: 2.2151e-04 | Tokens/s: 153324\n",
      "Step 5848/55081 | Loss: 4.899 | Norm: 0.105 | LR: 2.2033e-04 | Tokens/s: 153942\n",
      "Step 5856/55081 | Loss: 4.983 | Norm: 0.084 | LR: 2.1915e-04 | Tokens/s: 153498\n",
      "Step 5864/55081 | Loss: 4.911 | Norm: 0.081 | LR: 2.1798e-04 | Tokens/s: 152030\n",
      "Step 5872/55081 | Loss: 4.929 | Norm: 0.077 | LR: 2.1680e-04 | Tokens/s: 153546\n",
      "Step 5880/55081 | Loss: 4.964 | Norm: 0.080 | LR: 2.1564e-04 | Tokens/s: 152326\n",
      "Step 5888/55081 | Loss: 4.972 | Norm: 0.061 | LR: 2.1447e-04 | Tokens/s: 153718\n",
      "Step 5896/55081 | Loss: 4.978 | Norm: 0.062 | LR: 2.1331e-04 | Tokens/s: 153461\n",
      "Step 5904/55081 | Loss: 4.941 | Norm: 0.075 | LR: 2.1215e-04 | Tokens/s: 154089\n",
      "Step 5912/55081 | Loss: 4.959 | Norm: 0.075 | LR: 2.1099e-04 | Tokens/s: 152973\n",
      "Step 5920/55081 | Loss: 5.010 | Norm: 0.056 | LR: 2.0984e-04 | Tokens/s: 153728\n",
      "Step 5928/55081 | Loss: 5.039 | Norm: 0.092 | LR: 2.0869e-04 | Tokens/s: 154014\n",
      "Step 5936/55081 | Loss: 4.997 | Norm: 0.070 | LR: 2.0754e-04 | Tokens/s: 153351\n",
      "Step 5944/55081 | Loss: 5.034 | Norm: 0.076 | LR: 2.0640e-04 | Tokens/s: 153503\n",
      "Step 5952/55081 | Loss: 4.940 | Norm: 0.091 | LR: 2.0526e-04 | Tokens/s: 152583\n",
      "Step 5960/55081 | Loss: 4.982 | Norm: 0.079 | LR: 2.0413e-04 | Tokens/s: 153527\n",
      "Step 5968/55081 | Loss: 5.013 | Norm: 0.080 | LR: 2.0299e-04 | Tokens/s: 153452\n",
      "Step 5976/55081 | Loss: 4.920 | Norm: 0.065 | LR: 2.0186e-04 | Tokens/s: 153841\n",
      "Step 5984/55081 | Loss: 4.923 | Norm: 0.075 | LR: 2.0074e-04 | Tokens/s: 153660\n",
      "Step 5992/55081 | Loss: 4.889 | Norm: 0.087 | LR: 1.9962e-04 | Tokens/s: 152688\n",
      "Step 6000/55081 | Loss: 4.892 | Norm: 0.087 | LR: 1.9850e-04 | Tokens/s: 153482\n",
      "Epoch 6000 | Training checkpoint saved at models/tinyllm/checkpoint_6000.pt\n",
      "Step 6008/55081 | Loss: 4.950 | Norm: 0.089 | LR: 1.9738e-04 | Tokens/s: 153749\n",
      "Step 6016/55081 | Loss: 5.078 | Norm: 0.072 | LR: 1.9627e-04 | Tokens/s: 153095\n",
      "Step 6024/55081 | Loss: 5.054 | Norm: 0.184 | LR: 1.9516e-04 | Tokens/s: 153455\n",
      "Step 6032/55081 | Loss: 5.160 | Norm: 0.121 | LR: 1.9405e-04 | Tokens/s: 153502\n",
      "Step 6040/55081 | Loss: 5.084 | Norm: 0.066 | LR: 1.9295e-04 | Tokens/s: 153654\n",
      "Step 6048/55081 | Loss: 4.917 | Norm: 0.101 | LR: 1.9185e-04 | Tokens/s: 153039\n",
      "Step 6056/55081 | Loss: 5.004 | Norm: 0.079 | LR: 1.9076e-04 | Tokens/s: 153558\n",
      "Step 6064/55081 | Loss: 5.030 | Norm: 0.079 | LR: 1.8967e-04 | Tokens/s: 153300\n",
      "Step 6072/55081 | Loss: 5.152 | Norm: 0.258 | LR: 1.8858e-04 | Tokens/s: 153215\n",
      "Step 6080/55081 | Loss: 5.134 | Norm: 0.116 | LR: 1.8750e-04 | Tokens/s: 152744\n",
      "Step 6088/55081 | Loss: 5.205 | Norm: 0.269 | LR: 1.8642e-04 | Tokens/s: 152880\n",
      "Step 6096/55081 | Loss: 5.167 | Norm: 0.176 | LR: 1.8534e-04 | Tokens/s: 153284\n",
      "Step 6104/55081 | Loss: 5.223 | Norm: 0.089 | LR: 1.8426e-04 | Tokens/s: 153134\n",
      "Step 6112/55081 | Loss: 5.223 | Norm: 0.072 | LR: 1.8319e-04 | Tokens/s: 153476\n",
      "Step 6120/55081 | Loss: 4.920 | Norm: 0.095 | LR: 1.8213e-04 | Tokens/s: 152654\n",
      "Step 6128/55081 | Loss: 4.962 | Norm: 0.068 | LR: 1.8107e-04 | Tokens/s: 153749\n",
      "Step 6136/55081 | Loss: 4.826 | Norm: 0.115 | LR: 1.8001e-04 | Tokens/s: 153621\n",
      "Step 6144/55081 | Loss: 4.958 | Norm: 0.087 | LR: 1.7895e-04 | Tokens/s: 153651\n",
      "Step 6152/55081 | Loss: 4.971 | Norm: 0.070 | LR: 1.7790e-04 | Tokens/s: 152134\n",
      "Step 6160/55081 | Loss: 5.046 | Norm: 0.068 | LR: 1.7685e-04 | Tokens/s: 153831\n",
      "Step 6168/55081 | Loss: 4.929 | Norm: 0.071 | LR: 1.7581e-04 | Tokens/s: 153083\n",
      "Step 6176/55081 | Loss: 4.984 | Norm: 0.064 | LR: 1.7476e-04 | Tokens/s: 153575\n",
      "Step 6184/55081 | Loss: 4.975 | Norm: 0.093 | LR: 1.7373e-04 | Tokens/s: 153313\n",
      "Step 6192/55081 | Loss: 5.001 | Norm: 0.066 | LR: 1.7269e-04 | Tokens/s: 152518\n",
      "Step 6200/55081 | Loss: 4.968 | Norm: 0.072 | LR: 1.7166e-04 | Tokens/s: 153636\n",
      "Step 6208/55081 | Loss: 4.991 | Norm: 0.094 | LR: 1.7064e-04 | Tokens/s: 153030\n",
      "Step 6216/55081 | Loss: 5.090 | Norm: 0.160 | LR: 1.6961e-04 | Tokens/s: 153453\n",
      "Step 6224/55081 | Loss: 4.958 | Norm: 0.076 | LR: 1.6860e-04 | Tokens/s: 153567\n",
      "Step 6232/55081 | Loss: 5.060 | Norm: 0.139 | LR: 1.6758e-04 | Tokens/s: 153523\n",
      "Step 6240/55081 | Loss: 4.940 | Norm: 0.081 | LR: 1.6657e-04 | Tokens/s: 153020\n",
      "Step 6248/55081 | Loss: 5.053 | Norm: 0.175 | LR: 1.6556e-04 | Tokens/s: 153457\n",
      "Step 6256/55081 | Loss: 5.041 | Norm: 0.085 | LR: 1.6456e-04 | Tokens/s: 153291\n",
      "Step 6264/55081 | Loss: 4.989 | Norm: 0.064 | LR: 1.6356e-04 | Tokens/s: 153534\n",
      "Step 6272/55081 | Loss: 5.014 | Norm: 0.118 | LR: 1.6256e-04 | Tokens/s: 153883\n",
      "Step 6280/55081 | Loss: 5.031 | Norm: 0.094 | LR: 1.6157e-04 | Tokens/s: 153299\n",
      "Step 6288/55081 | Loss: 5.004 | Norm: 0.070 | LR: 1.6058e-04 | Tokens/s: 153648\n",
      "Step 6296/55081 | Loss: 4.951 | Norm: 0.076 | LR: 1.5960e-04 | Tokens/s: 151687\n",
      "Step 6304/55081 | Loss: 5.054 | Norm: 0.068 | LR: 1.5861e-04 | Tokens/s: 154061\n",
      "Step 6312/55081 | Loss: 4.904 | Norm: 0.073 | LR: 1.5764e-04 | Tokens/s: 152750\n",
      "Step 6320/55081 | Loss: 4.976 | Norm: 0.076 | LR: 1.5666e-04 | Tokens/s: 154183\n",
      "Step 6328/55081 | Loss: 4.988 | Norm: 0.077 | LR: 1.5569e-04 | Tokens/s: 153202\n",
      "Step 6336/55081 | Loss: 5.033 | Norm: 0.071 | LR: 1.5473e-04 | Tokens/s: 153629\n",
      "Step 6344/55081 | Loss: 4.987 | Norm: 0.069 | LR: 1.5377e-04 | Tokens/s: 153385\n",
      "Step 6352/55081 | Loss: 4.920 | Norm: 0.074 | LR: 1.5281e-04 | Tokens/s: 154241\n",
      "Step 6360/55081 | Loss: 4.958 | Norm: 0.071 | LR: 1.5186e-04 | Tokens/s: 153761\n",
      "Step 6368/55081 | Loss: 4.978 | Norm: 0.078 | LR: 1.5091e-04 | Tokens/s: 153879\n",
      "Step 6376/55081 | Loss: 4.903 | Norm: 0.097 | LR: 1.4996e-04 | Tokens/s: 153988\n",
      "Step 6384/55081 | Loss: 4.975 | Norm: 0.132 | LR: 1.4902e-04 | Tokens/s: 153108\n",
      "Step 6392/55081 | Loss: 5.002 | Norm: 0.086 | LR: 1.4808e-04 | Tokens/s: 152776\n",
      "Step 6400/55081 | Loss: 4.986 | Norm: 0.096 | LR: 1.4714e-04 | Tokens/s: 153911\n",
      "Step 6408/55081 | Loss: 5.011 | Norm: 0.068 | LR: 1.4621e-04 | Tokens/s: 153693\n",
      "Step 6416/55081 | Loss: 4.991 | Norm: 0.091 | LR: 1.4529e-04 | Tokens/s: 153481\n",
      "Step 6424/55081 | Loss: 4.972 | Norm: 0.123 | LR: 1.4437e-04 | Tokens/s: 154019\n",
      "Step 6432/55081 | Loss: 4.933 | Norm: 0.072 | LR: 1.4345e-04 | Tokens/s: 153602\n",
      "Step 6440/55081 | Loss: 4.903 | Norm: 0.094 | LR: 1.4253e-04 | Tokens/s: 153017\n",
      "Step 6448/55081 | Loss: 4.998 | Norm: 0.075 | LR: 1.4162e-04 | Tokens/s: 153922\n",
      "Step 6456/55081 | Loss: 4.999 | Norm: 0.081 | LR: 1.4072e-04 | Tokens/s: 152594\n",
      "Step 6464/55081 | Loss: 4.996 | Norm: 0.077 | LR: 1.3981e-04 | Tokens/s: 153860\n",
      "Step 6472/55081 | Loss: 4.908 | Norm: 0.073 | LR: 1.3892e-04 | Tokens/s: 153118\n",
      "Step 6480/55081 | Loss: 4.945 | Norm: 0.071 | LR: 1.3802e-04 | Tokens/s: 153738\n",
      "Step 6488/55081 | Loss: 5.007 | Norm: 0.071 | LR: 1.3713e-04 | Tokens/s: 153773\n",
      "Step 6496/55081 | Loss: 5.002 | Norm: 0.075 | LR: 1.3625e-04 | Tokens/s: 153489\n",
      "Epoch 6500 | Training checkpoint saved at models/tinyllm/checkpoint_6500.pt\n",
      "Step 6504/55081 | Loss: 5.035 | Norm: 0.107 | LR: 1.3536e-04 | Tokens/s: 153258\n",
      "Step 6512/55081 | Loss: 4.992 | Norm: 0.084 | LR: 1.3449e-04 | Tokens/s: 153085\n",
      "Step 6520/55081 | Loss: 5.003 | Norm: 0.067 | LR: 1.3361e-04 | Tokens/s: 153306\n",
      "Step 6528/55081 | Loss: 5.363 | Norm: 0.246 | LR: 1.3274e-04 | Tokens/s: 154187\n",
      "Step 6536/55081 | Loss: 4.958 | Norm: 0.098 | LR: 1.3188e-04 | Tokens/s: 153393\n",
      "Step 6544/55081 | Loss: 4.871 | Norm: 0.071 | LR: 1.3102e-04 | Tokens/s: 152934\n",
      "Step 6552/55081 | Loss: 4.977 | Norm: 0.091 | LR: 1.3016e-04 | Tokens/s: 153297\n",
      "Step 6560/55081 | Loss: 4.911 | Norm: 0.063 | LR: 1.2931e-04 | Tokens/s: 153451\n",
      "Step 6568/55081 | Loss: 4.794 | Norm: 0.102 | LR: 1.2846e-04 | Tokens/s: 153445\n",
      "Step 6576/55081 | Loss: 4.979 | Norm: 0.068 | LR: 1.2761e-04 | Tokens/s: 153368\n",
      "Step 6584/55081 | Loss: 5.074 | Norm: 0.066 | LR: 1.2677e-04 | Tokens/s: 153595\n",
      "Step 6592/55081 | Loss: 5.095 | Norm: 0.228 | LR: 1.2593e-04 | Tokens/s: 153494\n",
      "Step 6600/55081 | Loss: 4.984 | Norm: 0.079 | LR: 1.2510e-04 | Tokens/s: 154102\n",
      "Step 6608/55081 | Loss: 5.027 | Norm: 0.078 | LR: 1.2427e-04 | Tokens/s: 153634\n",
      "Step 6616/55081 | Loss: 4.987 | Norm: 0.080 | LR: 1.2345e-04 | Tokens/s: 153659\n",
      "Step 6624/55081 | Loss: 5.080 | Norm: 0.071 | LR: 1.2263e-04 | Tokens/s: 152641\n",
      "Step 6632/55081 | Loss: 5.109 | Norm: 0.186 | LR: 1.2182e-04 | Tokens/s: 153233\n",
      "Step 6640/55081 | Loss: 4.932 | Norm: 0.062 | LR: 1.2100e-04 | Tokens/s: 152837\n",
      "Step 6648/55081 | Loss: 5.032 | Norm: 0.078 | LR: 1.2020e-04 | Tokens/s: 153003\n",
      "Step 6656/55081 | Loss: 4.940 | Norm: 0.066 | LR: 1.1939e-04 | Tokens/s: 153309\n",
      "Step 6664/55081 | Loss: 5.098 | Norm: 0.097 | LR: 1.1860e-04 | Tokens/s: 153566\n",
      "Step 6672/55081 | Loss: 5.060 | Norm: 0.092 | LR: 1.1780e-04 | Tokens/s: 153635\n",
      "Step 6680/55081 | Loss: 4.961 | Norm: 0.092 | LR: 1.1701e-04 | Tokens/s: 153483\n",
      "Step 6688/55081 | Loss: 5.021 | Norm: 0.069 | LR: 1.1623e-04 | Tokens/s: 152796\n",
      "Step 6696/55081 | Loss: 5.199 | Norm: 0.087 | LR: 1.1544e-04 | Tokens/s: 153499\n",
      "Step 6704/55081 | Loss: 4.965 | Norm: 0.082 | LR: 1.1467e-04 | Tokens/s: 153964\n",
      "Step 6712/55081 | Loss: 5.002 | Norm: 0.102 | LR: 1.1389e-04 | Tokens/s: 152365\n",
      "Step 6720/55081 | Loss: 4.979 | Norm: 0.085 | LR: 1.1313e-04 | Tokens/s: 153903\n",
      "Step 6728/55081 | Loss: 4.972 | Norm: 0.066 | LR: 1.1236e-04 | Tokens/s: 153190\n",
      "Step 6736/55081 | Loss: 4.943 | Norm: 0.072 | LR: 1.1160e-04 | Tokens/s: 153205\n",
      "Step 6744/55081 | Loss: 4.851 | Norm: 0.073 | LR: 1.1085e-04 | Tokens/s: 152929\n",
      "Step 6752/55081 | Loss: 4.965 | Norm: 0.079 | LR: 1.1010e-04 | Tokens/s: 153420\n",
      "Step 6760/55081 | Loss: 5.007 | Norm: 0.126 | LR: 1.0935e-04 | Tokens/s: 153679\n",
      "Step 6768/55081 | Loss: 4.869 | Norm: 0.091 | LR: 1.0861e-04 | Tokens/s: 153376\n",
      "Step 6776/55081 | Loss: 4.960 | Norm: 0.079 | LR: 1.0787e-04 | Tokens/s: 153051\n",
      "Step 6784/55081 | Loss: 5.040 | Norm: 0.073 | LR: 1.0713e-04 | Tokens/s: 153813\n",
      "Step 6792/55081 | Loss: 4.963 | Norm: 0.069 | LR: 1.0641e-04 | Tokens/s: 153847\n",
      "Step 6800/55081 | Loss: 5.133 | Norm: 0.074 | LR: 1.0568e-04 | Tokens/s: 153119\n",
      "Step 6808/55081 | Loss: 5.065 | Norm: 0.090 | LR: 1.0496e-04 | Tokens/s: 153157\n",
      "Step 6816/55081 | Loss: 5.053 | Norm: 0.066 | LR: 1.0424e-04 | Tokens/s: 153003\n",
      "Step 6824/55081 | Loss: 5.039 | Norm: 0.070 | LR: 1.0353e-04 | Tokens/s: 153017\n",
      "Step 6832/55081 | Loss: 4.960 | Norm: 0.072 | LR: 1.0282e-04 | Tokens/s: 152992\n",
      "Step 6840/55081 | Loss: 4.958 | Norm: 0.062 | LR: 1.0212e-04 | Tokens/s: 153106\n",
      "Step 6848/55081 | Loss: 5.000 | Norm: 0.066 | LR: 1.0142e-04 | Tokens/s: 153655\n",
      "Step 6856/55081 | Loss: 4.946 | Norm: 0.103 | LR: 1.0073e-04 | Tokens/s: 152767\n",
      "Step 6864/55081 | Loss: 5.033 | Norm: 0.058 | LR: 1.0004e-04 | Tokens/s: 153423\n",
      "Step 6872/55081 | Loss: 5.081 | Norm: 0.183 | LR: 9.9355e-05 | Tokens/s: 152842\n",
      "Step 6880/55081 | Loss: 5.190 | Norm: 0.097 | LR: 9.8674e-05 | Tokens/s: 153185\n",
      "Step 6888/55081 | Loss: 4.929 | Norm: 0.078 | LR: 9.7998e-05 | Tokens/s: 153312\n",
      "Step 6896/55081 | Loss: 4.930 | Norm: 0.074 | LR: 9.7326e-05 | Tokens/s: 153311\n",
      "Step 6904/55081 | Loss: 5.042 | Norm: 0.070 | LR: 9.6659e-05 | Tokens/s: 152686\n",
      "Step 6912/55081 | Loss: 4.988 | Norm: 0.090 | LR: 9.5996e-05 | Tokens/s: 153306\n",
      "Step 6920/55081 | Loss: 4.927 | Norm: 0.069 | LR: 9.5338e-05 | Tokens/s: 153879\n",
      "Step 6928/55081 | Loss: 4.913 | Norm: 0.078 | LR: 9.4685e-05 | Tokens/s: 153152\n",
      "Step 6936/55081 | Loss: 5.048 | Norm: 0.079 | LR: 9.4035e-05 | Tokens/s: 151705\n",
      "Step 6944/55081 | Loss: 4.826 | Norm: 0.135 | LR: 9.3391e-05 | Tokens/s: 153759\n",
      "Step 6952/55081 | Loss: 4.872 | Norm: 0.069 | LR: 9.2751e-05 | Tokens/s: 152781\n",
      "Step 6960/55081 | Loss: 4.961 | Norm: 0.071 | LR: 9.2115e-05 | Tokens/s: 152426\n",
      "Step 6968/55081 | Loss: 4.943 | Norm: 0.067 | LR: 9.1484e-05 | Tokens/s: 154168\n",
      "Step 6976/55081 | Loss: 4.994 | Norm: 0.069 | LR: 9.0858e-05 | Tokens/s: 152458\n",
      "Step 6984/55081 | Loss: 5.179 | Norm: 0.094 | LR: 9.0236e-05 | Tokens/s: 153979\n",
      "Step 6992/55081 | Loss: 5.279 | Norm: 0.080 | LR: 8.9618e-05 | Tokens/s: 153129\n",
      "Step 7000/55081 | Loss: 4.960 | Norm: 0.068 | LR: 8.9005e-05 | Tokens/s: 153797\n",
      "Epoch 7000 | Training checkpoint saved at models/tinyllm/checkpoint_7000.pt\n",
      "Step 7008/55081 | Loss: 4.999 | Norm: 0.063 | LR: 8.8397e-05 | Tokens/s: 154140\n",
      "Step 7016/55081 | Loss: 5.008 | Norm: 0.098 | LR: 8.7794e-05 | Tokens/s: 153810\n",
      "Step 7024/55081 | Loss: 4.979 | Norm: 0.089 | LR: 8.7195e-05 | Tokens/s: 153329\n",
      "Step 7032/55081 | Loss: 4.932 | Norm: 0.057 | LR: 8.6600e-05 | Tokens/s: 154072\n",
      "Step 7040/55081 | Loss: 4.947 | Norm: 0.080 | LR: 8.6010e-05 | Tokens/s: 153149\n",
      "Step 7048/55081 | Loss: 4.984 | Norm: 0.065 | LR: 8.5425e-05 | Tokens/s: 153401\n",
      "Step 7056/55081 | Loss: 5.129 | Norm: 0.177 | LR: 8.4844e-05 | Tokens/s: 152964\n",
      "Step 7064/55081 | Loss: 5.013 | Norm: 0.108 | LR: 8.4268e-05 | Tokens/s: 153642\n",
      "Step 7072/55081 | Loss: 5.037 | Norm: 0.063 | LR: 8.3697e-05 | Tokens/s: 152844\n",
      "Step 7080/55081 | Loss: 4.930 | Norm: 0.070 | LR: 8.3130e-05 | Tokens/s: 153817\n",
      "Step 7088/55081 | Loss: 4.925 | Norm: 0.077 | LR: 8.2568e-05 | Tokens/s: 153522\n",
      "Step 7096/55081 | Loss: 4.993 | Norm: 0.086 | LR: 8.2010e-05 | Tokens/s: 153907\n",
      "Step 7104/55081 | Loss: 4.854 | Norm: 0.083 | LR: 8.1457e-05 | Tokens/s: 153616\n",
      "Step 7112/55081 | Loss: 5.013 | Norm: 0.075 | LR: 8.0909e-05 | Tokens/s: 153992\n",
      "Step 7120/55081 | Loss: 4.887 | Norm: 0.068 | LR: 8.0366e-05 | Tokens/s: 153399\n",
      "Step 7128/55081 | Loss: 4.932 | Norm: 0.078 | LR: 7.9827e-05 | Tokens/s: 153741\n",
      "Step 7136/55081 | Loss: 4.984 | Norm: 0.064 | LR: 7.9293e-05 | Tokens/s: 153866\n",
      "Step 7144/55081 | Loss: 4.881 | Norm: 0.080 | LR: 7.8763e-05 | Tokens/s: 153087\n",
      "Step 7152/55081 | Loss: 4.945 | Norm: 0.069 | LR: 7.8238e-05 | Tokens/s: 152639\n",
      "Step 7160/55081 | Loss: 4.888 | Norm: 0.057 | LR: 7.7718e-05 | Tokens/s: 151024\n",
      "Step 7168/55081 | Loss: 4.989 | Norm: 0.066 | LR: 7.7203e-05 | Tokens/s: 153406\n",
      "Step 7176/55081 | Loss: 4.932 | Norm: 0.069 | LR: 7.6692e-05 | Tokens/s: 152665\n",
      "Step 7184/55081 | Loss: 5.064 | Norm: 0.084 | LR: 7.6186e-05 | Tokens/s: 153952\n",
      "Step 7192/55081 | Loss: 4.943 | Norm: 0.079 | LR: 7.5684e-05 | Tokens/s: 153461\n",
      "Step 7200/55081 | Loss: 4.944 | Norm: 0.065 | LR: 7.5188e-05 | Tokens/s: 152533\n",
      "Step 7208/55081 | Loss: 5.072 | Norm: 0.276 | LR: 7.4696e-05 | Tokens/s: 151877\n",
      "Step 7216/55081 | Loss: 5.063 | Norm: 0.125 | LR: 7.4208e-05 | Tokens/s: 152313\n",
      "Step 7224/55081 | Loss: 4.948 | Norm: 0.070 | LR: 7.3726e-05 | Tokens/s: 153640\n",
      "Step 7232/55081 | Loss: 5.004 | Norm: 0.085 | LR: 7.3248e-05 | Tokens/s: 153846\n",
      "Step 7240/55081 | Loss: 4.966 | Norm: 0.061 | LR: 7.2775e-05 | Tokens/s: 153597\n",
      "Step 7248/55081 | Loss: 5.064 | Norm: 0.062 | LR: 7.2307e-05 | Tokens/s: 154412\n",
      "Step 7256/55081 | Loss: 5.114 | Norm: 0.083 | LR: 7.1843e-05 | Tokens/s: 153504\n",
      "Step 7264/55081 | Loss: 4.974 | Norm: 0.073 | LR: 7.1385e-05 | Tokens/s: 153244\n",
      "Step 7272/55081 | Loss: 4.933 | Norm: 0.075 | LR: 7.0931e-05 | Tokens/s: 153393\n",
      "Step 7280/55081 | Loss: 5.065 | Norm: 0.068 | LR: 7.0481e-05 | Tokens/s: 153719\n",
      "Step 7288/55081 | Loss: 4.950 | Norm: 0.057 | LR: 7.0037e-05 | Tokens/s: 152333\n",
      "Step 7296/55081 | Loss: 4.932 | Norm: 0.056 | LR: 6.9597e-05 | Tokens/s: 153835\n",
      "Step 7304/55081 | Loss: 5.027 | Norm: 0.090 | LR: 6.9162e-05 | Tokens/s: 153077\n",
      "Step 7312/55081 | Loss: 4.995 | Norm: 0.075 | LR: 6.8732e-05 | Tokens/s: 153415\n",
      "Step 7320/55081 | Loss: 4.988 | Norm: 0.101 | LR: 6.8307e-05 | Tokens/s: 152733\n",
      "Step 7328/55081 | Loss: 4.916 | Norm: 0.058 | LR: 6.7886e-05 | Tokens/s: 153832\n",
      "Step 7336/55081 | Loss: 4.927 | Norm: 0.077 | LR: 6.7470e-05 | Tokens/s: 153645\n",
      "Step 7344/55081 | Loss: 4.927 | Norm: 0.081 | LR: 6.7059e-05 | Tokens/s: 153594\n",
      "Step 7352/55081 | Loss: 4.919 | Norm: 0.111 | LR: 6.6653e-05 | Tokens/s: 153868\n",
      "Step 7360/55081 | Loss: 4.978 | Norm: 0.085 | LR: 6.6252e-05 | Tokens/s: 153673\n",
      "Step 7368/55081 | Loss: 4.901 | Norm: 0.062 | LR: 6.5855e-05 | Tokens/s: 153617\n",
      "Step 7376/55081 | Loss: 4.981 | Norm: 0.093 | LR: 6.5463e-05 | Tokens/s: 153751\n",
      "Step 7384/55081 | Loss: 5.019 | Norm: 0.078 | LR: 6.5076e-05 | Tokens/s: 152446\n",
      "Step 7392/55081 | Loss: 4.908 | Norm: 0.067 | LR: 6.4694e-05 | Tokens/s: 153383\n",
      "Step 7400/55081 | Loss: 5.239 | Norm: 0.186 | LR: 6.4317e-05 | Tokens/s: 153637\n",
      "Step 7408/55081 | Loss: 5.010 | Norm: 0.089 | LR: 6.3945e-05 | Tokens/s: 152249\n",
      "Step 7416/55081 | Loss: 4.987 | Norm: 0.081 | LR: 6.3577e-05 | Tokens/s: 153641\n",
      "Step 7424/55081 | Loss: 4.977 | Norm: 0.063 | LR: 6.3214e-05 | Tokens/s: 153404\n",
      "Step 7432/55081 | Loss: 4.946 | Norm: 0.074 | LR: 6.2856e-05 | Tokens/s: 152656\n",
      "Step 7440/55081 | Loss: 4.837 | Norm: 0.062 | LR: 6.2503e-05 | Tokens/s: 153598\n",
      "Step 7448/55081 | Loss: 5.031 | Norm: 0.064 | LR: 6.2155e-05 | Tokens/s: 152566\n",
      "Step 7456/55081 | Loss: 4.981 | Norm: 0.069 | LR: 6.1812e-05 | Tokens/s: 153574\n",
      "Step 7464/55081 | Loss: 4.904 | Norm: 0.068 | LR: 6.1473e-05 | Tokens/s: 153331\n",
      "Step 7472/55081 | Loss: 4.901 | Norm: 0.091 | LR: 6.1139e-05 | Tokens/s: 153534\n",
      "Step 7480/55081 | Loss: 4.968 | Norm: 0.082 | LR: 6.0811e-05 | Tokens/s: 152313\n",
      "Step 7488/55081 | Loss: 4.992 | Norm: 0.089 | LR: 6.0487e-05 | Tokens/s: 153761\n",
      "Step 7496/55081 | Loss: 5.077 | Norm: 0.073 | LR: 6.0168e-05 | Tokens/s: 153461\n",
      "Epoch 7500 | Training checkpoint saved at models/tinyllm/checkpoint_7500.pt\n",
      "Step 7504/55081 | Loss: 4.995 | Norm: 0.104 | LR: 5.9854e-05 | Tokens/s: 153506\n",
      "Step 7512/55081 | Loss: 5.117 | Norm: 0.088 | LR: 5.9544e-05 | Tokens/s: 153218\n",
      "Step 7520/55081 | Loss: 4.964 | Norm: 0.062 | LR: 5.9240e-05 | Tokens/s: 153742\n",
      "Step 7528/55081 | Loss: 5.010 | Norm: 0.125 | LR: 5.8940e-05 | Tokens/s: 153236\n",
      "Step 7536/55081 | Loss: 5.014 | Norm: 0.069 | LR: 5.8646e-05 | Tokens/s: 152691\n",
      "Step 7544/55081 | Loss: 5.082 | Norm: 0.073 | LR: 5.8356e-05 | Tokens/s: 153245\n",
      "Step 7552/55081 | Loss: 4.985 | Norm: 0.075 | LR: 5.8071e-05 | Tokens/s: 153340\n",
      "Step 7560/55081 | Loss: 5.294 | Norm: 0.068 | LR: 5.7791e-05 | Tokens/s: 153861\n",
      "Step 7568/55081 | Loss: 4.885 | Norm: 0.066 | LR: 5.7516e-05 | Tokens/s: 153784\n",
      "Step 7576/55081 | Loss: 4.967 | Norm: 0.060 | LR: 5.7246e-05 | Tokens/s: 152640\n",
      "Step 7584/55081 | Loss: 5.081 | Norm: 0.176 | LR: 5.6981e-05 | Tokens/s: 153827\n",
      "Step 7592/55081 | Loss: 4.948 | Norm: 0.054 | LR: 5.6720e-05 | Tokens/s: 151428\n",
      "Step 7600/55081 | Loss: 4.971 | Norm: 0.082 | LR: 5.6465e-05 | Tokens/s: 153448\n",
      "Step 7608/55081 | Loss: 4.927 | Norm: 0.066 | LR: 5.6215e-05 | Tokens/s: 153580\n",
      "Step 7616/55081 | Loss: 5.040 | Norm: 0.059 | LR: 5.5969e-05 | Tokens/s: 153000\n",
      "Step 7624/55081 | Loss: 4.930 | Norm: 0.070 | LR: 5.5728e-05 | Tokens/s: 153821\n",
      "Step 7632/55081 | Loss: 4.978 | Norm: 0.079 | LR: 5.5493e-05 | Tokens/s: 153752\n",
      "Step 7640/55081 | Loss: 4.884 | Norm: 0.059 | LR: 5.5262e-05 | Tokens/s: 153420\n",
      "Step 7648/55081 | Loss: 4.993 | Norm: 0.081 | LR: 5.5036e-05 | Tokens/s: 153369\n",
      "Step 7656/55081 | Loss: 4.968 | Norm: 0.096 | LR: 5.4815e-05 | Tokens/s: 153669\n",
      "Step 7664/55081 | Loss: 5.006 | Norm: 0.066 | LR: 5.4599e-05 | Tokens/s: 152805\n",
      "Step 7672/55081 | Loss: 5.026 | Norm: 0.078 | LR: 5.4388e-05 | Tokens/s: 153290\n",
      "Step 7680/55081 | Loss: 4.856 | Norm: 0.083 | LR: 5.4182e-05 | Tokens/s: 153703\n",
      "Step 7688/55081 | Loss: 4.934 | Norm: 0.067 | LR: 5.3980e-05 | Tokens/s: 153150\n",
      "Step 7696/55081 | Loss: 4.901 | Norm: 0.094 | LR: 5.3784e-05 | Tokens/s: 153020\n",
      "Step 7704/55081 | Loss: 4.992 | Norm: 0.093 | LR: 5.3593e-05 | Tokens/s: 154012\n",
      "Step 7712/55081 | Loss: 5.005 | Norm: 0.113 | LR: 5.3406e-05 | Tokens/s: 153628\n",
      "Step 7720/55081 | Loss: 5.127 | Norm: 0.064 | LR: 5.3225e-05 | Tokens/s: 153284\n",
      "Step 7728/55081 | Loss: 4.904 | Norm: 0.058 | LR: 5.3049e-05 | Tokens/s: 153835\n",
      "Step 7736/55081 | Loss: 5.013 | Norm: 0.066 | LR: 5.2877e-05 | Tokens/s: 153432\n",
      "Step 7744/55081 | Loss: 4.949 | Norm: 0.074 | LR: 5.2710e-05 | Tokens/s: 153517\n",
      "Step 7752/55081 | Loss: 4.965 | Norm: 0.070 | LR: 5.2549e-05 | Tokens/s: 153610\n",
      "Step 7760/55081 | Loss: 4.989 | Norm: 0.106 | LR: 5.2392e-05 | Tokens/s: 153441\n",
      "Step 7768/55081 | Loss: 4.901 | Norm: 0.067 | LR: 5.2240e-05 | Tokens/s: 152680\n",
      "Step 7776/55081 | Loss: 4.989 | Norm: 0.096 | LR: 5.2094e-05 | Tokens/s: 153497\n",
      "Step 7784/55081 | Loss: 4.944 | Norm: 0.091 | LR: 5.1952e-05 | Tokens/s: 152901\n",
      "Step 7792/55081 | Loss: 4.992 | Norm: 0.062 | LR: 5.1815e-05 | Tokens/s: 152942\n",
      "Step 7800/55081 | Loss: 4.973 | Norm: 0.083 | LR: 5.1683e-05 | Tokens/s: 153656\n",
      "Step 7808/55081 | Loss: 4.926 | Norm: 0.065 | LR: 5.1556e-05 | Tokens/s: 152606\n",
      "Step 7816/55081 | Loss: 4.940 | Norm: 0.075 | LR: 5.1434e-05 | Tokens/s: 153898\n",
      "Step 7824/55081 | Loss: 4.955 | Norm: 0.073 | LR: 5.1317e-05 | Tokens/s: 152431\n",
      "Step 7832/55081 | Loss: 5.002 | Norm: 0.065 | LR: 5.1205e-05 | Tokens/s: 153628\n",
      "Step 7840/55081 | Loss: 4.951 | Norm: 0.064 | LR: 5.1098e-05 | Tokens/s: 153790\n",
      "Step 7848/55081 | Loss: 5.094 | Norm: 0.080 | LR: 5.0996e-05 | Tokens/s: 152619\n",
      "Step 7856/55081 | Loss: 5.079 | Norm: 0.100 | LR: 5.0899e-05 | Tokens/s: 153562\n",
      "Step 7864/55081 | Loss: 5.030 | Norm: 0.074 | LR: 5.0807e-05 | Tokens/s: 153120\n",
      "Step 7872/55081 | Loss: 4.974 | Norm: 0.087 | LR: 5.0720e-05 | Tokens/s: 152985\n",
      "Step 7880/55081 | Loss: 5.046 | Norm: 0.082 | LR: 5.0638e-05 | Tokens/s: 152619\n",
      "Step 7888/55081 | Loss: 4.964 | Norm: 0.083 | LR: 5.0560e-05 | Tokens/s: 152728\n",
      "Step 7896/55081 | Loss: 4.950 | Norm: 0.063 | LR: 5.0488e-05 | Tokens/s: 153736\n",
      "Step 7904/55081 | Loss: 4.962 | Norm: 0.065 | LR: 5.0421e-05 | Tokens/s: 152710\n",
      "Step 7912/55081 | Loss: 4.872 | Norm: 0.061 | LR: 5.0359e-05 | Tokens/s: 152698\n",
      "Step 7920/55081 | Loss: 5.113 | Norm: 0.273 | LR: 5.0301e-05 | Tokens/s: 153636\n",
      "Step 7928/55081 | Loss: 5.389 | Norm: 0.228 | LR: 5.0249e-05 | Tokens/s: 153423\n",
      "Step 7936/55081 | Loss: 5.146 | Norm: 0.087 | LR: 5.0202e-05 | Tokens/s: 153869\n",
      "Step 7944/55081 | Loss: 5.017 | Norm: 0.063 | LR: 5.0159e-05 | Tokens/s: 153355\n",
      "Step 7952/55081 | Loss: 4.874 | Norm: 0.083 | LR: 5.0122e-05 | Tokens/s: 151248\n",
      "Step 7960/55081 | Loss: 4.921 | Norm: 0.093 | LR: 5.0090e-05 | Tokens/s: 153360\n",
      "Step 7968/55081 | Loss: 4.944 | Norm: 0.083 | LR: 5.0062e-05 | Tokens/s: 153128\n",
      "Step 7976/55081 | Loss: 5.062 | Norm: 0.327 | LR: 5.0040e-05 | Tokens/s: 153258\n",
      "Step 7984/55081 | Loss: 5.460 | Norm: 0.083 | LR: 5.0022e-05 | Tokens/s: 153956\n",
      "Step 7992/55081 | Loss: 4.920 | Norm: 0.062 | LR: 5.0010e-05 | Tokens/s: 153525\n",
      "Step 8000/55081 | Loss: 5.014 | Norm: 0.088 | LR: 5.0002e-05 | Tokens/s: 153618\n",
      "Epoch 8000 | Training checkpoint saved at models/tinyllm/checkpoint_8000.pt\n",
      "Step 8008/55081 | Loss: 4.994 | Norm: 0.144 | LR: 5.0000e-05 | Tokens/s: 153021\n",
      "Step 8016/55081 | Loss: 4.936 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153370\n",
      "Step 8024/55081 | Loss: 4.893 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 153126\n",
      "Step 8032/55081 | Loss: 5.013 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153255\n",
      "Step 8040/55081 | Loss: 4.943 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153662\n",
      "Step 8048/55081 | Loss: 5.123 | Norm: 0.104 | LR: 5.0000e-05 | Tokens/s: 153868\n",
      "Step 8056/55081 | Loss: 4.915 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153216\n",
      "Step 8064/55081 | Loss: 4.981 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153080\n",
      "Step 8072/55081 | Loss: 4.919 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153486\n",
      "Step 8080/55081 | Loss: 4.965 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153547\n",
      "Step 8088/55081 | Loss: 4.987 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153200\n",
      "Step 8096/55081 | Loss: 5.071 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 152767\n",
      "Step 8104/55081 | Loss: 4.876 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 154109\n",
      "Step 8112/55081 | Loss: 4.993 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153222\n",
      "Step 8120/55081 | Loss: 4.944 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153555\n",
      "Step 8128/55081 | Loss: 4.959 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 153558\n",
      "Step 8136/55081 | Loss: 4.902 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 152574\n",
      "Step 8144/55081 | Loss: 4.967 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153570\n",
      "Step 8152/55081 | Loss: 4.959 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 151762\n",
      "Step 8160/55081 | Loss: 5.233 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153636\n",
      "Step 8168/55081 | Loss: 4.987 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153037\n",
      "Step 8176/55081 | Loss: 4.946 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 152664\n",
      "Step 8184/55081 | Loss: 4.872 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153374\n",
      "Step 8192/55081 | Loss: 4.903 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153338\n",
      "Step 8200/55081 | Loss: 4.944 | Norm: 0.196 | LR: 5.0000e-05 | Tokens/s: 154218\n",
      "Step 8208/55081 | Loss: 4.913 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153380\n",
      "Step 8216/55081 | Loss: 4.984 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153741\n",
      "Step 8224/55081 | Loss: 4.982 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 152728\n",
      "Step 8232/55081 | Loss: 4.899 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 152575\n",
      "Step 8240/55081 | Loss: 4.989 | Norm: 0.055 | LR: 5.0000e-05 | Tokens/s: 152734\n",
      "Step 8248/55081 | Loss: 4.843 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153128\n",
      "Step 8256/55081 | Loss: 4.975 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 152465\n",
      "Step 8264/55081 | Loss: 4.915 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153285\n",
      "Step 8272/55081 | Loss: 4.904 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153326\n",
      "Step 8280/55081 | Loss: 5.035 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153731\n",
      "Step 8288/55081 | Loss: 5.059 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153376\n",
      "Step 8296/55081 | Loss: 4.887 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153910\n",
      "Step 8304/55081 | Loss: 5.011 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153621\n",
      "Step 8312/55081 | Loss: 4.845 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153334\n",
      "Step 8320/55081 | Loss: 5.250 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153198\n",
      "Step 8328/55081 | Loss: 5.029 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 153476\n",
      "Step 8336/55081 | Loss: 5.071 | Norm: 0.119 | LR: 5.0000e-05 | Tokens/s: 153000\n",
      "Step 8344/55081 | Loss: 5.050 | Norm: 0.237 | LR: 5.0000e-05 | Tokens/s: 153385\n",
      "Step 8352/55081 | Loss: 5.213 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153788\n",
      "Step 8360/55081 | Loss: 4.907 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153223\n",
      "Step 8368/55081 | Loss: 4.973 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 152987\n",
      "Step 8376/55081 | Loss: 4.939 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153876\n",
      "Step 8384/55081 | Loss: 5.009 | Norm: 0.186 | LR: 5.0000e-05 | Tokens/s: 152758\n",
      "Step 8392/55081 | Loss: 4.981 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153149\n",
      "Step 8400/55081 | Loss: 4.922 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 152401\n",
      "Step 8408/55081 | Loss: 4.943 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153836\n",
      "Step 8416/55081 | Loss: 4.860 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 153350\n",
      "Step 8424/55081 | Loss: 5.050 | Norm: 0.130 | LR: 5.0000e-05 | Tokens/s: 153113\n",
      "Step 8432/55081 | Loss: 4.922 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 153928\n",
      "Step 8440/55081 | Loss: 5.028 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152889\n",
      "Step 8448/55081 | Loss: 4.960 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 152269\n",
      "Step 8456/55081 | Loss: 4.916 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152532\n",
      "Step 8464/55081 | Loss: 5.024 | Norm: 0.118 | LR: 5.0000e-05 | Tokens/s: 153122\n",
      "Step 8472/55081 | Loss: 4.936 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153071\n",
      "Step 8480/55081 | Loss: 4.974 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 152470\n",
      "Step 8488/55081 | Loss: 4.942 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153519\n",
      "Step 8496/55081 | Loss: 4.997 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153536\n",
      "Epoch 8500 | Training checkpoint saved at models/tinyllm/checkpoint_8500.pt\n",
      "Step 8504/55081 | Loss: 4.911 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153232\n",
      "Step 8512/55081 | Loss: 5.059 | Norm: 0.122 | LR: 5.0000e-05 | Tokens/s: 153558\n",
      "Step 8520/55081 | Loss: 5.001 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153635\n",
      "Step 8528/55081 | Loss: 4.979 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 152321\n",
      "Step 8536/55081 | Loss: 5.031 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153820\n",
      "Step 8544/55081 | Loss: 5.090 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153377\n",
      "Step 8552/55081 | Loss: 4.963 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153786\n",
      "Step 8560/55081 | Loss: 4.987 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153580\n",
      "Step 8568/55081 | Loss: 5.006 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 152876\n",
      "Step 8576/55081 | Loss: 4.962 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153439\n",
      "Step 8584/55081 | Loss: 4.900 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 152139\n",
      "Step 8592/55081 | Loss: 4.980 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153430\n",
      "Step 8600/55081 | Loss: 4.972 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152646\n",
      "Step 8608/55081 | Loss: 5.021 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153118\n",
      "Step 8616/55081 | Loss: 5.045 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152922\n",
      "Step 8624/55081 | Loss: 4.966 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153076\n",
      "Step 8632/55081 | Loss: 4.905 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 153751\n",
      "Step 8640/55081 | Loss: 4.927 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153517\n",
      "Step 8648/55081 | Loss: 4.952 | Norm: 0.123 | LR: 5.0000e-05 | Tokens/s: 153939\n",
      "Step 8656/55081 | Loss: 5.040 | Norm: 0.123 | LR: 5.0000e-05 | Tokens/s: 152768\n",
      "Step 8664/55081 | Loss: 4.999 | Norm: 0.147 | LR: 5.0000e-05 | Tokens/s: 152379\n",
      "Step 8672/55081 | Loss: 5.002 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 152535\n",
      "Step 8680/55081 | Loss: 4.898 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 153631\n",
      "Step 8688/55081 | Loss: 4.936 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153450\n",
      "Step 8696/55081 | Loss: 4.811 | Norm: 0.105 | LR: 5.0000e-05 | Tokens/s: 152906\n",
      "Step 8704/55081 | Loss: 5.004 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153588\n",
      "Step 8712/55081 | Loss: 4.902 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 152253\n",
      "Step 8720/55081 | Loss: 4.927 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153841\n",
      "Step 8728/55081 | Loss: 5.007 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 152409\n",
      "Step 8736/55081 | Loss: 5.160 | Norm: 0.284 | LR: 5.0000e-05 | Tokens/s: 153115\n",
      "Step 8744/55081 | Loss: 4.901 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153516\n",
      "Step 8752/55081 | Loss: 5.209 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 152613\n",
      "Step 8760/55081 | Loss: 5.012 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152693\n",
      "Step 8768/55081 | Loss: 4.937 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153664\n",
      "Step 8776/55081 | Loss: 4.883 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153272\n",
      "Step 8784/55081 | Loss: 4.967 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 154047\n",
      "Step 8792/55081 | Loss: 4.889 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153320\n",
      "Step 8800/55081 | Loss: 4.940 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 152504\n",
      "Step 8808/55081 | Loss: 4.992 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153356\n",
      "Step 8816/55081 | Loss: 5.077 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 152137\n",
      "Step 8824/55081 | Loss: 4.874 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153328\n",
      "Step 8832/55081 | Loss: 4.915 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153345\n",
      "Step 8840/55081 | Loss: 5.012 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153865\n",
      "Step 8848/55081 | Loss: 5.068 | Norm: 0.196 | LR: 5.0000e-05 | Tokens/s: 152783\n",
      "Step 8856/55081 | Loss: 5.020 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153481\n",
      "Step 8864/55081 | Loss: 5.015 | Norm: 0.125 | LR: 5.0000e-05 | Tokens/s: 153075\n",
      "Step 8872/55081 | Loss: 5.141 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153022\n",
      "Step 8880/55081 | Loss: 4.912 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153702\n",
      "Step 8888/55081 | Loss: 5.051 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 152518\n",
      "Step 8896/55081 | Loss: 4.900 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153629\n",
      "Step 8904/55081 | Loss: 4.985 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 152988\n",
      "Step 8912/55081 | Loss: 5.020 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 152670\n",
      "Step 8920/55081 | Loss: 5.004 | Norm: 0.126 | LR: 5.0000e-05 | Tokens/s: 153542\n",
      "Step 8928/55081 | Loss: 4.951 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 152883\n",
      "Step 8936/55081 | Loss: 4.961 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 153395\n",
      "Step 8944/55081 | Loss: 4.969 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 151991\n",
      "Step 8952/55081 | Loss: 5.004 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153612\n",
      "Step 8960/55081 | Loss: 4.950 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152728\n",
      "Step 8968/55081 | Loss: 5.032 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 152900\n",
      "Step 8976/55081 | Loss: 5.058 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153393\n",
      "Step 8984/55081 | Loss: 4.951 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153222\n",
      "Step 8992/55081 | Loss: 4.917 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 152800\n",
      "Step 9000/55081 | Loss: 5.011 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 154241\n",
      "Epoch 9000 | Training checkpoint saved at models/tinyllm/checkpoint_9000.pt\n",
      "Step 9008/55081 | Loss: 5.076 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 152997\n",
      "Step 9016/55081 | Loss: 4.982 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 152909\n",
      "Step 9024/55081 | Loss: 5.141 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153516\n",
      "Step 9032/55081 | Loss: 4.971 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153891\n",
      "Step 9040/55081 | Loss: 4.972 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153656\n",
      "Step 9048/55081 | Loss: 4.936 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153356\n",
      "Step 9056/55081 | Loss: 4.996 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153316\n",
      "Step 9064/55081 | Loss: 5.017 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153377\n",
      "Step 9072/55081 | Loss: 4.996 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 152894\n",
      "Step 9080/55081 | Loss: 4.937 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153115\n",
      "Step 9088/55081 | Loss: 4.921 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 151922\n",
      "Step 9096/55081 | Loss: 5.053 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153352\n",
      "Step 9104/55081 | Loss: 5.095 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 152952\n",
      "Step 9112/55081 | Loss: 4.955 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153510\n",
      "Step 9120/55081 | Loss: 5.076 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153688\n",
      "Step 9128/55081 | Loss: 5.214 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 73836\n",
      "Step 9136/55081 | Loss: 4.954 | Norm: 0.095 | LR: 5.0000e-05 | Tokens/s: 153621\n",
      "Step 9144/55081 | Loss: 4.950 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 152767\n",
      "Step 9152/55081 | Loss: 5.057 | Norm: 0.113 | LR: 5.0000e-05 | Tokens/s: 153509\n",
      "Step 9160/55081 | Loss: 5.042 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153815\n",
      "Step 9168/55081 | Loss: 4.964 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153802\n",
      "Step 9176/55081 | Loss: 4.971 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153798\n",
      "Step 9184/55081 | Loss: 4.957 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 152938\n",
      "Step 9192/55081 | Loss: 4.917 | Norm: 0.096 | LR: 5.0000e-05 | Tokens/s: 153969\n",
      "Step 9200/55081 | Loss: 4.934 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153238\n",
      "Step 9208/55081 | Loss: 4.937 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153764\n",
      "Step 9216/55081 | Loss: 4.986 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 152446\n",
      "Step 9224/55081 | Loss: 4.940 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153421\n",
      "Step 9232/55081 | Loss: 4.948 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153123\n",
      "Step 9240/55081 | Loss: 4.937 | Norm: 0.104 | LR: 5.0000e-05 | Tokens/s: 154213\n",
      "Step 9248/55081 | Loss: 4.985 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 152662\n",
      "Step 9256/55081 | Loss: 5.007 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153743\n",
      "Step 9264/55081 | Loss: 4.949 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153246\n",
      "Step 9272/55081 | Loss: 4.911 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153476\n",
      "Step 9280/55081 | Loss: 4.945 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153625\n",
      "Step 9288/55081 | Loss: 4.889 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153880\n",
      "Step 9296/55081 | Loss: 5.097 | Norm: 0.182 | LR: 5.0000e-05 | Tokens/s: 153281\n",
      "Step 9304/55081 | Loss: 4.948 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153202\n",
      "Step 9312/55081 | Loss: 4.935 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 68672\n",
      "Step 9320/55081 | Loss: 5.019 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153667\n",
      "Step 9328/55081 | Loss: 4.960 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 153841\n",
      "Step 9336/55081 | Loss: 4.966 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 152959\n",
      "Step 9344/55081 | Loss: 4.967 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 153150\n",
      "Step 9352/55081 | Loss: 5.213 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 153736\n",
      "Step 9360/55081 | Loss: 4.884 | Norm: 0.097 | LR: 5.0000e-05 | Tokens/s: 153093\n",
      "Step 9368/55081 | Loss: 5.015 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153683\n",
      "Step 9376/55081 | Loss: 4.938 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 152609\n",
      "Step 9384/55081 | Loss: 4.982 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153836\n",
      "Step 9392/55081 | Loss: 4.910 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153371\n",
      "Step 9400/55081 | Loss: 4.885 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153200\n",
      "Step 9408/55081 | Loss: 5.022 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153615\n",
      "Step 9416/55081 | Loss: 5.014 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153899\n",
      "Step 9424/55081 | Loss: 4.993 | Norm: 0.122 | LR: 5.0000e-05 | Tokens/s: 153392\n",
      "Step 9432/55081 | Loss: 5.032 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 153505\n",
      "Step 9440/55081 | Loss: 4.964 | Norm: 0.056 | LR: 5.0000e-05 | Tokens/s: 153901\n",
      "Step 9448/55081 | Loss: 4.929 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 152917\n",
      "Step 9456/55081 | Loss: 4.964 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 153521\n",
      "Step 9464/55081 | Loss: 4.784 | Norm: 0.125 | LR: 5.0000e-05 | Tokens/s: 152925\n",
      "Step 9472/55081 | Loss: 5.081 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 152650\n",
      "Step 9480/55081 | Loss: 4.985 | Norm: 0.183 | LR: 5.0000e-05 | Tokens/s: 152564\n",
      "Step 9488/55081 | Loss: 5.183 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153352\n",
      "Step 9496/55081 | Loss: 5.056 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153873\n",
      "Epoch 9500 | Training checkpoint saved at models/tinyllm/checkpoint_9500.pt\n",
      "Step 9504/55081 | Loss: 5.190 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 151957\n",
      "Step 9512/55081 | Loss: 4.923 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153915\n",
      "Step 9520/55081 | Loss: 4.923 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153026\n",
      "Step 9528/55081 | Loss: 5.003 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153500\n",
      "Step 9536/55081 | Loss: 4.895 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 151864\n",
      "Step 9544/55081 | Loss: 4.953 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 152632\n",
      "Step 9552/55081 | Loss: 4.990 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153582\n",
      "Step 9560/55081 | Loss: 5.077 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 152910\n",
      "Step 9568/55081 | Loss: 4.925 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153777\n",
      "Step 9576/55081 | Loss: 5.037 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152526\n",
      "Step 9584/55081 | Loss: 5.069 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 153863\n",
      "Step 9592/55081 | Loss: 4.931 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153685\n",
      "Step 9600/55081 | Loss: 4.965 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 154048\n",
      "Step 9608/55081 | Loss: 4.900 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153069\n",
      "Step 9616/55081 | Loss: 4.978 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153635\n",
      "Step 9624/55081 | Loss: 4.922 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153239\n",
      "Step 9632/55081 | Loss: 5.011 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153540\n",
      "Step 9640/55081 | Loss: 4.985 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153582\n",
      "Step 9648/55081 | Loss: 4.953 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153237\n",
      "Step 9656/55081 | Loss: 4.948 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153389\n",
      "Step 9664/55081 | Loss: 5.053 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 152749\n",
      "Step 9672/55081 | Loss: 4.942 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153526\n",
      "Step 9680/55081 | Loss: 4.956 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 152976\n",
      "Step 9688/55081 | Loss: 5.090 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153419\n",
      "Step 9696/55081 | Loss: 4.924 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152614\n",
      "Step 9704/55081 | Loss: 4.853 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 152852\n",
      "Step 9712/55081 | Loss: 4.968 | Norm: 0.103 | LR: 5.0000e-05 | Tokens/s: 153512\n",
      "Step 9720/55081 | Loss: 4.928 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153681\n",
      "Step 9728/55081 | Loss: 4.928 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153313\n",
      "Step 9736/55081 | Loss: 4.986 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152938\n",
      "Step 9744/55081 | Loss: 4.962 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153566\n",
      "Step 9752/55081 | Loss: 5.064 | Norm: 0.357 | LR: 5.0000e-05 | Tokens/s: 152585\n",
      "Step 9760/55081 | Loss: 5.573 | Norm: 0.337 | LR: 5.0000e-05 | Tokens/s: 153596\n",
      "Step 9768/55081 | Loss: 5.213 | Norm: 0.104 | LR: 5.0000e-05 | Tokens/s: 152730\n",
      "Step 9776/55081 | Loss: 4.934 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153581\n",
      "Step 9784/55081 | Loss: 4.986 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153188\n",
      "Step 9792/55081 | Loss: 4.947 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153078\n",
      "Step 9800/55081 | Loss: 4.956 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153446\n",
      "Step 9808/55081 | Loss: 4.959 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 152869\n",
      "Step 9816/55081 | Loss: 4.837 | Norm: 0.097 | LR: 5.0000e-05 | Tokens/s: 153213\n",
      "Step 9824/55081 | Loss: 4.934 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 153177\n",
      "Step 9832/55081 | Loss: 4.900 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153597\n",
      "Step 9840/55081 | Loss: 4.976 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 152998\n",
      "Step 9848/55081 | Loss: 4.963 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153567\n",
      "Step 9856/55081 | Loss: 4.962 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153383\n",
      "Step 9864/55081 | Loss: 4.897 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 153321\n",
      "Step 9872/55081 | Loss: 4.997 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153771\n",
      "Step 9880/55081 | Loss: 4.861 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152348\n",
      "Step 9888/55081 | Loss: 4.962 | Norm: 0.106 | LR: 5.0000e-05 | Tokens/s: 153563\n",
      "Step 9896/55081 | Loss: 4.822 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152087\n",
      "Step 9904/55081 | Loss: 4.964 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 152986\n",
      "Step 9912/55081 | Loss: 4.996 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152196\n",
      "Step 9920/55081 | Loss: 5.113 | Norm: 0.096 | LR: 5.0000e-05 | Tokens/s: 153947\n",
      "Step 9928/55081 | Loss: 4.992 | Norm: 0.139 | LR: 5.0000e-05 | Tokens/s: 152974\n",
      "Step 9936/55081 | Loss: 4.908 | Norm: 0.113 | LR: 5.0000e-05 | Tokens/s: 153234\n",
      "Step 9944/55081 | Loss: 4.884 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153733\n",
      "Step 9952/55081 | Loss: 4.933 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 154115\n",
      "Step 9960/55081 | Loss: 4.926 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153280\n",
      "Step 9968/55081 | Loss: 4.982 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153903\n",
      "Step 9976/55081 | Loss: 5.057 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153721\n",
      "Step 9984/55081 | Loss: 5.065 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153302\n",
      "Step 9992/55081 | Loss: 4.903 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153824\n",
      "Step 10000/55081 | Loss: 5.018 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153532\n",
      "Epoch 10000 | Training checkpoint saved at models/tinyllm/checkpoint_10000.pt\n",
      "Step 10008/55081 | Loss: 5.057 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152581\n",
      "Step 10016/55081 | Loss: 5.017 | Norm: 0.097 | LR: 5.0000e-05 | Tokens/s: 153567\n",
      "Step 10024/55081 | Loss: 4.936 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152813\n",
      "Step 10032/55081 | Loss: 4.879 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153733\n",
      "Step 10040/55081 | Loss: 5.030 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 152597\n",
      "Step 10048/55081 | Loss: 4.994 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153090\n",
      "Step 10056/55081 | Loss: 5.027 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 152996\n",
      "Step 10064/55081 | Loss: 4.868 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153718\n",
      "Step 10072/55081 | Loss: 5.002 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 152997\n",
      "Step 10080/55081 | Loss: 4.899 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153287\n",
      "Step 10088/55081 | Loss: 4.943 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153765\n",
      "Step 10096/55081 | Loss: 4.913 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 152168\n",
      "Step 10104/55081 | Loss: 5.035 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153317\n",
      "Step 10112/55081 | Loss: 4.994 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 152891\n",
      "Step 10120/55081 | Loss: 4.898 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153187\n",
      "Step 10128/55081 | Loss: 4.978 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153876\n",
      "Step 10136/55081 | Loss: 4.923 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153731\n",
      "Step 10144/55081 | Loss: 5.153 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153662\n",
      "Step 10152/55081 | Loss: 5.013 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153168\n",
      "Step 10160/55081 | Loss: 4.912 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153526\n",
      "Step 10168/55081 | Loss: 4.996 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 153284\n",
      "Step 10176/55081 | Loss: 4.938 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153758\n",
      "Step 10184/55081 | Loss: 4.932 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 151859\n",
      "Step 10192/55081 | Loss: 4.968 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153678\n",
      "Step 10200/55081 | Loss: 4.893 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153451\n",
      "Step 10208/55081 | Loss: 4.966 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153164\n",
      "Step 10216/55081 | Loss: 4.919 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153735\n",
      "Step 10224/55081 | Loss: 4.939 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153191\n",
      "Step 10232/55081 | Loss: 4.911 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 153399\n",
      "Step 10240/55081 | Loss: 4.983 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 152345\n",
      "Step 10248/55081 | Loss: 4.912 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 154131\n",
      "Step 10256/55081 | Loss: 5.162 | Norm: 0.104 | LR: 5.0000e-05 | Tokens/s: 153025\n",
      "Step 10264/55081 | Loss: 4.996 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153553\n",
      "Step 10272/55081 | Loss: 5.006 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153650\n",
      "Step 10280/55081 | Loss: 4.896 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 152679\n",
      "Step 10288/55081 | Loss: 4.949 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153595\n",
      "Step 10296/55081 | Loss: 5.066 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 153127\n",
      "Step 10304/55081 | Loss: 5.091 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 152222\n",
      "Step 10312/55081 | Loss: 5.127 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 152386\n",
      "Step 10320/55081 | Loss: 5.340 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153590\n",
      "Step 10328/55081 | Loss: 4.956 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 152576\n",
      "Step 10336/55081 | Loss: 5.049 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153436\n",
      "Step 10344/55081 | Loss: 4.936 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 152934\n",
      "Step 10352/55081 | Loss: 4.916 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152970\n",
      "Step 10360/55081 | Loss: 4.837 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153557\n",
      "Step 10368/55081 | Loss: 4.970 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153443\n",
      "Step 10376/55081 | Loss: 4.962 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 152504\n",
      "Step 10384/55081 | Loss: 4.904 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 154108\n",
      "Step 10392/55081 | Loss: 4.908 | Norm: 0.055 | LR: 5.0000e-05 | Tokens/s: 153481\n",
      "Step 10400/55081 | Loss: 5.119 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153407\n",
      "Step 10408/55081 | Loss: 5.096 | Norm: 0.130 | LR: 5.0000e-05 | Tokens/s: 153701\n",
      "Step 10416/55081 | Loss: 5.067 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153907\n",
      "Step 10424/55081 | Loss: 5.035 | Norm: 0.143 | LR: 5.0000e-05 | Tokens/s: 153117\n",
      "Step 10432/55081 | Loss: 5.082 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153460\n",
      "Step 10440/55081 | Loss: 4.844 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153400\n",
      "Step 10448/55081 | Loss: 5.008 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153874\n",
      "Step 10456/55081 | Loss: 4.973 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152813\n",
      "Step 10464/55081 | Loss: 5.056 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153289\n",
      "Step 10472/55081 | Loss: 4.954 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153118\n",
      "Step 10480/55081 | Loss: 4.922 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153500\n",
      "Step 10488/55081 | Loss: 4.978 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 152700\n",
      "Step 10496/55081 | Loss: 4.926 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153355\n",
      "Epoch 10500 | Training checkpoint saved at models/tinyllm/checkpoint_10500.pt\n",
      "Step 10504/55081 | Loss: 5.018 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153411\n",
      "Step 10512/55081 | Loss: 5.095 | Norm: 0.276 | LR: 5.0000e-05 | Tokens/s: 152828\n",
      "Step 10520/55081 | Loss: 5.361 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153536\n",
      "Step 10528/55081 | Loss: 5.018 | Norm: 0.127 | LR: 5.0000e-05 | Tokens/s: 152823\n",
      "Step 10536/55081 | Loss: 5.010 | Norm: 0.127 | LR: 5.0000e-05 | Tokens/s: 153584\n",
      "Step 10544/55081 | Loss: 4.998 | Norm: 0.109 | LR: 5.0000e-05 | Tokens/s: 152904\n",
      "Step 10552/55081 | Loss: 4.969 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152651\n",
      "Step 10560/55081 | Loss: 5.038 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 154022\n",
      "Step 10568/55081 | Loss: 5.172 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 153442\n",
      "Step 10576/55081 | Loss: 4.975 | Norm: 0.104 | LR: 5.0000e-05 | Tokens/s: 153398\n",
      "Step 10584/55081 | Loss: 4.958 | Norm: 0.095 | LR: 5.0000e-05 | Tokens/s: 153858\n",
      "Step 10592/55081 | Loss: 5.019 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153868\n",
      "Step 10600/55081 | Loss: 4.922 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 152586\n",
      "Step 10608/55081 | Loss: 4.998 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 152421\n",
      "Step 10616/55081 | Loss: 4.944 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 153428\n",
      "Step 10624/55081 | Loss: 4.996 | Norm: 0.106 | LR: 5.0000e-05 | Tokens/s: 152419\n",
      "Step 10632/55081 | Loss: 4.893 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 153886\n",
      "Step 10640/55081 | Loss: 4.977 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153244\n",
      "Step 10648/55081 | Loss: 5.002 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 154117\n",
      "Step 10656/55081 | Loss: 4.948 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 152558\n",
      "Step 10664/55081 | Loss: 5.023 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153589\n",
      "Step 10672/55081 | Loss: 4.907 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 152469\n",
      "Step 10680/55081 | Loss: 4.920 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152332\n",
      "Step 10688/55081 | Loss: 4.805 | Norm: 0.136 | LR: 5.0000e-05 | Tokens/s: 152549\n",
      "Step 10696/55081 | Loss: 4.945 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153056\n",
      "Step 10704/55081 | Loss: 4.941 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153680\n",
      "Step 10712/55081 | Loss: 5.001 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153239\n",
      "Step 10720/55081 | Loss: 4.985 | Norm: 0.198 | LR: 5.0000e-05 | Tokens/s: 153355\n",
      "Step 10728/55081 | Loss: 5.078 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 152423\n",
      "Step 10736/55081 | Loss: 4.928 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153495\n",
      "Step 10744/55081 | Loss: 4.913 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 152616\n",
      "Step 10752/55081 | Loss: 4.995 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153886\n",
      "Step 10760/55081 | Loss: 4.838 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152412\n",
      "Step 10768/55081 | Loss: 5.111 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153170\n",
      "Step 10776/55081 | Loss: 4.981 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153421\n",
      "Step 10784/55081 | Loss: 5.062 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153613\n",
      "Step 10792/55081 | Loss: 4.879 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 153619\n",
      "Step 10800/55081 | Loss: 4.892 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 151465\n",
      "Step 10808/55081 | Loss: 4.950 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153651\n",
      "Step 10816/55081 | Loss: 4.951 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 152361\n",
      "Step 10824/55081 | Loss: 4.934 | Norm: 0.103 | LR: 5.0000e-05 | Tokens/s: 152925\n",
      "Step 10832/55081 | Loss: 4.953 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 153386\n",
      "Step 10840/55081 | Loss: 4.922 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153314\n",
      "Step 10848/55081 | Loss: 4.869 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 153381\n",
      "Step 10856/55081 | Loss: 4.955 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153691\n",
      "Step 10864/55081 | Loss: 4.997 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153128\n",
      "Step 10872/55081 | Loss: 4.955 | Norm: 0.053 | LR: 5.0000e-05 | Tokens/s: 153261\n",
      "Step 10880/55081 | Loss: 4.914 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153747\n",
      "Step 10888/55081 | Loss: 5.009 | Norm: 0.103 | LR: 5.0000e-05 | Tokens/s: 152473\n",
      "Step 10896/55081 | Loss: 5.116 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 154036\n",
      "Step 10904/55081 | Loss: 5.047 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152513\n",
      "Step 10912/55081 | Loss: 4.971 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153778\n",
      "Step 10920/55081 | Loss: 5.011 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 153362\n",
      "Step 10928/55081 | Loss: 5.003 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 152541\n",
      "Step 10936/55081 | Loss: 5.038 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153478\n",
      "Step 10944/55081 | Loss: 5.086 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153328\n",
      "Step 10952/55081 | Loss: 4.895 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153207\n",
      "Step 10960/55081 | Loss: 5.017 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153412\n",
      "Step 10968/55081 | Loss: 4.949 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153376\n",
      "Step 10976/55081 | Loss: 4.927 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 152537\n",
      "Step 10984/55081 | Loss: 4.957 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153805\n",
      "Step 10992/55081 | Loss: 4.956 | Norm: 0.109 | LR: 5.0000e-05 | Tokens/s: 152206\n",
      "Step 11000/55081 | Loss: 4.908 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153392\n",
      "Epoch 11000 | Training checkpoint saved at models/tinyllm/checkpoint_11000.pt\n",
      "Step 11008/55081 | Loss: 5.921 | Norm: 0.341 | LR: 5.0000e-05 | Tokens/s: 153593\n",
      "Step 11016/55081 | Loss: 5.747 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 152442\n",
      "Step 11024/55081 | Loss: 4.984 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153762\n",
      "Step 11032/55081 | Loss: 5.121 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 154061\n",
      "Step 11040/55081 | Loss: 5.017 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 153194\n",
      "Step 11048/55081 | Loss: 5.160 | Norm: 0.201 | LR: 5.0000e-05 | Tokens/s: 153516\n",
      "Step 11056/55081 | Loss: 5.129 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153406\n",
      "Step 11064/55081 | Loss: 4.918 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153711\n",
      "Step 11072/55081 | Loss: 4.946 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 152514\n",
      "Step 11080/55081 | Loss: 4.927 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 154071\n",
      "Step 11088/55081 | Loss: 5.041 | Norm: 0.105 | LR: 5.0000e-05 | Tokens/s: 152890\n",
      "Step 11096/55081 | Loss: 4.915 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153414\n",
      "Step 11104/55081 | Loss: 4.993 | Norm: 0.116 | LR: 5.0000e-05 | Tokens/s: 153761\n",
      "Step 11112/55081 | Loss: 4.883 | Norm: 0.117 | LR: 5.0000e-05 | Tokens/s: 153584\n",
      "Step 11120/55081 | Loss: 4.992 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153435\n",
      "Step 11128/55081 | Loss: 4.944 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 154124\n",
      "Step 11136/55081 | Loss: 5.052 | Norm: 0.056 | LR: 5.0000e-05 | Tokens/s: 153264\n",
      "Step 11144/55081 | Loss: 4.926 | Norm: 0.107 | LR: 5.0000e-05 | Tokens/s: 154246\n",
      "Step 11152/55081 | Loss: 5.012 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 152684\n",
      "Step 11160/55081 | Loss: 5.127 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153521\n",
      "Step 11168/55081 | Loss: 4.929 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153561\n",
      "Step 11176/55081 | Loss: 4.960 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152902\n",
      "Step 11184/55081 | Loss: 5.084 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153510\n",
      "Step 11192/55081 | Loss: 5.149 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 152596\n",
      "Step 11200/55081 | Loss: 5.015 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 152867\n",
      "Step 11208/55081 | Loss: 4.909 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153473\n",
      "Step 11216/55081 | Loss: 5.134 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153417\n",
      "Step 11224/55081 | Loss: 5.017 | Norm: 0.190 | LR: 5.0000e-05 | Tokens/s: 153338\n",
      "Step 11232/55081 | Loss: 5.006 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153504\n",
      "Step 11240/55081 | Loss: 4.985 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 154128\n",
      "Step 11248/55081 | Loss: 4.936 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153218\n",
      "Step 11256/55081 | Loss: 4.966 | Norm: 0.128 | LR: 5.0000e-05 | Tokens/s: 153631\n",
      "Step 11264/55081 | Loss: 4.914 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 152519\n",
      "Step 11272/55081 | Loss: 5.241 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 153548\n",
      "Step 11280/55081 | Loss: 5.031 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153737\n",
      "Step 11288/55081 | Loss: 5.026 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 154137\n",
      "Step 11296/55081 | Loss: 4.972 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153842\n",
      "Step 11304/55081 | Loss: 4.960 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153119\n",
      "Step 11312/55081 | Loss: 4.971 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 152958\n",
      "Step 11320/55081 | Loss: 5.081 | Norm: 0.171 | LR: 5.0000e-05 | Tokens/s: 152763\n",
      "Step 11328/55081 | Loss: 5.069 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153173\n",
      "Step 11336/55081 | Loss: 4.937 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 151576\n",
      "Step 11344/55081 | Loss: 5.126 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 152941\n",
      "Step 11352/55081 | Loss: 4.935 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 153282\n",
      "Step 11360/55081 | Loss: 4.980 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153586\n",
      "Step 11368/55081 | Loss: 4.898 | Norm: 0.133 | LR: 5.0000e-05 | Tokens/s: 153917\n",
      "Step 11376/55081 | Loss: 4.899 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153524\n",
      "Step 11384/55081 | Loss: 4.872 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 154088\n",
      "Step 11392/55081 | Loss: 4.960 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153795\n",
      "Step 11400/55081 | Loss: 4.919 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153530\n",
      "Step 11408/55081 | Loss: 4.900 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153017\n",
      "Step 11416/55081 | Loss: 4.967 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153654\n",
      "Step 11424/55081 | Loss: 4.967 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152583\n",
      "Step 11432/55081 | Loss: 4.982 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153557\n",
      "Step 11440/55081 | Loss: 4.897 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 152980\n",
      "Step 11448/55081 | Loss: 4.970 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153633\n",
      "Step 11456/55081 | Loss: 4.942 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153989\n",
      "Step 11464/55081 | Loss: 5.033 | Norm: 0.103 | LR: 5.0000e-05 | Tokens/s: 152843\n",
      "Step 11472/55081 | Loss: 4.977 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153848\n",
      "Step 11480/55081 | Loss: 4.895 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 153056\n",
      "Step 11488/55081 | Loss: 5.020 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 153796\n",
      "Step 11496/55081 | Loss: 5.056 | Norm: 0.155 | LR: 5.0000e-05 | Tokens/s: 153838\n",
      "Epoch 11500 | Training checkpoint saved at models/tinyllm/checkpoint_11500.pt\n",
      "Step 11504/55081 | Loss: 4.933 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 152986\n",
      "Step 11512/55081 | Loss: 4.962 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 154052\n",
      "Step 11520/55081 | Loss: 4.918 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152901\n",
      "Step 11528/55081 | Loss: 4.987 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153717\n",
      "Step 11536/55081 | Loss: 4.918 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152249\n",
      "Step 11544/55081 | Loss: 4.972 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153760\n",
      "Step 11552/55081 | Loss: 4.974 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153622\n",
      "Step 11560/55081 | Loss: 4.909 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153696\n",
      "Step 11568/55081 | Loss: 4.918 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 154175\n",
      "Step 11576/55081 | Loss: 4.999 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 152633\n",
      "Step 11584/55081 | Loss: 4.866 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152036\n",
      "Step 11592/55081 | Loss: 5.034 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153611\n",
      "Step 11600/55081 | Loss: 4.959 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153722\n",
      "Step 11608/55081 | Loss: 5.078 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152778\n",
      "Step 11616/55081 | Loss: 5.055 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 153331\n",
      "Step 11624/55081 | Loss: 5.051 | Norm: 0.111 | LR: 5.0000e-05 | Tokens/s: 152581\n",
      "Step 11632/55081 | Loss: 5.177 | Norm: 0.056 | LR: 5.0000e-05 | Tokens/s: 153963\n",
      "Step 11640/55081 | Loss: 4.928 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153586\n",
      "Step 11648/55081 | Loss: 4.910 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153828\n",
      "Step 11656/55081 | Loss: 4.903 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153827\n",
      "Step 11664/55081 | Loss: 4.902 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153212\n",
      "Step 11672/55081 | Loss: 5.086 | Norm: 0.222 | LR: 5.0000e-05 | Tokens/s: 153895\n",
      "Step 11680/55081 | Loss: 4.899 | Norm: 0.128 | LR: 5.0000e-05 | Tokens/s: 151335\n",
      "Step 11688/55081 | Loss: 5.178 | Norm: 0.162 | LR: 5.0000e-05 | Tokens/s: 152771\n",
      "Step 11696/55081 | Loss: 4.982 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153772\n",
      "Step 11704/55081 | Loss: 5.041 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 153457\n",
      "Step 11712/55081 | Loss: 4.873 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153285\n",
      "Step 11720/55081 | Loss: 4.844 | Norm: 0.156 | LR: 5.0000e-05 | Tokens/s: 153518\n",
      "Step 11728/55081 | Loss: 4.908 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153537\n",
      "Step 11736/55081 | Loss: 4.959 | Norm: 0.106 | LR: 5.0000e-05 | Tokens/s: 153469\n",
      "Step 11744/55081 | Loss: 5.101 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 154005\n",
      "Step 11752/55081 | Loss: 4.997 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 152600\n",
      "Step 11760/55081 | Loss: 4.947 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153295\n",
      "Step 11768/55081 | Loss: 5.043 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153003\n",
      "Step 11776/55081 | Loss: 4.966 | Norm: 0.146 | LR: 5.0000e-05 | Tokens/s: 153436\n",
      "Step 11784/55081 | Loss: 4.833 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 153246\n",
      "Step 11792/55081 | Loss: 4.899 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153310\n",
      "Step 11800/55081 | Loss: 4.905 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 153082\n",
      "Step 11808/55081 | Loss: 4.879 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152965\n",
      "Step 11816/55081 | Loss: 5.035 | Norm: 0.106 | LR: 5.0000e-05 | Tokens/s: 152770\n",
      "Step 11824/55081 | Loss: 4.907 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 153384\n",
      "Step 11832/55081 | Loss: 5.026 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 152551\n",
      "Step 11840/55081 | Loss: 4.901 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 152678\n",
      "Step 11848/55081 | Loss: 4.908 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 152617\n",
      "Step 11856/55081 | Loss: 5.069 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 152847\n",
      "Step 11864/55081 | Loss: 4.920 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153461\n",
      "Step 11872/55081 | Loss: 4.890 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 152741\n",
      "Step 11880/55081 | Loss: 5.018 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153396\n",
      "Step 11888/55081 | Loss: 4.963 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 68755\n",
      "Step 11896/55081 | Loss: 4.908 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 152698\n",
      "Step 11904/55081 | Loss: 4.848 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153193\n",
      "Step 11912/55081 | Loss: 4.972 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153330\n",
      "Step 11920/55081 | Loss: 5.137 | Norm: 0.114 | LR: 5.0000e-05 | Tokens/s: 153634\n",
      "Step 11928/55081 | Loss: 4.834 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 154100\n",
      "Step 11936/55081 | Loss: 4.964 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 152537\n",
      "Step 11944/55081 | Loss: 4.973 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 154012\n",
      "Step 11952/55081 | Loss: 4.904 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153435\n",
      "Step 11960/55081 | Loss: 4.964 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153268\n",
      "Step 11968/55081 | Loss: 5.038 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153715\n",
      "Step 11976/55081 | Loss: 4.951 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 152203\n",
      "Step 11984/55081 | Loss: 4.930 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 152588\n",
      "Step 11992/55081 | Loss: 4.981 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 152856\n",
      "Step 12000/55081 | Loss: 4.904 | Norm: 0.106 | LR: 5.0000e-05 | Tokens/s: 152681\n",
      "Epoch 12000 | Training checkpoint saved at models/tinyllm/checkpoint_12000.pt\n",
      "Step 12008/55081 | Loss: 4.978 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153232\n",
      "Step 12016/55081 | Loss: 4.986 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153649\n",
      "Step 12024/55081 | Loss: 5.015 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153019\n",
      "Step 12032/55081 | Loss: 5.031 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 152879\n",
      "Step 12040/55081 | Loss: 4.922 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 152748\n",
      "Step 12048/55081 | Loss: 4.974 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153451\n",
      "Step 12056/55081 | Loss: 4.959 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 152545\n",
      "Step 12064/55081 | Loss: 4.960 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153145\n",
      "Step 12072/55081 | Loss: 5.052 | Norm: 0.115 | LR: 5.0000e-05 | Tokens/s: 153823\n",
      "Step 12080/55081 | Loss: 5.179 | Norm: 0.136 | LR: 5.0000e-05 | Tokens/s: 74386\n",
      "Step 12088/55081 | Loss: 5.181 | Norm: 0.118 | LR: 5.0000e-05 | Tokens/s: 153836\n",
      "Step 12096/55081 | Loss: 5.028 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153244\n",
      "Step 12104/55081 | Loss: 5.020 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153810\n",
      "Step 12112/55081 | Loss: 4.904 | Norm: 0.096 | LR: 5.0000e-05 | Tokens/s: 152479\n",
      "Step 12120/55081 | Loss: 4.859 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152811\n",
      "Step 12128/55081 | Loss: 4.982 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152193\n",
      "Step 12136/55081 | Loss: 5.185 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153666\n",
      "Step 12144/55081 | Loss: 5.050 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153488\n",
      "Step 12152/55081 | Loss: 4.996 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153860\n",
      "Step 12160/55081 | Loss: 5.040 | Norm: 0.115 | LR: 5.0000e-05 | Tokens/s: 153464\n",
      "Step 12168/55081 | Loss: 4.893 | Norm: 0.255 | LR: 5.0000e-05 | Tokens/s: 153478\n",
      "Step 12176/55081 | Loss: 4.904 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153302\n",
      "Step 12184/55081 | Loss: 5.057 | Norm: 0.132 | LR: 5.0000e-05 | Tokens/s: 153115\n",
      "Step 12192/55081 | Loss: 5.081 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 153578\n",
      "Step 12200/55081 | Loss: 5.123 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153823\n",
      "Step 12208/55081 | Loss: 4.887 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153590\n",
      "Step 12216/55081 | Loss: 5.041 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153669\n",
      "Step 12224/55081 | Loss: 4.988 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 152383\n",
      "Step 12232/55081 | Loss: 5.015 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 154150\n",
      "Step 12240/55081 | Loss: 4.981 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 152800\n",
      "Step 12248/55081 | Loss: 4.908 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153764\n",
      "Step 12256/55081 | Loss: 5.078 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153519\n",
      "Step 12264/55081 | Loss: 5.079 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153880\n",
      "Step 12272/55081 | Loss: 5.110 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152254\n",
      "Step 12280/55081 | Loss: 4.952 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 152674\n",
      "Step 12288/55081 | Loss: 5.124 | Norm: 0.115 | LR: 5.0000e-05 | Tokens/s: 153496\n",
      "Step 12296/55081 | Loss: 4.971 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153181\n",
      "Step 12304/55081 | Loss: 4.962 | Norm: 0.096 | LR: 5.0000e-05 | Tokens/s: 153820\n",
      "Step 12312/55081 | Loss: 4.978 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153147\n",
      "Step 12320/55081 | Loss: 4.992 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 153419\n",
      "Step 12328/55081 | Loss: 4.927 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153546\n",
      "Step 12336/55081 | Loss: 4.955 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 153300\n",
      "Step 12344/55081 | Loss: 4.951 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153344\n",
      "Step 12352/55081 | Loss: 4.933 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153076\n",
      "Step 12360/55081 | Loss: 5.002 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153305\n",
      "Step 12368/55081 | Loss: 4.972 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 152878\n",
      "Step 12376/55081 | Loss: 4.923 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153402\n",
      "Step 12384/55081 | Loss: 4.980 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 152524\n",
      "Step 12392/55081 | Loss: 4.954 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153674\n",
      "Step 12400/55081 | Loss: 5.017 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153688\n",
      "Step 12408/55081 | Loss: 5.012 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153504\n",
      "Step 12416/55081 | Loss: 5.039 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153691\n",
      "Step 12424/55081 | Loss: 4.971 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 153551\n",
      "Step 12432/55081 | Loss: 4.922 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 152950\n",
      "Step 12440/55081 | Loss: 4.895 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 153633\n",
      "Step 12448/55081 | Loss: 5.099 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153345\n",
      "Step 12456/55081 | Loss: 4.915 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153349\n",
      "Step 12464/55081 | Loss: 4.965 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153174\n",
      "Step 12472/55081 | Loss: 4.957 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153028\n",
      "Step 12480/55081 | Loss: 5.051 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153295\n",
      "Step 12488/55081 | Loss: 5.011 | Norm: 0.240 | LR: 5.0000e-05 | Tokens/s: 152374\n",
      "Step 12496/55081 | Loss: 5.266 | Norm: 0.233 | LR: 5.0000e-05 | Tokens/s: 154128\n",
      "Epoch 12500 | Training checkpoint saved at models/tinyllm/checkpoint_12500.pt\n",
      "Step 12504/55081 | Loss: 5.226 | Norm: 0.166 | LR: 5.0000e-05 | Tokens/s: 152844\n",
      "Step 12512/55081 | Loss: 5.038 | Norm: 0.241 | LR: 5.0000e-05 | Tokens/s: 153546\n",
      "Step 12520/55081 | Loss: 5.181 | Norm: 0.238 | LR: 5.0000e-05 | Tokens/s: 153738\n",
      "Step 12528/55081 | Loss: 5.157 | Norm: 0.242 | LR: 5.0000e-05 | Tokens/s: 153627\n",
      "Step 12536/55081 | Loss: 5.318 | Norm: 0.243 | LR: 5.0000e-05 | Tokens/s: 153205\n",
      "Step 12544/55081 | Loss: 5.169 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153145\n",
      "Step 12552/55081 | Loss: 5.153 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 152537\n",
      "Step 12560/55081 | Loss: 4.922 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 151669\n",
      "Step 12568/55081 | Loss: 5.017 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 152310\n",
      "Step 12576/55081 | Loss: 4.923 | Norm: 0.109 | LR: 5.0000e-05 | Tokens/s: 154025\n",
      "Step 12584/55081 | Loss: 4.944 | Norm: 0.111 | LR: 5.0000e-05 | Tokens/s: 152647\n",
      "Step 12592/55081 | Loss: 5.047 | Norm: 0.107 | LR: 5.0000e-05 | Tokens/s: 153436\n",
      "Step 12600/55081 | Loss: 5.002 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153155\n",
      "Step 12608/55081 | Loss: 5.024 | Norm: 0.122 | LR: 5.0000e-05 | Tokens/s: 153471\n",
      "Step 12616/55081 | Loss: 4.959 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153128\n",
      "Step 12624/55081 | Loss: 4.937 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 152936\n",
      "Step 12632/55081 | Loss: 4.967 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153415\n",
      "Step 12640/55081 | Loss: 4.917 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 152821\n",
      "Step 12648/55081 | Loss: 4.939 | Norm: 0.111 | LR: 5.0000e-05 | Tokens/s: 153497\n",
      "Step 12656/55081 | Loss: 4.980 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153124\n",
      "Step 12664/55081 | Loss: 4.916 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153498\n",
      "Step 12672/55081 | Loss: 4.881 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 152853\n",
      "Step 12680/55081 | Loss: 4.881 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153841\n",
      "Step 12688/55081 | Loss: 4.929 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 153373\n",
      "Step 12696/55081 | Loss: 4.853 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 151554\n",
      "Step 12704/55081 | Loss: 4.893 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152898\n",
      "Step 12712/55081 | Loss: 4.974 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153344\n",
      "Step 12720/55081 | Loss: 5.109 | Norm: 0.127 | LR: 5.0000e-05 | Tokens/s: 153393\n",
      "Step 12728/55081 | Loss: 5.180 | Norm: 0.142 | LR: 5.0000e-05 | Tokens/s: 152169\n",
      "Step 12736/55081 | Loss: 5.088 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153419\n",
      "Step 12744/55081 | Loss: 4.986 | Norm: 0.097 | LR: 5.0000e-05 | Tokens/s: 152923\n",
      "Step 12752/55081 | Loss: 4.855 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153239\n",
      "Step 12760/55081 | Loss: 4.921 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153791\n",
      "Step 12768/55081 | Loss: 4.862 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 152254\n",
      "Step 12776/55081 | Loss: 4.957 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153271\n",
      "Step 12784/55081 | Loss: 4.964 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153456\n",
      "Step 12792/55081 | Loss: 4.948 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153243\n",
      "Step 12800/55081 | Loss: 5.261 | Norm: 0.124 | LR: 5.0000e-05 | Tokens/s: 152928\n",
      "Step 12808/55081 | Loss: 4.956 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153302\n",
      "Step 12816/55081 | Loss: 5.076 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153406\n",
      "Step 12824/55081 | Loss: 4.905 | Norm: 0.135 | LR: 5.0000e-05 | Tokens/s: 153312\n",
      "Step 12832/55081 | Loss: 4.912 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153670\n",
      "Step 12840/55081 | Loss: 4.890 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153194\n",
      "Step 12848/55081 | Loss: 4.877 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 152949\n",
      "Step 12856/55081 | Loss: 4.905 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153503\n",
      "Step 12864/55081 | Loss: 5.041 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153736\n",
      "Step 12872/55081 | Loss: 4.950 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 152482\n",
      "Step 12880/55081 | Loss: 4.968 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153386\n",
      "Step 12888/55081 | Loss: 4.922 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 152652\n",
      "Step 12896/55081 | Loss: 4.960 | Norm: 0.237 | LR: 5.0000e-05 | Tokens/s: 153553\n",
      "Step 12904/55081 | Loss: 4.969 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 153575\n",
      "Step 12912/55081 | Loss: 4.961 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153598\n",
      "Step 12920/55081 | Loss: 5.053 | Norm: 0.119 | LR: 5.0000e-05 | Tokens/s: 153725\n",
      "Step 12928/55081 | Loss: 5.035 | Norm: 0.129 | LR: 5.0000e-05 | Tokens/s: 152379\n",
      "Step 12936/55081 | Loss: 5.050 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 152096\n",
      "Step 12944/55081 | Loss: 5.084 | Norm: 0.107 | LR: 5.0000e-05 | Tokens/s: 153125\n",
      "Step 12952/55081 | Loss: 4.975 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153259\n",
      "Step 12960/55081 | Loss: 4.838 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153869\n",
      "Step 12968/55081 | Loss: 5.083 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 153598\n",
      "Step 12976/55081 | Loss: 4.930 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 152365\n",
      "Step 12984/55081 | Loss: 5.016 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153639\n",
      "Step 12992/55081 | Loss: 4.945 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153119\n",
      "Step 13000/55081 | Loss: 4.880 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153480\n",
      "Epoch 13000 | Training checkpoint saved at models/tinyllm/checkpoint_13000.pt\n",
      "Step 13008/55081 | Loss: 5.000 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153641\n",
      "Step 13016/55081 | Loss: 4.933 | Norm: 0.099 | LR: 5.0000e-05 | Tokens/s: 153700\n",
      "Step 13024/55081 | Loss: 4.878 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153389\n",
      "Step 13032/55081 | Loss: 4.835 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153315\n",
      "Step 13040/55081 | Loss: 4.976 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153444\n",
      "Step 13048/55081 | Loss: 4.954 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153568\n",
      "Step 13056/55081 | Loss: 4.893 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 152082\n",
      "Step 13064/55081 | Loss: 4.974 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153077\n",
      "Step 13072/55081 | Loss: 4.957 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 151822\n",
      "Step 13080/55081 | Loss: 4.925 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153365\n",
      "Step 13088/55081 | Loss: 4.946 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 152607\n",
      "Step 13096/55081 | Loss: 4.977 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 151332\n",
      "Step 13104/55081 | Loss: 5.047 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 154173\n",
      "Step 13112/55081 | Loss: 4.928 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153044\n",
      "Step 13120/55081 | Loss: 4.918 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153647\n",
      "Step 13128/55081 | Loss: 4.975 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153348\n",
      "Step 13136/55081 | Loss: 4.946 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153106\n",
      "Step 13144/55081 | Loss: 4.862 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153458\n",
      "Step 13152/55081 | Loss: 4.909 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153333\n",
      "Step 13160/55081 | Loss: 4.935 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153709\n",
      "Step 13168/55081 | Loss: 4.940 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153149\n",
      "Step 13176/55081 | Loss: 4.937 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153506\n",
      "Step 13184/55081 | Loss: 4.941 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152376\n",
      "Step 13192/55081 | Loss: 4.944 | Norm: 0.085 | LR: 5.0000e-05 | Tokens/s: 152623\n",
      "Step 13200/55081 | Loss: 5.035 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 152455\n",
      "Step 13208/55081 | Loss: 4.930 | Norm: 0.104 | LR: 5.0000e-05 | Tokens/s: 153615\n",
      "Step 13216/55081 | Loss: 4.943 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152158\n",
      "Step 13224/55081 | Loss: 5.137 | Norm: 0.311 | LR: 5.0000e-05 | Tokens/s: 153323\n",
      "Step 13232/55081 | Loss: 5.350 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 152847\n",
      "Step 13240/55081 | Loss: 4.938 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 154009\n",
      "Step 13248/55081 | Loss: 4.947 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153514\n",
      "Step 13256/55081 | Loss: 4.921 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 152998\n",
      "Step 13264/55081 | Loss: 4.985 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153250\n",
      "Step 13272/55081 | Loss: 4.940 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153697\n",
      "Step 13280/55081 | Loss: 4.921 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 153447\n",
      "Step 13288/55081 | Loss: 5.163 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152666\n",
      "Step 13296/55081 | Loss: 4.977 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153317\n",
      "Step 13304/55081 | Loss: 4.997 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152682\n",
      "Step 13312/55081 | Loss: 4.960 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 154097\n",
      "Step 13320/55081 | Loss: 4.953 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 152684\n",
      "Step 13328/55081 | Loss: 4.993 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153611\n",
      "Step 13336/55081 | Loss: 5.152 | Norm: 0.114 | LR: 5.0000e-05 | Tokens/s: 153140\n",
      "Step 13344/55081 | Loss: 5.280 | Norm: 0.137 | LR: 5.0000e-05 | Tokens/s: 153613\n",
      "Step 13352/55081 | Loss: 5.248 | Norm: 0.137 | LR: 5.0000e-05 | Tokens/s: 153603\n",
      "Step 13360/55081 | Loss: 5.118 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153368\n",
      "Step 13368/55081 | Loss: 5.122 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153946\n",
      "Step 13376/55081 | Loss: 5.135 | Norm: 0.128 | LR: 5.0000e-05 | Tokens/s: 153792\n",
      "Step 13384/55081 | Loss: 5.186 | Norm: 0.125 | LR: 5.0000e-05 | Tokens/s: 153297\n",
      "Step 13392/55081 | Loss: 5.134 | Norm: 0.119 | LR: 5.0000e-05 | Tokens/s: 153702\n",
      "Step 13400/55081 | Loss: 5.162 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 153517\n",
      "Step 13408/55081 | Loss: 5.176 | Norm: 0.123 | LR: 5.0000e-05 | Tokens/s: 153261\n",
      "Step 13416/55081 | Loss: 5.198 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153855\n",
      "Step 13424/55081 | Loss: 5.199 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153788\n",
      "Step 13432/55081 | Loss: 5.094 | Norm: 0.166 | LR: 5.0000e-05 | Tokens/s: 153074\n",
      "Step 13440/55081 | Loss: 4.864 | Norm: 0.134 | LR: 5.0000e-05 | Tokens/s: 153770\n",
      "Step 13448/55081 | Loss: 4.923 | Norm: 0.090 | LR: 5.0000e-05 | Tokens/s: 152896\n",
      "Step 13456/55081 | Loss: 4.917 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 154058\n",
      "Step 13464/55081 | Loss: 5.172 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 152507\n",
      "Step 13472/55081 | Loss: 4.989 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 152902\n",
      "Step 13480/55081 | Loss: 4.996 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153631\n",
      "Step 13488/55081 | Loss: 5.004 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153537\n",
      "Step 13496/55081 | Loss: 4.926 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153740\n",
      "Epoch 13500 | Training checkpoint saved at models/tinyllm/checkpoint_13500.pt\n",
      "Step 13504/55081 | Loss: 4.993 | Norm: 0.105 | LR: 5.0000e-05 | Tokens/s: 152820\n",
      "Step 13512/55081 | Loss: 4.897 | Norm: 0.142 | LR: 5.0000e-05 | Tokens/s: 153040\n",
      "Step 13520/55081 | Loss: 4.892 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153521\n",
      "Step 13528/55081 | Loss: 4.964 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153515\n",
      "Step 13536/55081 | Loss: 5.027 | Norm: 0.111 | LR: 5.0000e-05 | Tokens/s: 153532\n",
      "Step 13544/55081 | Loss: 4.931 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 153316\n",
      "Step 13552/55081 | Loss: 4.983 | Norm: 0.097 | LR: 5.0000e-05 | Tokens/s: 153550\n",
      "Step 13560/55081 | Loss: 4.887 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153154\n",
      "Step 13568/55081 | Loss: 5.022 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 153710\n",
      "Step 13576/55081 | Loss: 4.981 | Norm: 0.095 | LR: 5.0000e-05 | Tokens/s: 153715\n",
      "Step 13584/55081 | Loss: 4.971 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153597\n",
      "Step 13592/55081 | Loss: 4.941 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153483\n",
      "Step 13600/55081 | Loss: 4.920 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 153792\n",
      "Step 13608/55081 | Loss: 4.901 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153639\n",
      "Step 13616/55081 | Loss: 4.836 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153500\n",
      "Step 13624/55081 | Loss: 4.845 | Norm: 0.062 | LR: 5.0000e-05 | Tokens/s: 153454\n",
      "Step 13632/55081 | Loss: 4.980 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 152815\n",
      "Step 13640/55081 | Loss: 4.850 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153496\n",
      "Step 13648/55081 | Loss: 4.949 | Norm: 0.098 | LR: 5.0000e-05 | Tokens/s: 152854\n",
      "Step 13656/55081 | Loss: 4.925 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153488\n",
      "Step 13664/55081 | Loss: 5.024 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 153197\n",
      "Step 13672/55081 | Loss: 4.988 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153859\n",
      "Step 13680/55081 | Loss: 4.989 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153111\n",
      "Step 13688/55081 | Loss: 4.975 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153903\n",
      "Step 13696/55081 | Loss: 4.967 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153602\n",
      "Step 13704/55081 | Loss: 4.896 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 154194\n",
      "Step 13712/55081 | Loss: 4.913 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153272\n",
      "Step 13720/55081 | Loss: 5.135 | Norm: 0.254 | LR: 5.0000e-05 | Tokens/s: 152381\n",
      "Step 13728/55081 | Loss: 4.838 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153700\n",
      "Step 13736/55081 | Loss: 4.951 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 152780\n",
      "Step 13744/55081 | Loss: 4.820 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 152933\n",
      "Step 13752/55081 | Loss: 4.971 | Norm: 0.143 | LR: 5.0000e-05 | Tokens/s: 153197\n",
      "Step 13760/55081 | Loss: 4.995 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153616\n",
      "Step 13768/55081 | Loss: 4.983 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153847\n",
      "Step 13776/55081 | Loss: 4.991 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 152927\n",
      "Step 13784/55081 | Loss: 5.026 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 153536\n",
      "Step 13792/55081 | Loss: 5.281 | Norm: 0.113 | LR: 5.0000e-05 | Tokens/s: 152962\n",
      "Step 13800/55081 | Loss: 5.025 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153284\n",
      "Step 13808/55081 | Loss: 4.929 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 153716\n",
      "Step 13816/55081 | Loss: 4.939 | Norm: 0.127 | LR: 5.0000e-05 | Tokens/s: 153619\n",
      "Step 13824/55081 | Loss: 4.632 | Norm: 0.111 | LR: 5.0000e-05 | Tokens/s: 153183\n",
      "Step 13832/55081 | Loss: 4.804 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 152488\n",
      "Step 13840/55081 | Loss: 4.987 | Norm: 0.060 | LR: 5.0000e-05 | Tokens/s: 152439\n",
      "Step 13848/55081 | Loss: 4.942 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153445\n",
      "Step 13856/55081 | Loss: 5.054 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 153666\n",
      "Step 13864/55081 | Loss: 4.933 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153015\n",
      "Step 13872/55081 | Loss: 4.969 | Norm: 0.102 | LR: 5.0000e-05 | Tokens/s: 153249\n",
      "Step 13880/55081 | Loss: 5.216 | Norm: 0.269 | LR: 5.0000e-05 | Tokens/s: 152141\n",
      "Step 13888/55081 | Loss: 5.039 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153065\n",
      "Step 13896/55081 | Loss: 5.006 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 152953\n",
      "Step 13904/55081 | Loss: 5.082 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153683\n",
      "Step 13912/55081 | Loss: 4.881 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153445\n",
      "Step 13920/55081 | Loss: 4.937 | Norm: 0.084 | LR: 5.0000e-05 | Tokens/s: 151922\n",
      "Step 13928/55081 | Loss: 4.994 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152581\n",
      "Step 13936/55081 | Loss: 5.021 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153791\n",
      "Step 13944/55081 | Loss: 4.998 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 154124\n",
      "Step 13952/55081 | Loss: 4.959 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152549\n",
      "Step 13960/55081 | Loss: 4.997 | Norm: 0.134 | LR: 5.0000e-05 | Tokens/s: 153107\n",
      "Step 13968/55081 | Loss: 4.805 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 151842\n",
      "Step 13976/55081 | Loss: 5.017 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153582\n",
      "Step 13984/55081 | Loss: 4.958 | Norm: 0.095 | LR: 5.0000e-05 | Tokens/s: 150791\n",
      "Step 13992/55081 | Loss: 5.021 | Norm: 0.087 | LR: 5.0000e-05 | Tokens/s: 154156\n",
      "Step 14000/55081 | Loss: 4.912 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153286\n",
      "Epoch 14000 | Training checkpoint saved at models/tinyllm/checkpoint_14000.pt\n",
      "Step 14008/55081 | Loss: 4.958 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 153762\n",
      "Step 14016/55081 | Loss: 4.920 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153082\n",
      "Step 14024/55081 | Loss: 5.008 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153440\n",
      "Step 14032/55081 | Loss: 4.982 | Norm: 0.136 | LR: 5.0000e-05 | Tokens/s: 152988\n",
      "Step 14040/55081 | Loss: 5.223 | Norm: 0.130 | LR: 5.0000e-05 | Tokens/s: 153667\n",
      "Step 14048/55081 | Loss: 4.986 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152764\n",
      "Step 14056/55081 | Loss: 4.993 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153158\n",
      "Step 14064/55081 | Loss: 4.922 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 152987\n",
      "Step 14072/55081 | Loss: 4.983 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153496\n",
      "Step 14080/55081 | Loss: 4.950 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152876\n",
      "Step 14088/55081 | Loss: 5.008 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153588\n",
      "Step 14096/55081 | Loss: 4.951 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153479\n",
      "Step 14104/55081 | Loss: 4.933 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153774\n",
      "Step 14112/55081 | Loss: 4.890 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153350\n",
      "Step 14120/55081 | Loss: 4.940 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 152668\n",
      "Step 14128/55081 | Loss: 4.919 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153294\n",
      "Step 14136/55081 | Loss: 4.992 | Norm: 0.093 | LR: 5.0000e-05 | Tokens/s: 152791\n",
      "Step 14144/55081 | Loss: 5.007 | Norm: 0.114 | LR: 5.0000e-05 | Tokens/s: 154010\n",
      "Step 14152/55081 | Loss: 4.973 | Norm: 0.124 | LR: 5.0000e-05 | Tokens/s: 152343\n",
      "Step 14160/55081 | Loss: 5.026 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 152037\n",
      "Step 14168/55081 | Loss: 4.931 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152381\n",
      "Step 14176/55081 | Loss: 5.001 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153245\n",
      "Step 14184/55081 | Loss: 4.918 | Norm: 0.069 | LR: 5.0000e-05 | Tokens/s: 152610\n",
      "Step 14192/55081 | Loss: 4.955 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153814\n",
      "Step 14200/55081 | Loss: 4.943 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 152805\n",
      "Step 14208/55081 | Loss: 5.047 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152694\n",
      "Step 14216/55081 | Loss: 4.881 | Norm: 0.071 | LR: 5.0000e-05 | Tokens/s: 153372\n",
      "Step 14224/55081 | Loss: 4.956 | Norm: 0.119 | LR: 5.0000e-05 | Tokens/s: 153772\n",
      "Step 14232/55081 | Loss: 4.980 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153171\n",
      "Step 14240/55081 | Loss: 4.991 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 153389\n",
      "Step 14248/55081 | Loss: 5.037 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153271\n",
      "Step 14256/55081 | Loss: 5.249 | Norm: 0.095 | LR: 5.0000e-05 | Tokens/s: 153863\n",
      "Step 14264/55081 | Loss: 5.318 | Norm: 0.222 | LR: 5.0000e-05 | Tokens/s: 153418\n",
      "Step 14272/55081 | Loss: 4.925 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 154151\n",
      "Step 14280/55081 | Loss: 4.874 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153264\n",
      "Step 14288/55081 | Loss: 5.099 | Norm: 0.216 | LR: 5.0000e-05 | Tokens/s: 153842\n",
      "Step 14296/55081 | Loss: 5.003 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 152417\n",
      "Step 14304/55081 | Loss: 4.898 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153843\n",
      "Step 14312/55081 | Loss: 4.956 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 152877\n",
      "Step 14320/55081 | Loss: 4.951 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153008\n",
      "Step 14328/55081 | Loss: 4.945 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153077\n",
      "Step 14336/55081 | Loss: 4.952 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 152785\n",
      "Step 14344/55081 | Loss: 5.034 | Norm: 0.108 | LR: 5.0000e-05 | Tokens/s: 153175\n",
      "Step 14352/55081 | Loss: 5.064 | Norm: 0.103 | LR: 5.0000e-05 | Tokens/s: 153200\n",
      "Step 14360/55081 | Loss: 4.836 | Norm: 0.089 | LR: 5.0000e-05 | Tokens/s: 153690\n",
      "Step 14368/55081 | Loss: 5.002 | Norm: 0.088 | LR: 5.0000e-05 | Tokens/s: 153105\n",
      "Step 14376/55081 | Loss: 5.012 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153347\n",
      "Step 14384/55081 | Loss: 4.959 | Norm: 0.059 | LR: 5.0000e-05 | Tokens/s: 152592\n",
      "Step 14392/55081 | Loss: 4.963 | Norm: 0.119 | LR: 5.0000e-05 | Tokens/s: 153507\n",
      "Step 14400/55081 | Loss: 4.834 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 152928\n",
      "Step 14408/55081 | Loss: 4.967 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153059\n",
      "Step 14416/55081 | Loss: 5.004 | Norm: 0.086 | LR: 5.0000e-05 | Tokens/s: 152932\n",
      "Step 14424/55081 | Loss: 4.973 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 152537\n",
      "Step 14432/55081 | Loss: 4.892 | Norm: 0.056 | LR: 5.0000e-05 | Tokens/s: 153118\n",
      "Step 14440/55081 | Loss: 4.930 | Norm: 0.115 | LR: 5.0000e-05 | Tokens/s: 152957\n",
      "Step 14448/55081 | Loss: 4.956 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153404\n",
      "Step 14456/55081 | Loss: 4.986 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 152630\n",
      "Step 14464/55081 | Loss: 4.919 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153364\n",
      "Step 14472/55081 | Loss: 5.040 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153142\n",
      "Step 14480/55081 | Loss: 4.937 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 151885\n",
      "Step 14488/55081 | Loss: 4.919 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153549\n",
      "Step 14496/55081 | Loss: 5.070 | Norm: 0.113 | LR: 5.0000e-05 | Tokens/s: 153420\n",
      "Epoch 14500 | Training checkpoint saved at models/tinyllm/checkpoint_14500.pt\n",
      "Step 14504/55081 | Loss: 4.951 | Norm: 0.081 | LR: 5.0000e-05 | Tokens/s: 153852\n",
      "Step 14512/55081 | Loss: 4.997 | Norm: 0.101 | LR: 5.0000e-05 | Tokens/s: 152787\n",
      "Step 14520/55081 | Loss: 4.946 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153978\n",
      "Step 14528/55081 | Loss: 4.982 | Norm: 0.128 | LR: 5.0000e-05 | Tokens/s: 153055\n",
      "Step 14536/55081 | Loss: 5.132 | Norm: 0.228 | LR: 5.0000e-05 | Tokens/s: 153581\n",
      "Step 14544/55081 | Loss: 4.967 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153578\n",
      "Step 14552/55081 | Loss: 4.926 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153757\n",
      "Step 14560/55081 | Loss: 4.917 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153153\n",
      "Step 14568/55081 | Loss: 5.016 | Norm: 0.141 | LR: 5.0000e-05 | Tokens/s: 152568\n",
      "Step 14576/55081 | Loss: 4.939 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153389\n",
      "Step 14584/55081 | Loss: 5.077 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 152877\n",
      "Step 14592/55081 | Loss: 4.866 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153680\n",
      "Step 14600/55081 | Loss: 4.915 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 154037\n",
      "Step 14608/55081 | Loss: 4.973 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153228\n",
      "Step 14616/55081 | Loss: 4.918 | Norm: 0.079 | LR: 5.0000e-05 | Tokens/s: 153856\n",
      "Step 14624/55081 | Loss: 4.935 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 152766\n",
      "Step 14632/55081 | Loss: 4.952 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153116\n",
      "Step 14640/55081 | Loss: 4.936 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 152991\n",
      "Step 14648/55081 | Loss: 4.915 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 152610\n",
      "Step 14656/55081 | Loss: 4.918 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153314\n",
      "Step 14664/55081 | Loss: 5.033 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153348\n",
      "Step 14672/55081 | Loss: 4.944 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153140\n",
      "Step 14680/55081 | Loss: 4.930 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153830\n",
      "Step 14688/55081 | Loss: 4.959 | Norm: 0.067 | LR: 5.0000e-05 | Tokens/s: 153190\n",
      "Step 14696/55081 | Loss: 4.946 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153373\n",
      "Step 14704/55081 | Loss: 4.851 | Norm: 0.105 | LR: 5.0000e-05 | Tokens/s: 153867\n",
      "Step 14712/55081 | Loss: 4.904 | Norm: 0.116 | LR: 5.0000e-05 | Tokens/s: 152534\n",
      "Step 14720/55081 | Loss: 4.964 | Norm: 0.147 | LR: 5.0000e-05 | Tokens/s: 153941\n",
      "Step 14728/55081 | Loss: 4.950 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 152764\n",
      "Step 14736/55081 | Loss: 4.909 | Norm: 0.094 | LR: 5.0000e-05 | Tokens/s: 153569\n",
      "Step 14744/55081 | Loss: 4.951 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 153832\n",
      "Step 14752/55081 | Loss: 5.000 | Norm: 0.063 | LR: 5.0000e-05 | Tokens/s: 152725\n",
      "Step 14760/55081 | Loss: 5.018 | Norm: 0.078 | LR: 5.0000e-05 | Tokens/s: 153322\n",
      "Step 14768/55081 | Loss: 4.901 | Norm: 0.057 | LR: 5.0000e-05 | Tokens/s: 153808\n",
      "Step 14776/55081 | Loss: 4.946 | Norm: 0.065 | LR: 5.0000e-05 | Tokens/s: 153287\n",
      "Step 14784/55081 | Loss: 4.965 | Norm: 0.070 | LR: 5.0000e-05 | Tokens/s: 153590\n",
      "Step 14792/55081 | Loss: 4.893 | Norm: 0.091 | LR: 5.0000e-05 | Tokens/s: 153644\n",
      "Step 14800/55081 | Loss: 4.844 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 152758\n",
      "Step 14808/55081 | Loss: 4.980 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 153476\n",
      "Step 14816/55081 | Loss: 4.933 | Norm: 0.083 | LR: 5.0000e-05 | Tokens/s: 152201\n",
      "Step 14824/55081 | Loss: 4.984 | Norm: 0.120 | LR: 5.0000e-05 | Tokens/s: 153630\n",
      "Step 14832/55081 | Loss: 4.925 | Norm: 0.076 | LR: 5.0000e-05 | Tokens/s: 153536\n",
      "Step 14840/55081 | Loss: 4.972 | Norm: 0.112 | LR: 5.0000e-05 | Tokens/s: 153332\n",
      "Step 14848/55081 | Loss: 4.932 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153683\n",
      "Step 14856/55081 | Loss: 4.925 | Norm: 0.053 | LR: 5.0000e-05 | Tokens/s: 153133\n",
      "Step 14864/55081 | Loss: 4.980 | Norm: 0.075 | LR: 5.0000e-05 | Tokens/s: 153245\n",
      "Step 14872/55081 | Loss: 4.920 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 152503\n",
      "Step 14880/55081 | Loss: 4.968 | Norm: 0.068 | LR: 5.0000e-05 | Tokens/s: 153345\n",
      "Step 14888/55081 | Loss: 4.850 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153672\n",
      "Step 14896/55081 | Loss: 4.938 | Norm: 0.061 | LR: 5.0000e-05 | Tokens/s: 152778\n",
      "Step 14904/55081 | Loss: 4.886 | Norm: 0.100 | LR: 5.0000e-05 | Tokens/s: 153364\n",
      "Step 14912/55081 | Loss: 4.865 | Norm: 0.077 | LR: 5.0000e-05 | Tokens/s: 153316\n",
      "Step 14920/55081 | Loss: 4.900 | Norm: 0.064 | LR: 5.0000e-05 | Tokens/s: 153398\n",
      "Step 14928/55081 | Loss: 4.940 | Norm: 0.092 | LR: 5.0000e-05 | Tokens/s: 153607\n",
      "Step 14936/55081 | Loss: 4.959 | Norm: 0.072 | LR: 5.0000e-05 | Tokens/s: 153274\n",
      "Step 14944/55081 | Loss: 4.891 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 151808\n",
      "Step 14952/55081 | Loss: 4.955 | Norm: 0.073 | LR: 5.0000e-05 | Tokens/s: 153919\n",
      "Step 14960/55081 | Loss: 5.011 | Norm: 0.119 | LR: 5.0000e-05 | Tokens/s: 152941\n",
      "Step 14968/55081 | Loss: 4.956 | Norm: 0.066 | LR: 5.0000e-05 | Tokens/s: 153992\n",
      "Step 14976/55081 | Loss: 4.938 | Norm: 0.080 | LR: 5.0000e-05 | Tokens/s: 153192\n",
      "Step 14984/55081 | Loss: 4.896 | Norm: 0.082 | LR: 5.0000e-05 | Tokens/s: 153313\n",
      "Step 14992/55081 | Loss: 4.904 | Norm: 0.058 | LR: 5.0000e-05 | Tokens/s: 153494\n",
      "Step 15000/55081 | Loss: 4.830 | Norm: 0.074 | LR: 5.0000e-05 | Tokens/s: 152512\n",
      "Epoch 15000 | Training checkpoint saved at models/tinyllm/checkpoint_15000.pt\n"
     ]
    }
   ],
   "source": [
    "model_params = tinyllm.ModelParams(vocab_size=len(tokenizer.vocab))\n",
    "print(f\"Model params: {dataclasses.asdict(model_params)}\")\n",
    "model = tinyllm.TinyLLM(model_params)\n",
    "model = model.to(model_params.device)\n",
    "print(f\"Number of parameters: {model._num_parameters():,}\")\n",
    "\n",
    "learning_rate = 3e-3\n",
    "batch_size = 8\n",
    "total_train_tokens = 226_052_077\n",
    "optimizer = tinyllm.configure_adamw_optimizer(model, weight_decay=1e-2, learning_rate=learning_rate, betas=(0.9, 0.95), eps=1e-8, device_type=model_params.device)\n",
    "training_dataset = tinyllm.TinyLLMDataset(shards_path=f\"{dataset_path}/shards\", T=model_params.context_length, split='train', total_tokens=total_train_tokens)\n",
    "training_dataloader = tinyllm.DataLoader(training_dataset, batch_size=batch_size, shuffle=False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_params = tinyllm.TrainerParams(model=model, train_data=training_dataloader, optimizer=optimizer, gpu_id=0, save_every=500, loss_fn=loss_fn, model_path=model_path)\n",
    "trainer = tinyllm.Trainer(train_params)\n",
    "model = trainer.train(steps=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- Plot the loss curve\n",
    "- Generate some samples\n",
    "- Calculate the Perplexity using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZjpJREFUeJzt3Xd0FNXfBvBn0wmQhJYGgYQeIHTBgAL+iAQEpShopEqxoQhKMa8FFSWiIqAoVUCkg3QQDFVK6Amd0AIJkISaJpA67x8xy26ybXZnd7Y8n3P2nGT2zsy9uzsz37lzi0IQBAFEREREMnGSOwNERETk2BiMEBERkawYjBAREZGsGIwQERGRrBiMEBERkawYjBAREZGsGIwQERGRrBiMEBERkaxc5M6AIYqKinDr1i1UrFgRCoVC7uwQERGRAQRBQHZ2NgIDA+HkpL3+wyaCkVu3biEoKEjubBAREZERUlJSUKNGDa3v20QwUrFiRQDFhfHy8pI5N0RERGSIrKwsBAUFKa/j2thEMFLyaMbLy4vBCBERkY3R18SCDViJiIhIVgxGiIiISFYMRoiIiEhWNtFmhIiIbIcgCCgoKEBhYaHcWSEzc3Z2houLi8nDbjAYISIiyeTl5SE1NRUPHz6UOytkIZ6enggICICbm5vR22AwQkREkigqKkJSUhKcnZ0RGBgINzc3DlRpxwRBQF5eHu7cuYOkpCTUq1dP58BmujAYISIiSeTl5aGoqAhBQUHw9PSUOztkAeXKlYOrqyuuX7+OvLw8eHh4GLUdUSFMYWEhPvvsM4SEhKBcuXKoU6cOJk2aBEEQdK63Z88etGzZEu7u7qhbty4WLVpkVGaJiMj6GXt3TLZJiu9bVM3IlClTMGvWLPz+++9o3Lgxjh07hjfeeAPe3t4YNWqUxnWSkpLQvXt3vP3221i6dCl27tyJ4cOHIyAgAJGRkSYXgIiIiGybqGDk4MGD6NmzJ7p37w4ACA4OxvLly3HkyBGt68yePRshISGYOnUqACA0NBT79+/HtGnTGIwQERGRuMc07dq1w86dO3Hx4kUAwMmTJ7F//35069ZN6zpxcXGIiIhQWxYZGYm4uDit6+Tm5iIrK0vtRURERPZJVDDy8ccf47XXXkPDhg3h6uqKFi1aYPTo0ejfv7/WddLS0uDn56e2zM/PD1lZWXj06JHGdWJiYuDt7a18ccZeIiIypyFDhqBXr15yZ0Np9+7deOGFF1ClShV4enqiUaNG+Oijj3Dz5k25s2YWooKRVatWYenSpVi2bBlOnDiB33//HT/88AN+//13STMVHR2NzMxM5SslJUXS7RPZuqRdSYhfGC93NojIDObMmYOIiAj4+/vjzz//xLlz5zB79mxkZmYqmzwYIy8vT8JcSktUMDJu3Dhl7UhYWBgGDhyIMWPGICYmRus6/v7+SE9PV1uWnp4OLy8vlCtXTuM67u7uyhl6OVMvUVmLOy/GxqEbkZaQJndWiHQSBAF5/+ZZ/KWvl6dYe/fuRZs2beDu7o6AgAB8/PHHKCgoUL6/Zs0ahIWFoVy5cqhSpQoiIiLw77//AijuUdqmTRuUL18ePj4+aN++Pa5fv65xPzdu3MCoUaMwatQoLFiwAJ06dUJwcDA6dOiA+fPn4/PPPwcAfPHFF2jevLnautOnT0dwcLDy/5Lanm+++QaBgYFo0KAB/u///g9t27Yts99mzZrhq6++Uv4/f/58hIaGwsPDAw0bNsSvv/5q7EdnEFENWB8+fFimC4+zszOKioq0rhMeHo6tW7eqLYuNjUV4eLiYXRORBpkpmfBv7i93Noi0yn+Yj5gK2m9YzSU6Jxpu5Y0fEVTVzZs38cILL2DIkCFYvHgxLly4gBEjRsDDwwNffPEFUlNTERUVhe+++w69e/dGdnY29u3bpxwWv1evXhgxYgSWL1+OvLw8HDlyROtgcKtXr0ZeXh7Gjx+v8X0fHx9Red+5cye8vLwQGxurXBYTE4MrV66gTp06AICzZ8/i1KlT+PPPPwEAS5cuxeeff46ZM2eiRYsWiI+Px4gRI1C+fHkMHjxY1P4NJSoYefHFF/HNN9+gZs2aaNy4MeLj4/Hjjz9i6NChyjTR0dG4efMmFi9eDAB4++23MXPmTIwfPx5Dhw7Frl27sGrVKmzZskXakhAREZnBr7/+iqCgIMycORMKhQINGzbErVu3MGHCBHz++edITU1FQUEB+vTpg1q1agEAwsLCAAD3799HZmYmevToobz4h4aGat3XpUuX4OXlhYCAAEnyXr58ecyfP19tqPZmzZph2bJl+OyzzwAUBx9t27ZF3bp1AQATJ07E1KlT0adPHwBASEgIzp07hzlz5lhHMPLzzz/js88+w7vvvovbt28jMDAQb731lrLaCABSU1ORnJys/D8kJARbtmzBmDFjMGPGDNSoUQPz589nt14iIgfg6umK6JxoWfYrlfPnzyM8PFytNqN9+/bIycnBjRs30KxZM3Tu3BlhYWGIjIxEly5d8Morr6BSpUqoXLkyhgwZgsjISDz//POIiIhAv379tAYbgiBIOoR+WFhYmTlj+vfvjwULFuCzzz6DIAhYvnw5PvzwQwDAv//+iytXrmDYsGEYMWKEcp2CggJ4e3tLlq/SRAUjFStWxPTp0zF9+nStaTSNrtqpUyfEx7OxHRGRo1EoFJI9LrFWzs7OiI2NxcGDB/H333/j559/xieffILDhw8jJCQECxcuxKhRo7Bt2zasXLkSn376KWJjY/H000+X2Vb9+vWRmZmJ1NRUnbUjTk5OZdrF5Ofnl0lXvnz5MsuioqIwYcIEnDhxAo8ePUJKSgpeffVVAEBOTg4AYN68eWXaljg7O+v/MIzEMXuJiIh0CA0NRVxcnNrF/8CBA6hYsSJq1KgBoDjoat++Pb788kvEx8fDzc0N69atU6Zv0aIFoqOjcfDgQTRp0gTLli3TuK9XXnkFbm5u+O677zS+n5GRAQCoVq0a0tLS1PKUkJBgUHlq1KiBjh07YunSpVi6dCmef/55+Pr6AigeeiMwMBBXr15F3bp11V4hISEGbd8YnCiPiIgIQGZmZpkLepUqVfDuu+9i+vTpeP/99/Hee+8hMTEREydOxIcffggnJyccPnwYO3fuRJcuXeDr64vDhw/jzp07CA0NRVJSEubOnYuXXnoJgYGBSExMxKVLlzBo0CCNeQgKCsK0adPw3nvvISsrC4MGDUJwcDBu3LiBxYsXo0KFCpg6dSo6deqEO3fu4LvvvsMrr7yCbdu24a+//jK492n//v0xceJE5OXlYdq0aWrvffnllxg1ahS8vb3RtWtX5Obm4tixY3jw4IHycY7kBBuQmZkpABAyMzPlzgqRVfgCXwhf4AvhwsYLcmeFSOnRo0fCuXPnhEePHsmdFdEGDx4sACjzGjZsmCAIgrBnzx7hqaeeEtzc3AR/f39hwoQJQn5+viAIgnDu3DkhMjJSqFatmuDu7i7Ur19f+PnnnwVBEIS0tDShV69eQkBAgODm5ibUqlVL+Pzzz4XCwkKd+YmNjRUiIyOFSpUqCR4eHkLDhg2FsWPHCrdu3VKmmTVrlhAUFCSUL19eGDRokPDNN98ItWrVUitTz549NW7/wYMHgru7u+Dp6SlkZ2eXeX/p0qVC8+bNBTc3N6FSpUpChw4dhLVr12rclq7v3dDrt0IQJO6MbQZZWVnw9vZGZmYmxxwhAvCl4ksAwGsbX0ODFxvInBuiYo8fP0ZSUhJCQkKMnkqebI+u793Q6zfbjBAREZGsGIwQERGRrBiMEBERkawYjBAREZGsGIwQ2TApR2okkooN9IsgCUnxfTMYISIiSbi6Fg/B/vDhQ5lzQpZU8n2XfP/G4KBnREQkCWdnZ/j4+OD27dsAAE9PT9be2TFBEPDw4UPcvn0bPj4+Jg0Xz2CEiIgk4+/vDwDKgITsn4+Pj/J7NxaDESIikoxCoUBAQAB8fX01TtxG9sXV1VWSCfQYjBARkeScnZ3NOssr2Rc2YCUiIiJZMRghIiIiWTEYISIiIlkxGCEiIiJZMRghIiIiWTEYISIiIlkxGCEiIiJZMRghsmUcaZuI7ACDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIhumUHByGiKyfQxGiGyYIAhyZ4GIyGQMRoiIiEhWDEaIiIhIVqKCkeDgYCgUijKvkSNHaky/aNGiMmk9PDwkyTgRERHZBxcxiY8ePYrCwkLl/2fOnMHzzz+Pvn37al3Hy8sLiYmJyv/Z4I6IiIhUiQpGqlWrpvb/t99+izp16qBjx45a11EoFPD39zcud0RERGT3jG4zkpeXhyVLlmDo0KE6aztycnJQq1YtBAUFoWfPnjh79qzebefm5iIrK0vtRURERPbJ6GBk/fr1yMjIwJAhQ7SmadCgARYsWIANGzZgyZIlKCoqQrt27XDjxg2d246JiYG3t7fyFRQUZGw2iYiIyMopBCMHKoiMjISbmxs2bdpk8Dr5+fkIDQ1FVFQUJk2apDVdbm4ucnNzlf9nZWUhKCgImZmZ8PLyMia7RHblS8WXAICozVGo372+zLkhItIsKysL3t7eeq/fotqMlLh+/Tp27NiBtWvXilrP1dUVLVq0wOXLl3Wmc3d3h7u7uzFZIyIiIhtj1GOahQsXwtfXF927dxe1XmFhIU6fPo2AgABjdktERER2SHQwUlRUhIULF2Lw4MFwcVGvWBk0aBCio6OV/3/11Vf4+++/cfXqVZw4cQIDBgzA9evXMXz4cNNzTkRERHZB9GOaHTt2IDk5GUOHDi3zXnJyMpycnsQ3Dx48wIgRI5CWloZKlSqhVatWOHjwIBo1amRarokIAMftISL7YHQDVksytAEMkaMoacD6+pbXUe+FejLnhohIM0Ov35ybhoiIiGTFYISIiIhkxWCEiIiIZMVghIiIiGTFYISIiIhkxWCEiIiIZMVghIiIiGTFYISIiIhkxWCEiIiIZMVghIiIiGTFYITIlnFqGiKyAwxGiIiISFYMRoiIiEhWDEaIiIhIVgxGiIiISFYMRoiIiEhWDEaIiIhIVgxGiIiISFYMRoiIiEhWDEaIiIjswP3L93F29VkIgiB3VkRjMEJERGQHfq73M9b0W4PEDYlyZ0U0BiNERER2JCUuRe4siMZghIiIiGTFYITIhikUnCmPiGwfgxEiIjsiCAJy0nPkzgaRKAxGiGyYLbaaJ/P6e+zfmOo/FQmLEuTOCpHBGIwQEdmRQz8eAgD8/dHfMueEyHAMRoiIiEhWDEaIiIhIVgxGiIiISFYMRoiIiEhWDEaIiIhIVgxGiIiISFYMRoiIiEhWDEaIiIhIVgxGiGwY56YhInvAYISIiIhkJSoYCQ4OhkKhKPMaOXKk1nVWr16Nhg0bwsPDA2FhYdi6davJmSYiIiLNbLHGVFQwcvToUaSmpipfsbGxAIC+fftqTH/w4EFERUVh2LBhiI+PR69evdCrVy+cOXPG9JwTERGRXRAVjFSrVg3+/v7K1+bNm1GnTh107NhRY/oZM2aga9euGDduHEJDQzFp0iS0bNkSM2fOlCTzRESkGWd0JltidJuRvLw8LFmyBEOHDtVaJRQXF4eIiAi1ZZGRkYiLi9O57dzcXGRlZam9iIiIyD4ZHYysX78eGRkZGDJkiNY0aWlp8PPzU1vm5+eHtLQ0nduOiYmBt7e38hUUFGRsNomIyEQZ1zOQGp8qdzbIjhkdjPz222/o1q0bAgMDpcwPACA6OhqZmZnKV0pKiuT7ICIiw8wInoG5Leci43qG3FkhO+VizErXr1/Hjh07sHbtWp3p/P39kZ6errYsPT0d/v7+Otdzd3eHu7u7MVkjIiIzuX3mNnxq+cidDbJDRtWMLFy4EL6+vujevbvOdOHh4di5c6fastjYWISHhxuzWyIiIrJDooORoqIiLFy4EIMHD4aLi3rFyqBBgxAdHa38/4MPPsC2bdswdepUXLhwAV988QWOHTuG9957z/ScExGRVrY41gQ5LtHByI4dO5CcnIyhQ4eWeS85ORmpqU8aObVr1w7Lli3D3Llz0axZM6xZswbr169HkyZNTMs1ERFZHnsLk5mIbjPSpUsXrf3X9+zZU2ZZ3759tQ6KRkRERNKyxTFmODcNkS1jTTwR2QEGI0RERCQrBiNERGQQW6z+J9vAYISIiIhkxWCEiMgOsRaDbAmDESIiIjtii2PMMBghIiLDsLKFzITBCBEREcnKoYORxxmPcSX2CgpyC+TOChERkcMyatZeezGl0hQAQFC7IAw9UHZ4eyIiIjI/h64ZKZFyMEXuLBARETksBiPkkArzCuXOAhER/YfBCDmc+5fv42v3r7H57c1yZ8VkttiFj4ioNIcORgKfCgQAPD3maZlzQpa0f8p+AMDxOcdlzgmRbeFAamQuDh2MBLULAgC4eDh0O14iIoNk38yWOwtkpxw6GOH062TreKdKlnRy8Um5s0B2yrGDkf/whE5E9obticiWOHQwwoOViEgE3reRmTh0MKLEA4yI7Iw5anxZi0zm4tDByMO7DwEA59eelzknREREjsuhg5FTf5wCANy/dF/mnBARETkuhw5GiIhIBD6lITNhMEJERAZhmxEbYYN9MxiMEBERkawYjJDDYZduIiLrwmCEyIYxsCIie8BghOyWUCTg9LLTeHD1gdxZIbIPbDJiG2zwe2IwQnYrYVEC1vZfi5/q/CR3VojsAhuwkrkwGCG7dX3vdbmzQEREBmAwQkREhmHFCJkJgxEiIiKSFYMRIiJ7ZIZaDLYZIXNhMEJERIZhLEJmwmCEHA7v7oiIrAuDESIiIpIVgxFyOBy1lBwCf+ZkQxiMkN3i4xgiafGYInNx6GDEr6mf3FkgEk0oUrkg8O6XiOyA6GDk5s2bGDBgAKpUqYJy5cohLCwMx44d05p+z549UCgUZV5paWkmZVwKnb7qBAAIaBkgb0aIRDi5+KTcWSAia2aDNykuYhI/ePAA7du3x3PPPYe//voL1apVw6VLl1CpUiW96yYmJsLLy0v5v6+vr/jcSkzhVPyNObk4dAUR2Zhzq8/JnQUiIkmJCkamTJmCoKAgLFy4ULksJCTEoHV9fX3h4+MjKnPmVtKQkc9BiexXYV4hHmc+Rvlq5eXOiu3jqZLMRFSVwMaNG9G6dWv07dsXvr6+aNGiBebNm2fQus2bN0dAQACef/55HDhwQGfa3NxcZGVlqb2IiIwxK2wWfvD9AQ+SHsidFZvHGzcyF1HByNWrVzFr1izUq1cP27dvxzvvvINRo0bh999/17pOQEAAZs+ejT///BN//vkngoKC0KlTJ5w4cULrOjExMfD29la+goKCxGTTcCXP1Xh82SV24SUAuHfxHgAgcWOizDkhIm1EPaYpKipC69atMXnyZABAixYtcObMGcyePRuDBw/WuE6DBg3QoEED5f/t2rXDlStXMG3aNPzxxx8a14mOjsaHH36o/D8rK8t8AQlJRhAECEUCnJzZBoeIiAwn6qoREBCARo0aqS0LDQ1FcnKyqJ22adMGly9f1vq+u7s7vLy81F5k/Vb2WokZITOQ/yhf7qwQkTmwFpnMRFQw0r59eyQmqld1Xrx4EbVq1RK104SEBAQEyN+dlg1YpZW4MRFZKVm4uuOq3FkhIs7aSzZE1GOaMWPGoF27dpg8eTL69euHI0eOYO7cuZg7d64yTXR0NG7evInFixcDAKZPn46QkBA0btwYjx8/xvz587Fr1y78/fff0paEqBStJ042JSEyDmMRMhNRwchTTz2FdevWITo6Gl999RVCQkIwffp09O/fX5kmNTVV7bFNXl4ePvroI9y8eROenp5o2rQpduzYgeeee066UhiLFyUiIiLZiQpGAKBHjx7o0aOH1vcXLVqk9v/48eMxfvx40RmzKEb7REREsmG3ByJbwxo9IrIzDh2MsAEr2TqOpUJE9sChgxFLy83ORV5OntzZsDuF+YXIScuROxtEds/V01XuLJCdcuhgpKiwCACQFm/+GYQL8wrxrde3iKkYo9wvSWNBuwWYGjAVaQnyzwRN1ou1SKZrEtVE7iyQnXLoYOTW0VsW21dO+pM79/yHdj4omIWfet06Vvw9nvzjpGV3TGTNzBB7Obk69CWDzIi/LBtz8+hNpJ1kDYBJ7KiJENs7EZE9cOxgxMZqbR9nPMb8NvMxp/kcPurRpPR1mddpImnxmCIzcexgxMgDK+/fPFnuSP+986/yb6GIZwUiIrIPDh2MGHNBv3fpHmIqxGDVy6vMkCMyRZkAUVvNl43ViJE0+EiLyHo5djCicnIy9ER19NejAIAL6y6YJU9E+rBXCBmEE+WRDXHsYISPOsgG8YJARPbGoYORvGwOQEZERCQ3hw5GqjWuJnodVpFbMVYYEBHZJIcORkL7hD75hxcyyfAxguUwODYcPysJ8NAmM3HoYETh9OTkdG3vNfkyYiieCIiIyA45dDCiKutGlmEJzXhztWHYBqwbuM58O7BzZWpkGLwRSYuVS2QmDEZKGHrhMtMF7nHmYyQsSMCpJaeQnZqtNz2rnInI4hjgk5kwGLEUPQexajdjW+9yzECJiIjEcOhgxKiLJq+zJDMGe0TS2Dd5H2InxMqdDQLgIncGiCRjYIWSLV7MBUGwyXyTfbGnnnJCkYBdn+wCALR+uzUqhVSSOUeOzaFrRohsQcHjAswKm4WNwzfKnRUiu6EaWBU8LpAxJwQ4ejCicqNpaMTPu1PrZU93baoubr6IO2fvIP63eLmzQkQ2wBavU44djFiSxL8Na77wWk3ebO94JLJuVnJoS8KeymIHGIyIZa4LnI0fGNvGbJM7CzZ5N2AI1cH5iIjsEYMRsSwQNBhyUbW2C+/h6YfNuv0NwzZg01ubzLoPa6Vwtq7vmsgu8LCyKg4djKhd0G28ZsKe5aTlIGFBAk7MPYHcrFyt6ex1BFbWjJC1sJpHsFKwo6LYA4cORozC64LFFRUWKf+W4mRoaydUJ2cdhyl/j0Qms7aaZkfEYITIyrFmhMi8bO0GxR45djCico53dnOWLx8OIic9B6eWnEJBLvv0i1EmGGFsQkR2xrGDERX5D/MNSsfqPOPNbzMf6wauw56Je0ze1oOkB9jx8Q5k39I/qaCtYwNWIump1obwvC4/BiP/OTHvhNxZ0MvWqxIzkzMBAIkbEk3e1qKOi3BgygGsenmVwesIRQKWdV+GE3Ot/7tWpXqitPXfABGRJg4djKjNjmvuwFjMNcTWg3QNZc1/ZFjNk6GyUrIAADcO3TB4ndT4VFzaeknSfFiErf8eyH4wFrYKdxPvYnHnxbi255rcWZGMYwcjhU+OLA8fD4vtV2+VoB0e8D/4/SB3FlBUUKQ/kS2ww98HkcXZ8HG0qs8qJO1Kwu/P/a7xfVusQXXoYES1y2izQc1kzIn9y8vOM/9ObO/4M4y9lsvSWMNEdsIe28o5dDCiOn6Dq6erYStJcEKzxahVSpYqv6N/zkREtsKhgxHPqp7Kv13LGxiMkPVyhDtfO4yvCnILrKq7d0FuAeY9NQ9/ffCX3FmxOvYU4NtiWU7MP2FX7URUOXQwAgB+Tf2K/7C936XNupd4D4X5hXJnw3bYcZB1be81fOPxDb7x+MZq2vRcWH8Bt47dwpGfjsidFZPY4sWWtLtx6AY2jdiktZ2IrRMdjNy8eRMDBgxAlSpVUK5cOYSFheHYsWM619mzZw9atmwJd3d31K1bF4sWLTI2v5IrGVDK0APXWvqj2/qJ5uLmi9JvtNRHUua7su2P7Anr+AlK4vdOT06s/975V8acPGEtQRGRqgdJDwxOay3XKTFEBSMPHjxA+/bt4erqir/++gvnzp3D1KlTUalSJa3rJCUloXv37njuueeQkJCA0aNHY/jw4di+fbvJmZfEf9+ZWjdfMo0Bx0FhHmtGpGCLJx3Z8BA3nT19hvZUFjvgIibxlClTEBQUhIULFyqXhYSE6Fxn9uzZCAkJwdSpUwEAoaGh2L9/P6ZNm4bIyEgjsiwt5cnc3D9Mfb15BQuOeUK2hSdNIvOysXOurdeMayKqZmTjxo1o3bo1+vbtC19fX7Ro0QLz5s3TuU5cXBwiIiLUlkVGRiIuLk7rOrm5ucjKylJ7mU1JLGKHX65s+FGajT3/Tuc9NQ/3r9yXOxtG+ff2v3b93dg9fnWyExWMXL16FbNmzUK9evWwfft2vPPOOxg1ahR+/117g5q0tDT4+fmpLfPz80NWVhYePXqkcZ2YmBh4e3srX0FBQWKyKYpyEjIz/xgLHltPbwGbY6UniseZj5G4MdH8j5xU7tpmNpiJ63uvm3d/Msm+mY1NwzfJnQ3Rzq05hx/8fsCWd7bInRUimyUqGCkqKkLLli0xefJktGjRAm+++SZGjBiB2bNnS5qp6OhoZGZmKl8pKSmSbl9VyWMac7cZmddadw2So5GlrYPEu1z2wjKs6LkCOz/ZKWo9U35rD648QG5WrtHrWztbLNuuT3YBAI7POS5zTtSxPZFufDRuXUQFIwEBAWjUqJHastDQUCQnJ2tdx9/fH+np6WrL0tPT4eXlhXLlymlcx93dHV5eXmovc7l55CaA4nlLDGLkj9YWT7LmZIkq7TL7MHKXRQVFuLD+QpneHikHi4Pkk4tOGryt82vPI6ZiDC5suGBcZsh4hh67VloTB9j3YzrSzd6DS1HBSPv27ZGYqD7j6sWLF1GrVi2t64SHh2PnTvU7x9jYWISHh4vZtdnt/WKv3FkgE5nrRH3wh4NY2Xsl5reZrzmBiHPEqpdXIf9hPlb2WilJ3nhxchzbP9yO6bWm49F9zY+3LcHU31tRYRE2DN2A+AXxEuWI7IWoYGTMmDE4dOgQJk+ejMuXL2PZsmWYO3cuRo4cqUwTHR2NQYMGKf9/++23cfXqVYwfPx4XLlzAr7/+ilWrVmHMmDHSlcIO2HvUW5qo8sr80Zz/8zwAIONahsb3He27I3kcmnYIWSlZODrrqNxZMdrZVWeRsDABG4dtlDsrVl0D5ohEBSNPPfUU1q1bh+XLl6NJkyaYNGkSpk+fjv79+yvTpKamqj22CQkJwZYtWxAbG4tmzZph6tSpmD9/vlV06zWGFBcefdvQ9v6lLZeUf+c/zDc5H2ScgscFkgyMVfKIkJ5gTY99k7NWh6ybqHFGAKBHjx7o0aOH1vc1ja7aqVMnxMezWk4XfSfhB1cf4O+P/lb+v6DdArx79l1zZ4tKKcwrxI81foRbebcnC42MT+e3nY83j7+JgJYB0mSOyNwYK5KZOPzcNLYi43qG2v93zt2RJyMO7kHSAzy69wiZyZnKZabUll3fZ5/ddB0Ja3NsE78368JgRAb6DgJbP0isNf9S5MvFQ3RlItkwPg4lq2HnTdMYjBCZKCctx+h12fi1FGuIY1W+koNTD2pMknE9A/98849Vt4Ewx02Bqdvk790E1nBsmBFv88SS+FgqzCvEqSWn4N/cX9oNk0Oc+ByhjBanctLPvpWtMclv4b8hJzUHqccMHJ9ISlZ4UVrZZyUe3nmIIXuHPBnVmkgE1ozIbP+U/dg4bCPmtpord1Ys67/z1c5PduKnOj/h4d2Hpm/TCk/Stiz7Vja2f7gd9y7dk3S7jx48wup+q3Fx88Wyb9rIdSwntbg2LGlXksw5kZ8gCLiw7gKS9yfjznkbasumOgArg3rZMRiRWdKOsiczQw8MKbqXym3/5P14cPUBDv6guTrcFKWrlHnCEWfVK6twaNohLGi/QNLt7vp0F86tPoflLy6XdLv68PuXgJ0G/Nbazs1oNvhTZzAiklwnNE37nVx+MvZ8ucfymZHAmn5rcPPok3E2JAmsbPAAtOY834i7AQB4eEeCWisVObeMb2OjcXtpOci6oX9mb7u74JDjssOfMoMRkazphFaYV2jTw9j/GfWn8m9LPGe2pu/Ontw5f0fzIxdjiPyKhCIBUwOmYlrQNOT9mydNHsh8rCj4Vj0f2EStmQ1k0RQMRmTm0BdI1aJLcaBZ4KOU46Rl7SfKXxv9iuUvLkdKnGGza0v5my/ML1T+ra2xKZHUCvMKkbw/We33R6ZhMEKyMfjORO54zYyxgCGBhq0ErGkJaXJngeRmGz9Vk219bysWPrsQf73/l9xZsRsMRkSy9rtUm2XNH6uDnGCpmLUHf9aeP5thwsd4Yt4JAMDxOcclygwxGJEBA5qySj6TR/cfYWXvlbiw4YLMOSKrDhCJSDsbjFcZjJBVKLnb2xG9AxfWX8DKXitlzpEKXpQNYnCQreNEybt+68bvh8yFwYg1EnnxO7PyjHnyYSwBeJzx2KhV/037V+LMSIsnYysm11dj4f3qCvpUG1RarAZWpfwpcSmI+zEOQpH1Hye2diyrfp+2lndDOHwwEjElAgBQtWFVmXNivD9f+1N/IgtaN3AdplSagpSDhvWuAMx04jTxeM3NysXmdzbj1tFbymXJ+5NNzFQpBhSbj/W042fzxI1DN/C129ey5mHzm5vx90d/4+yqs7LmQy7n/jyHqzuvyp0Nm+TwwUi5yuUAAHcv3DVo0qv8R09m8czNzjU9A/YX4CpnOj0w5YDMOTFexrUMbH1vK47PVm+gVvC4wOJ50XkXZIPXYl3lMWdwYcy2FQoFLmy4gE1vbUJBruW/ezG2fbBN7X+z3D0buMm7iXel3zeAosIiJO9PVjsP6yIUCbh95rZFamoykzOx+pXV+CPiD7Pvyx45fDBy49AN5d9b3tmiN73qlOLGTi9uyEnCHoZ6N8a1vdfKLjS0KYJEJ9/s1GzMCJmBU3+ckmR79uD0stNyZwG7J+7GuoHrzF5FXbo6fGWvlTgx9wSO/nrUrPs1t5SDKVg3aJ1Js0ybytRgc9/kfVj47EKs6rPKoPS7PtuFWWGz8Nco83fBzUk37+dqc4O0ieTwwYjqhe7sqrNY238tzq46q300R6nPgxp+U7dP38Y3nt9g12e7TNp0UWERHmeWbbshCAIOTT+EK39fMWn75pCbKUFtUwkjj9fUEzpmYpX4+7fmk4pq3ja8scEi+9QVaPzz1T84teSU2mMzS+bBWgZVMzYYW9B+AU79cQqb39oscY4s5+jM4oDw8rbLBqXfP3l/8Xq/aAgkpR50UY/U+FT8/r/fcfPITf2JHZDDByOlLwanl53GmlfXGFRLorpuUWERVvZZiX0x+0zOU+y4WBTlF2Hf18XbyrqZhbwc8UNdz287H1N8piDjeoba8ut7r2P7mO1YErnE5LyaxAofUaUcTMHyHtoncLPHhmPaiC2rpgD+1JJTuLzdsAuHoVQfl6jl0XrjOqty//J9o9c1+Ddha4eJBfK7qOMiXNt9DfPbzjf/zmyQwwcj2qau11pFr+WEd3HzRVxYdwG7/k9kbYaegyDrZham1Zhm1AynqceL7/DP/3lebXnp4MRemFrLcH7declnqJWCrnIV5VvmcZ4hF6GS4LnE/Sv3sW7gOiztutQsecrNzpV8Ej+SR8b1DOyeuBv/3rbu3nSmyMs2be4ka65FlYLDByMX1okcXEvLOdnY9iMaqfzmJO+9YaSM6xk4+cdJq27LoveCqeftPZ/vMWAnBmfHIvZ9Y3pNnFRKd+c2d9uEKT5TMC1o2pMFVvbdaHJxy0UsiVyCrJv6Zxk2xLW917Bu0DqtN1Wy0Xbd1LJ8UYdF+Oerf7DmtTVmy1JpUteq2XuwYG4ucmfApqn89iT9IQpa/pZI/G/xoteZETwDAPD4wWO0HdVW6ixhf8x+NHipgeTbtQe6gqzr/1y3YE6si9geEsY8YpP6AlPyCHDLO1sQtTHK5O393ul3k7dhDTKTMwEA13ZfM3idre9vhX8zf7Qc3tJMubJeen/LNhgXOXzNiFjGnNDOrtbR517Pj+bG4Ru6Exii1D6S9xlf25K0KwlpCWlmqSH5Lfw3ybdpKWLGVFFjxScNKS/ED64+ePKPrkPIkMPLBmpA9Pk3XfPjiILcApz78xwePdA/zICqjKQMs/2WjGonpW0VCb+7ozOPYtOITdJtUEJ3zt+ROws2h8GIBazpp6PqUc/BmfyP5R/TCEUCUg6maGyQmLghEXNazMGGoZbpXSHG8dnH1e+WzXDRStyYqHG5NbY1kYO2C9fcVnMtnBNpWLrB8q5Pd2H1K6uxpIvMjcv/s+XdLfgl9BedaWy2UbcZsz2nxRzzbdxOMRgRSevdornucGW4cz42+xgWtF+Axf9brDWNtY7BkbQryazbv7TlkkHp8nLysLrvapxZYfpQ/dbyLDrrRhaW9Vims0v4D34/4Nbxsl1vDZ0eIP1UutH5MzsLXHPPLCv+vdw6pqP7sgWv/cdmHcO9xHuy7NuWFeYW6k9EahiMiCTFXYC+i4vUdxpiL2Yn5hdPj13SH76o0HobrZZmLaNkHpx6EOfWnMOfUbqH6reWQMMQG4dvxKUtl3R2CX9456GyzMaW7c4581RxSzGRn8bkEh6vCicTfg8mZuP2mdtGzylVhsgGrCZv1xHYedkZjJhAiguJphOZ2nYlOM9l3RDZcl/1SUeRgFlhs0zPhCls8G7M6no3mEoAsm8aOOiXvk5Nei7eOmsFTNiuqUwdbK0wT+VuWcupw9gymFr2W8dvYVbYLPxY/UeTtvMkQ9JshhwHgxEdxBzgkt7hKrT8baS4qXFGr/s48zHunjfPPBM2+6zZACbd4ZZiq59T6Xwb2tXXmsqrelyb2nPp9HL5h9TXpmREU31DFMj+3Ui4e9nLQmoYjOjwldNXGodTV1K53tw8atwQv9ZYTc+D1HROzgYeWtb39T8hcVBs6Pwgxk5qZo3HkiqDxiIy5dCz7uKXsenNTUg7mSZ3NqRjyc/fDk/RDh+MtJ/QXuf7hs48G/eD8bUPZdjhD00Ta794WAur+JyM6d1ZKqjISc0xbFt2+vuX7HvUspmbh0vdEGn5HOW82VD9DE7MO4E5zUv1OtH3EZnpULCKY8zBOXww4hPso/P9Mg26JD6O9Z0YZDlILHSuYg2MjTH0p/hfukUdFhm1G0tM966TapMtKX+j1ny9M7SYpdLlP8rHnbPyjqmxcfhGk8dC4blIfg4fjMhxghDzw5fjILHmqarTEsxXrSvHQEW6Pt+8f/Nw5e8r6g0fLUmm87OtXRgMPUYMSWdI2fd8vqfsoGgiPjKpjumkXUmY7DkZs5vNlmR7xor/LV73TNv2yLpOy5Jw+GDE2i62lnb77O2yC634WpCblWt44lJfrb4TvVBoXQVf9fIqLIlcgp3ROy2639jxsdg32Yxz3ug55PTVjGj7HiULYszVtbdUua/EXsH2j7ajMF8l2DRwU4emHRK9P+UuJPqcFnfWPg6RpRU8to4u/dZC9tpFIzj83DSm9HqwRCBj7n1c2noJvo191ZaZe4Izi5HxeJTihH9le/HgYpa863tw9QEOfn8QAKBw1v7bO/zTYTR/ozncK7obvvGSzdlQmxFzHX8KhUI5yqpPsA/avl8835Ohvxu1AEYCFquN0vBxPrwnTzd4a64BNtWBbw+gasOqaD64udxZMZjD14zU7VpXdwKRd9dSkLuaWrIxMsxxfFvRhcoe5T8ybPbpbR9sw7YPtonbuIHfndy/f3PRdsHLuJZhFfmQy/dVv5c7C3ZpwxDrm7JDF4cPRlw89FQOGXBezMspO4eLVIwdAMoqSHVNMVeDQitRkFuAuxfMM5aLaFo+Xk0TI5bU3FgqDyaT+xpsyP6NLbuIsuk7hsz6yMNc3639nRYcjsMHI2JPUIWPn1SP7vy/nUhLSEPyAWkns7O2OxdbVJhfWOaO0yo/V0Vxr5NfQn/ROgmftZAq6NbbdsdcPcyM6XChIS/GDpluSL7/va15Nl+925LwYnzge8OGM7B5NhbAXNtz7ck/RuQ9fmE8bhySYBZ4MxEVjHzxxRdQKBRqr4YNG2pNv2jRojLpPTw8TM60pER+qaeWPJkg7vic45jTYo7ok2P+w3zMbj4b2z/arnH/tnL3/zjzMQ7/fBjZqQYOE25BizouQsrBFLmzYZCSOYDif4uXOSellPoZlm7ga67fqbENWC0lcZORQaPKaULOi8Ld83d1TnZ478I9re+VfPZZN0VOMeEAzH2zc+zXY0ave23PNWwcuhG/hf8mYY6kJboBa+PGjbFjx44nG3DRvQkvLy8kJj45eK3y7tRA2rpYim0Ee2b5GaSfTEf6yXQEtQuSImt6nfjtBHLSctDhkw5a05xaegoV/CuoL9RRtM1vbcbZlWdx9JejEuXSAAZeh27ElT3Zy30Rs3WGVt9rO8aT9xtYg2hrX5OhnWm0nftkKO+SyCWYKEw0ev11A9bpfF/rsabnVGnL1wdrdjfRSh4D6yA6GHFxcYG/v7/B6RUKhaj0libmAjXvqXma3xB5/Gwfs93odY21afgmAEBo71CN7985f0fvCaa0i5suAoD6FONiSFRtbsvUTr5G/hZS4oprgILCTQ9s1T7fUvkx9BGCLpranpTJgwHdEo35HaQe194raf+3+3Fm+RkM3jNYbbmpF8ec9BzcPn0bIZ1DTNqO2YkcAOz2GQ1DAtgwezuv2CLRbUYuXbqEwMBA1K5dG/3790dysu67nZycHNSqVQtBQUHo2bMnzp49q3cfubm5yMrKUnvJpaQKHQDST6VrTCN1NG/OBmTa5toRPbOvXGzwnGHux1gL2i3AgnYLDO4JIxkjvouiwiKTZ/U1VsKiBK0NhXdG70T6qXTE/Rgn6f5nBM/AH8//gfNrz0u2TUu5c67sIIB52Xl2c+G2l3KIZa2P2EQFI23btsWiRYuwbds2zJo1C0lJSXj22WeRna35ZNugQQMsWLAAGzZswJIlS1BUVIR27drhxg3dz0tjYmLg7e2tfAUFWeZRhiapx1P1/2hNiUU0bNromgYjpcSlqNfWkOlUvldLjVBp0ERsYsg1AqshAzYZmTfVmwtNpB7ttuTG4vK2y0afJxI3JWLVy6t0ptF0jsrNFDFAoAZnVp4pm5cNifi+2vd6u/+b43FLdmo2/k03vXbOIan8PKbVmGaVExSKCka6deuGvn37omnTpoiMjMTWrVuRkZGBVas0Hyjh4eEYNGgQmjdvjo4dO2Lt2rWoVq0a5syZozF9iejoaGRmZipfKSnma4hoyEFzcfNF3dswYeA0nbMCW8iCdgtkn19CJxu/gXl4x3KDOl2JvYI1r63Bv3fs56QtCAIykzOV/68bsM7ou1p9651ZdkZtX5IRjL9Ar3hpRdmaFUs83tXwUWVcy8Cje4/KvlF6VS2fsylByq5Pd4neH2l2ZkXZQFNuJo3A6uPjg/r16+Py5csGpXd1dUWLFi30pnd3d4e7u4iRHU1hwLFxbfc1NHixgdb3TRl61+JBgMismnMMFWOsenkVei3uJXo9Q9oqyELKi4oA5aiezm7O6L24t1HbMCtB/IVjf8x+7PrkyYUo60aW0aPS6jtWs25kYc/ne56kN+EiV6aWxYrbZtrCxbzgocSPrwUtf9sAW/i+xDJpnJGcnBxcuXIFAQEBBqUvLCzE6dOnDU5vCVJUJ9pz1WH2LRPaO+j5aI0ddn79oPWi15nVdJZR+5KSvoBIyqrtrJSyz4VLn8C2jd6GP7r8UdyOQ0saa6AaiJT4I+IP4zYmQfEM+Z7ST6fja/evVVYybD29gzBqUZir+fFSUWEREhYl4N4l8Y9+rfG3YC5XYo0fwK/MpIVkFFHByNixY7F3715cu3YNBw8eRO/eveHs7IyoqCgAwKBBgxAdHa1M/9VXX+Hvv//G1atXceLECQwYMADXr1/H8OHDpS2FCdwquJm8DXdvC9XiyMCkC6Sec5klZ6O9e/6urHc/1/ZcwyS3SRbbX+kLyYpeK7DwmYVqNQOHZxzG1dirysGU4hfGY07zJ49QrXmyLY0TJgrA3Qt3sfervcjN1txeQuwFVtPvX3Ub2ra3+9Pdav+nn0yXtOFg6XxpqymKXxCPDW9swMz6M7Vu6+Hdh9j7xd6yb1jv16/T/Sv3sS9mn6hH4H+99xcA4PL2y/i5/s+ixij6rvJ32D9lv9E1X8fnHcfmtzeb5XjLy8lD/sP8sr9TK/xuRYXhN27cQFRUFO7du4dq1arhmWeewaFDh1CtWjUAQHJyMpycnsQ3Dx48wIgRI5CWloZKlSqhVatWOHjwIBo1aiRtKUzg7OZs8jY8q3pKkBPppManomJgRYPS7hi/Q38iIwlFAm6fvY1qodXMtg9bsW7gOllPAIkbisf6uXnkJmo8XUPtvZIam41DN0qzM1MreEz4nH4J/QVAcY+BF+e8KOm2TXHr6C3cOird1A6GBlUp+/VfVNcPXm9ibiSk67ej6z2Vj2NOiznIy87TOXgboPkzXNp1KQBgQfsF+Lzwc4PbA+78eCfqdKljUNrSNr+5GQBQ/8X6qN+9vkHrGHqTGFMxBgDQbWY3o/JmSaKCkRUrVuh8f8+ePWr/T5s2DdOmTROdKZtjJVFm6olUrOy9UtoGeMaeHFDc8Pfi5osIHxsuXX5McGCKlQ9zLWGbgut7r+Pf2/+ivG95teW/hf+GCRkT4OH9ZCRkYxvYarsg3ku8hxPzTxi1TamkHNB8EZb60YO2i4LY/TzOfCxbI/Jre6+pL/gv62Z5TGOBdjN52cXt3M6vM607dVFhEZydyt6s6uvQYKxd/7cLuVm5CIsK05tWbFs+yXvamQHnpjGEyANIEASLPoIAgL1f7cVv4b9J3hNAinYMcT/ESZAT05nrJGIIc/QuKK30nCkzG2iumi89FPnuz3drTGeKTSM2Gb2u0RdB1QkVC7Vsw0JtRsSa23IuFrRfYPIot6VZXbsPA7Jz++xtbHl3C7JuZiEnPQenlp5CQa74xqslQYmU0k+nY/mLyyXfLlA8jtXa19eaZdvWcsOsi0m9aUizr5y+AgCMTR8LV09Xi+xzz8Q9FtmPvdLYBsEEUlbJG6owXz0ANnRCN3OctOVmzlmQDbrAizj5C4KAB1cfGJ8hOzOn+RwUFRTh9pnbyErJQsa1DNw+c1v33b0x8aG+70jD+6eXnjZiR2XdTbyLCn4V4OFjZXO1yYjBiBntnbQXR2dacN4WM8hJN67Hi61Z0VP3I0gxMlMyyzSAM7RxWsb1DCTvS0aT15qI37GRdz/G3j0b3YvMTHdpq/roHhgMgFV3r7U6ZvieTszT//iupA1TWnya8nHE4RmHUfBIR+2Ihe78b5+WZhj8Xxr+AhcPF3zy6BNJtqeP1dWQacBgxACHfjyEtBPaR6zT9kWf+uOUxuVyEvujXPbCMjPlxDy0Vs/roTY9t5EyrmXg0tZL8K7pXea9nFQtQV2pi+NPtX+CUCRI1vNiftv5GH5YvfeanJORialVuHHohqjPQdPw5VLLuJZRpmu5xjIx6NFI3wi4alQ+Q52BiIUIRQJuHhWRfz3MOe2HLWIwYiCjLlZWGIw+fiD/iK/mtKLnCnSd0VWWfc8Km4W8nDz4tzBuYkiFQqGsQbmyzfhxD1TdPHJTbwBgrTOlmnu689PLTuPG4RvoOs3w38uGoRsMSyji2NfX60MjA78yg26ItDWvsYG7aXMqXf5LWy8Z3dj78nbDBgYFimujM65loEbbGvoTG6p0z14r/G4ZjEjBhg5mczW+Moe0k2nwb+aP22fFVY1u+2CbmXKkW0mVclq8cfM+yNXi/eHdh7h/+b7F9qfvuJDiuKngX0Hn+wWPC7C2f3FjweCOwVrTXd6mfhG5tvuaqVnTuw+rIfPpy9xBst7fWam39U2xUDq/59edh7ObMyrXrazsMmyIqf5TAQDD4oaV6YZvzxiMmJP1xSKSyv/XvBfPOc3nYKIwUdSBbMvULkrGnIdNOHdvfW+r8SuLYOlamNIjayoUCtw6dgvznpqnXJYar31o+YykDKP2a/YbEQHY9Zn2uVoMsXbAWnhWKztG0pkVZxDYJtCkbVv63GeOz9vUgcJK2jD1+7OfUftP2pXkUMEIu/YC6PRlJ7mzQGQ5GuKBonzrmbtHqoAl/2G+cq4eVbs/U+/KvO/rfUbvIy8nz+iAxRTXdl8zKd9Acc+Qw9MPa7yQ//3h32rTBFi7qzuuGtX9V5Wp62tj7Miq1lizbk4MRlA88p05ONqPicTTduG11nYcpjLkmJDiuMlJy0H+I/M/9pJryHxj53USxZSiGfvzNXKf+77eh60jDa/du7T1Upkeb7ObzTYpL1ofs1rjoWyFlyYGI4DBQ/5qc3zecc1vWOEXTuosclI3hjnGTbAChXmFyL5pwuSLIvelCW8SrJcpwV38b/EGpbv01yUs674Ma/qtUVt+L1G9IfHW98U9ulzQfoHG5UbfWBj4UVzYcEH0IJv5j/Jx/4rl2okZgsEITA9GEhYkaNmwSZslC5gaMNXo6eilcHLxSY3LTf1N6nJmxRkkLEow2/Z1mffUPP1dcCWKFey1dgmApOcWbQGASV1PpfgOzfT1GTpcQcl5/fDPh7HpzU1GB7G6juVbx7QPjigIAgpyC7C482KsekX7GDore63Erk93If10us5tqTo68yh+rvszUuIMnxDQ3NiAFWY88fMGzKq4lnfV2Og24fcEy2fmP+bonVFGqd9hwoKEsgG0ha7b9y9Z8G5MU5ksFZ9IcOwLRYJZg9InO9K8+Ogv8g7YaEwweW7NOcnzsW1Uce88oxvs6yiGakNqTU4vPY2kXUl6d3Hw+4M4+P1BsTnD2VVnERQeJHo9c2DNCMwXjLA62LqoTg5n7Yy6q7fjigBbsuvTXbi09ZLJ25nbaq4EuZGXUCRg6/tbcWqpZQaAXN13tdm2LfWUEXoJtjHBnVQYjMB81bnWMGqgrbu4RbrJ7Rgc6mBFH41k35NMZdr3jWm9XEqkJegYr0bCspmlEe5/p9Tza8/j6MyjWDdgnfT7INNZ0XHPYATgHaUVW95DwkHarOjA08uI3+Sj+4+kz4ed2TR8k1l/B6ZOWy8HswTp/21SdaCwf775R9w2JD4vC4KAe5eMGO1WZX1LknR/NnDuYzACO2/oRkpa7wCt8EA15jcppmtj2R0av6rkpPo+tJTpyt/SDLWviUGT9Vkbc/7+Vba9+9Pd2tNZwNFfj2Jm/Zmi1tn89uYn/xj5OZm7N429YDBCDkNbMJKXnWfhnOhnTE+G9JOaW9ObcywMsV0KSRpS3jVb6+NLqW8S/3rvL9HrHJ+jZdgGC3CE71gVgxHAuu4KyWy0XZTl6uaqixSzCJc4s+KM3jTaAhl94hcaNrYDWTEzXqdMuQg6/GNH648fJMVgBHxM4ygc9eSma+6VEg/vGjcbqbHrkWls4ZyVm53rcBdUMh7HGQFQIUD3DJ9ENs1CbQKsjaEjctoia692v3nkJr71+lbubEjK6NmVjW0yIghW/z1LiTUjANzKu8mdBSKzsbUTWnaqNMPF7/rEtFltrZlcc+IY6sp28zUSloulP/M7Z+8oB1wzl9Tjqci4nmHWfRiKwQiRnTs8/bDZtm2OuX2OzjyKB0kPJN+uPcm8nil3FsjMLqy7IN3GtMRRyfuTMSN4Bh49kP8RNoMRIjKauYYMN7pKnMjaWHclFgDgRtwNubPAYISIrI81dreWw+KIxXJngUxk0vg/EtH3qNYaHuUyGCEiq7Njwg65s2AVknbqnySNrFvWjSy5s4CLG6WbVsNcGIwQERHZsVvHbul8XyhkzQgRERHJaEXPFXJngcEIERGRo5O7uziDESIiIgdXmC/vPFMMRv7zzP89I3cWiIiI5CFzsxEGI//x8PGQOwtEREQOicHIf1zLucqdBSIiIlnIPdYIg5H/tBjaQu4sEBERyWL94PWy7p/ByH9cPVkzQkREjunc6nOy7p/BCBEREcmKwQgRERHJisEIERERyUpUMPLFF19AoVCovRo2bKhzndWrV6Nhw4bw8PBAWFgYtm6VfwZDIiIish6ia0YaN26M1NRU5Wv//v1a0x48eBBRUVEYNmwY4uPj0atXL/Tq1QtnzpwxKdNERERkP0QHIy4uLvD391e+qlatqjXtjBkz0LVrV4wbNw6hoaGYNGkSWrZsiZkzZ5qUaSIiIrIfooORS5cuITAwELVr10b//v2RnJysNW1cXBwiIiLUlkVGRiIuLk58TomIiMguiQpG2rZti0WLFmHbtm2YNWsWkpKS8OyzzyI7O1tj+rS0NPj5+akt8/PzQ1pams795ObmIisrS+1laQGtAiy+TyIiIkfkIiZxt27dlH83bdoUbdu2Ra1atbBq1SoMGzZMskzFxMTgyy+/lGx7REREZL1M6trr4+OD+vXr4/Llyxrf9/f3R3p6utqy9PR0+Pv769xudHQ0MjMzla+UlBRTsmkUJxf2eiYiIrIEk664OTk5uHLlCgICND/SCA8Px86dO9WWxcbGIjw8XOd23d3d4eXlpfayhJZvtlT+XSO8Rpn3W73dCt1mdiuznGxDpTqV5M4CERFpICoYGTt2LPbu3Ytr167h4MGD6N27N5ydnREVFQUAGDRoEKKjo5XpP/jgA2zbtg1Tp07FhQsX8MUXX+DYsWN47733pC2FRP436X/qfyvU3+8xqwfajGxj4VyRVIbsHYKuM7rKnQ0iIquUddPy7TNLiApGbty4gaioKDRo0AD9+vVDlSpVcOjQIVSrVg0AkJycjNTUVGX6du3aYdmyZZg7dy6aNWuGNWvWYP369WjSpIm0pZBIed/ymChMxERhItwquGHE0RGo2rC463KNp8vWlBjDxUNUMx2SkFd1L7Qd1VbubBARWaXcrFzZ9i3qyrhixQqd7+/Zs6fMsr59+6Jv376iMmUtAlsFYuT5kXj04BHcvdwttt/nvn4Oedl5ODDlgMX2SUREjs3FXb6bZbbSNEC5SuXg5Pzko3pl1StGb+t/k/+nN02zQc0Q8W2E3nRknNA+oXJngYjI6ji7O8u2bwYjRmjct7HR6z49+mm9abyDvAEAwZ2Cjd4PaffyipflzgIREalgMGKkxq8aF5AoFE9axTaJaoKQziFoP6G9xrR9lvYxah+km7OrfNE/ERGVxWDESH7N/PQn0iOkcwgG7RiE5756DmGvh8G7pjf6rnnSvqZiYEWT90FEROY1+vpoubMgDUG+XbNrh5FUazhM3Yazm7PWWpARx0ZgXut5Ju/L0lw8XFDwuEDubBARmV2FgApyZ8HmsWbESFXqVzF5G55VPfWmCWwVaPJ+5CBnQygiIouSsUZBSoIgX0EYjBipYe+GeP6H541at9fvvdDm/Tao36O+SXnwrOqJ4UeGK/+3piHspQjWiIjIMVjP1cvGKBQKtPuonVHrNhvUDN1+6gaFk3GPejp81gEDdwzEuDvjUP2p6kZtw9x6L+4tdxbUBD8XLHcWiIism4w1PAxGbFDHiR1Ru3Nt5f9dpnZBed/yiJweKel+Xllp3Hgqn+Z9Cq8gy8wnZKhX170q+TZrdagl+TaJiBwRgxEb88b+N9QGYAOA8A/D8VHaR6gbWbdMetXJ/8R4ecXLaNzPuO7Lzq7OkjTwFaP9x5q7R5fw8PaQdH/DjwxHQGvNE0QSEdkithmxYd1nd4d3TW+L7a9m+5oalysUClSuWxmvb3ldbXmP2T0M2q7qY4yJwkQ0edW4+YMG7RwEAHD1dEXD3g0t1sr8uS+fE5V+WNwwBLUPwvAjw/F/D/9P9P4CWwXaTaM1IiK5MRgxUeu3WpfpY/7+pffR5DV5JgOs90I9VK5XWfm/phqK4E7B8A3zVVvWf2t/dJnaBQP+HmDS/kP+F6L8+9W1r+KjWx8h4jtph7av/XztMsuc3ZwxIWOC2rLAp7T3RKrxdA0M3T8U1Z+qrjYfQ4+5hgVvUMh7F2EOr214Te4sENkkuzkXsM2I/VA4F9dQ9FnaB2NujMGA7aZd3IHiifMAoNvP3UzeFgDUfLYmGvZqqLbMxcMF4R+Go87zdfSu32J4C7W/9eWr/Tjdj1DEqhpaVe3/Dp93AFD2UYxfUz+0Hd0WEVMMD4aC2gVpfa/rjK7KvxUKBYQizUduBf8KeP77sj2t3jz+JkYcG4EPkj7AB9c+MDhPqjp+0dGo9Ur4NvHV+l553/ImbZuIbFthXqFs+2YwIrGSmgiFkwJe1b1Qp0sdhL5s2sRsHT7pgLG3x6LNe20MSl8ppJLa/yGdQ7SkLDb04FBR+Xlp3kvKvz28PfDUyKfQY04PvHniTVHbMVqpGED1EU3NZ9UfY3Wd1hXtxxseDOlq6+JVQ71RrrZgxLeJL9qNbYfPCz/H+5ffVy4PaBmAwFaB8An2gU8tH4PzpKphz4Yal5erXE7vugpnBZ4a+ZTW990quundRo3wGnrTOLImUU9qRN29DZ/pu/mQ5qL3pTpasxyMbVMmNUPOrxOFiQgfG26B3Ni2O+fvyLZvBiMW8MIvL6DZ4GYYekDcRV9V+WqG37W+tOAlhL4cisF7BgPQ33gzKFx7bYAhFAoFWr3ZCgEtpGvQ6RXkhept9HdbLt1LJmpjlPidlYo/gtpr+TxKpdMWjCiTOylQuU5ljLkxBuPvjxefr1J8Qny0vjdk7xC96zs5O+nMs29j7bUmJfpv7a83jTm9Ff+WrPvXS+XjHbDN8FrRGu3EB3n1Xqincbmlari6/NjFIvsBgFZvtdL6XrPBzQzaRpfvzZdfUxrsqz7allvJJK1yYDAiNQ2/yQp+FdBrUS+djwCk5FXdC/3W9ENwx2CtedJVXS+GuZ6V+jfzx/DDw1G3W9keQqr7rNVRvXuth4+JvWYUxQFOl6llT1ylt60vGCnhVd0L5SqVrbno+lNXDam1y0jK0Lg8qF0Q3Cror9Vo8FIDo6phFc5PfkAePh4mPyoyhUclaXtFmULfKMM1njZvLZJrOVeNy8emj5Vk+24V3DD+nuYg+uOsj+FVvWz3fU2PJ6WgrSF+s8HNNA4eGdDqyY2Rk+uTy1zpmlNrMGjnIIy9Lc13Zir2prEDJXetdbrob3NhaW1HtVVfoAAa9W2Ebj93UxvBVZN3zryDyGnSjl9iiHo9iu/6Xt/yOqKzo9FzYU/le4GtnzRM1XVHIuYOMbB1ICrVroQq9aqgfLXyCP9QvUp3+OHhcPVUP/m3fru1xm2pnvx0aft+W/FBoYbiPjdJf0+ibj93w4vzX1Qb/6X50ObKv7VV+T/zf8/gs7zP0GZUG3T6qhMA49sAjbszTut7vRb30rt+Bf8Kkjaw0xToivH+xffV/q8aWlXvI1FzK90WzCSK4sd/mgJz94qaH0GFvR6md7NS3pQFtQ/SeA4o6dUHQG0ohI4TzRdI99/WH//75n9Grevu9eTzlOpGcfCeweIHe2QDVts3ZO8QdI7pjF6/95I7K2XU6lALH6V+pLZMoVCgzXtt9I7g6tvYF0+Pflp7AgN/vFGboxDUPgidYzprfN+/hT8AoPW7rfHahtfQcnhLZT7dKrih+ZDmGHlhJPqt7aderanh4vzKqlcQ+nIonol+xqC8KRQKDD88HO9dfE/jkPrV21YvfmRUqqz+zfwx7u449P6jeLRZnxAf+IT4iGpo7FlN8/xEmvLR6JVGGtOWruZtO1o9+OwxpwfavNcGHt4eCO0diue+fg4DYwei52898XnR55iQMQGNXta87c7fdIbCSYFuM7qh42fFJ3JXT1etUyH4hvlqHfDOs6qnWk8vVc0GNlM2RNakTmQdvJXwlnqtmImDzpkyZcH4e+PLdOkftGMQmr/RHK+sfEX0LK5VG1bVn0hFSfus9y89CYhqdaylfGzZ6m3tjzV0KXm0C0A5QrRqLYOxVH/PhtTiGar0DUIJbY+mtaU3lSAIqBtZF8/+37NGra86Gnfj16RpixPcMRiDdw3Wn1AFa0bsgHeQN575+Bl4VtE/+Z0cKvhXUJ68mw007BmrlOp3r4+h+4eWuRj5hPig9vO1MXjXYLy+9XV0m9ENDV5qUGZgNwCo2qAqQnuHqp3YNN0VNe7bGP3W9NN696aJwkmhcZ9A8SBugOYD1bOKJ5oOaIqPsz7GB1c/wAdXP0DlOpovuJoEtNR8oi9dU9CobyO8tOAlg4K//01Svztr9eaTC5PCSYEOn3RA7Yji7tEKhULrifvtk29r3YfqCMBAcbDQckRLvH3ybUT+WLYmrV53ze0bVD0zQXPw2HVGVwzYNgAV/CqoPRrrv013+xVdXbvF+DTvU7SfoF4bVNJg+LlJz6Hlmy0x9vZYVAysCCdnJzTu11gZqBgyErGTqxNqPVsLvRb3wvDDumsqu/zYBe+ee1fZPqtyXc3d+Lv9ZFzPuxptnzxaKtlecMdg/G+y+Dv+sP5PaknajHrS+F7KObQa99V/4VY959R4ugaav9Ec4R+Fa7xZeSb6GbUAT9XgPYPR+dvOZRqyS0H1uwvuFIxhh4ap9Rrs/1f/Mr0ISzOk3Ziq17e+Xra2kjUjZAmDdw/GhAcT1E5gphIbSZe+UL9/6X0M2D4AHj4eqNetnkEnqooBFdFiWAu0fqe1WvWmuZRcUHyCfbSmERP4qOr0ZSc8+2nZu6nS2+u7qi/cK7ob1k5FosFv/Zr6aX3Pv7k/+q5+8min/fj2eHHui1ofm5XUHulS+q61wUsNMOHBBLXHjKrtdpxdncu0aXAtX7yN17e+Xqa6u+nApmr/G/rbdXZ1RueYzhq/pw6fdsCLc17U2sB86P6heG7SczqDkpI2D80GNtPbaLvpgKaoFlrNoDwbEgDqpPJVPhut+46/RngNvJWg3rhY9fGu6g2QahukEgO2D8Ab+94Qlb2oTVFwdtPebmfEsRFo3K8xXlv/ZPwchUKBngt6ossPXdB5cucyAx5W8K+AynUra3zcHtwxWGvAbAhdvatUa0bcvdxRo20NtHn/SQBXt2tdjDw3Ei/Oe1HrNmo+86Q9TONXDatd8azqqXacyFkz4qI/CdkLhZPC5AaepR8BiOXf3B8NezXEhfUXAEBrbYQ+L81/SX8iE72+5XUcn3scXacXNzStGFARQw8MFdVlUx+38m7436T/ITcrF0d+OqJcrq0LraGNZi1BtUul2kWh1LVm4I6BGhvwltDWzqHJ603K/F49q3ji9a2vw8XDBU4uTihXuRz8W/gjLT4NADD62mjcvXAXQe2DcG71ObV1e8zpgVN/nNK4r6hNUVj+4nKteVQoFDqDM228a3qjw6cdULleZfz52p9at20oMUHvq+texb3Eezg25xjcK7pjf8x+vevouhj5N/dHWkKaxvc6T+4M/2b+yL6VrVzm5OKEz/I/w8O7D4vb+/xHU01cnS51UJBboDd/avR8bIGtAvXOr6WtEfCA7QOwuPNiJO1KAlA8RpCpdAVOqmUpOcZbvdkKzm7Oao8jWw5viRbDWmDDkA04ufgkgOKalNbvqLdf01Yuffs2uQOACVgzQgbp+EVH+AT76L1DMkTf1X0R+nKoLA1jxaj3Qj28tv41tYawQe2CDOoCK1a3Gd3UHs14VvHEK6teQcXqFTFo15PGeNqqatVmgLZQvKJQKND63dao06WO1kaJIy+MVHuko3rhjc6OxiurXtFaa6JtHIt63eoh5Lkn7WSG7BmCwKcCETElAp5VPVHzmZoaL/CllynvfhXQ2CNDSk1ebWJUF07/5sVtqV5Z+QpGnh8JFw/t94/VmqjXmDi7OsO3iS9e+PkFdJ6sua1WaaoXTDENwDUFMQqFAk4uTspApOeinqjetrrWEZld3F3UGqrro+k7Lle5HEYcG2HwNoDiRvoat69Sg6PtcaoupXtb6fqNqZalJBhxcnZCy2EtUaVelTJpu0ztgrDXwzBo5yAM3j24zLFSrmrZ4L9Bzwb4vOjzMvtU3XfVBuLaLkmJNSNkkE4TO6HTxE5l3zDiwufk4oR+a/qZnCd717hv4zLPxN3Ku2FCxgTcPHITS7osUS73CvJCrQ614FLORfmowhK6/9Jd5/u6Tm5uFdy0PvNv8loTg2sM3L3cMeJI2QtQyxEtkbAw4cmCUpur90I9DNo1qMxjj+ptq+PpMU+XqcmoGFjRoPxo411L8xgO7ca307rO8CPD8fDuQ1QM0L7vEcdG4OzKs+jwmfYGwABQO6I2ru64iir1q8Cjkgfqda+HPZ/vUUujWlMpphbGkEH8mg9ujuaDm+tMoxoAVK5bGfcv39ee1qns72PowaGiL6hiby7EPN4rzH3Slb5OZB3136O27RtQ++lZ1RN9lvZRX6jycWhskC6YNh6KuTEYIbIxHt4eZe6QFQoFBu8ZrDzZVKpTCQ+uPNDae8WcTDnhvRX/FhIWJei9sBoiKDwIr295Hds/3I7nv3++TDW5QqFQq2EpEdI5BI37NsbFTRfV5nCq2b4mIr6LkOzuMXJ6JJoPaV7msUXD3g1xYd0FNHmtCZxdnXUGIkDx44jAVvob67684mUkLEpA0/5NlbUVpYMRAMUXNcGwMTlGHBuBh3ceolLtSnrTll5vXut5ZZarjs3S+NXG2PfNPuX//f7sh1Uvr1L+r9r2beztsXh456Hp340hP10Db8DcKrghLyfvyaYNPC501X4ZSlO7HG06ftERq/qsMmoUYCkxGCGT2M0EUTZGUxW66sluYOxAHJp2qMx4KbLTc470b+6vbKMjhXov1FMbqdSzmice3nmoc53gTsFQOCnQZ0mfMu+ZNM+SyqHSblw7tB3VVuMFqvfi3rgSewV1I00bB6U0zyqeaPeR9lqYEu9ffB8XNlzAU+9onzqgROkgyNDzQWCrQLh6uiL/Yb7a8ir1quCd0+/As5onjvx8RO290D6hmChMRNrJNPx7+1+1YKR8tfKiRqk2iAFFKe9bHm4V3NQC3T5L++DvsX/j1XWv4renfwOgp73If577+jnkpOVI8hhYU61Rme/mvyShvUPxUdpHss9NxWCETCJlzxwyXNUGVdF9dndU8Kug8f1KIZWM6t5Z+m5Oan2W9MGSyCVax5sxt3Zj22HHhB3Krs2qxtwYg3uJ9ywyPPfz32kfqdStghtCe5s2n5UpKtetrDFoCXomqLgBq5lr+kt6d2gLbPyb+Ztt36o95rTtv837bbAzeifqRNZB1KYoKJwUakFl2OthaBKl/pjRkCCtwyem1waWEFs7qe08YkkMRsgog3cPxpW/r2gdhZTEK7kIahsIrbTWb1nnZ69rYKnA1oEYd3ecbM+u241th5rP1FQOsqfKq7qXxiHOpWLttYj6espFfBsB7yBvNOwt4SivVmLgjoFIPZ5qUEPmduPaoVaHWvBv4a8cg6g02dtmaNq9df/8GIyQcYI7BSO4U7Dc2bArXtW9MDZ9rEXGTjGn2s/XRujLocqeIKXJeaJWOCksNkeUrek6TffjMbfybqJmwNbLitpS1u5cu8xAfto4OTuJ/w0JUCtvxcCKat2gpaDvuNI3aJrcGIwQWRG5n9vWe6Eezq46q3OGYH2cnNlbivSTvfbAgsr7llcb0NE3zFfyYESVapuRYXHDcO7Pc2adl0cKDEaISKnH3B6o/nR1g4bZJpGssJp8QsYEbBu1rcxw9xahLxaR+/OSYP+Ddw/Grk924YVfX0C10Gqo8XQNBLYJxL0L90zfuA6qgV6Np2uYfQZpKTAYISIlD28PhI+xsh44dsIa24x4eHvINrmnVw0v3D1/V5Z9W0pwp2AMPTBU+f+wuGEAgFlhs8y6X0Me9VpbzRRHYCUiIunpuda9uu5V1I6orXWCN7mDt5LpDswxMd7tM7cl3yZQXLP5/PfP65xHy1qxZoSIiCShOmqri7vuy0vVBlUxMHagubNktFZvtYJPiA+qP6V78kJjVKlfBfcuSv+optWIVvoTWSkGI0RElmB9T2kk5+7ljte3vA4nFyeTRxINiwrDgW8PlJl92VKcnJ1Qr5uJMx9r4VbRzSzbFcW6ntIwGCEisgS5HztYiuqIt6bwa+qHD29+CM+qho27Y0usrb2GNWCbESIiskoVAysaNJS6rZEzMHWrUFwrY8h8RpbEmhEiIiIHMfb2WBQ8KkC5yuXkzooaBiNERJbgGE9pyMq5lnOFazntUzbIxaTHNN9++y0UCgVGjx6tNc2iRYugUCjUXh4eHlrTExERkWMxumbk6NGjmDNnDpo2bao3rZeXFxITE5X/s/EOETma9hPa4/Sy02g2uJncWSG5sZasDKOCkZycHPTv3x/z5s3D119/rTe9QqGAv7/5pn0mIrJ2fk39EJ0TrXNWYyJHZdRjmpEjR6J79+6IiIgwKH1OTg5q1aqFoKAg9OzZE2fPntWZPjc3F1lZWWovIiJb51bejTXDRBqIDkZWrFiBEydOICYmxqD0DRo0wIIFC7BhwwYsWbIERUVFaNeuHW7cuKF1nZiYGHh7eytfQUGc8puIiOxDSEQIAMC1PGvJSigEER2eU1JS0Lp1a8TGxirbinTq1AnNmzfH9OnTDdpGfn4+QkNDERUVhUmTJmlMk5ubi9zcXOX/WVlZCAoKQmZmJry8pJ8ngIiIyFLyH+UjYVEC6r1QDz61fOTOjlllZWXB29tb7/VbVDCyfv169O7dG87OTwahKSwshEKhgJOTE3Jzc9Xe06Zv375wcXHB8uXLDdqvoYUhIiIi62Ho9VtUA9bOnTvj9OnTasveeOMNNGzYEBMmTDAoECksLMTp06fxwgsviNk1ERER2SlRwUjFihXRpEkTtWXly5dHlSpVlMsHDRqE6tWrK9uUfPXVV3j66adRt25dZGRk4Pvvv8f169cxfPhwiYpAREREtkzyEViTk5Ph5PSkXeyDBw8wYsQIpKWloVKlSmjVqhUOHjyIRo0aSb1rIiIiskGi2ozIhW1GiIiIbI+h12/O2ktERESyYjBCREREsmIwQkRERLJiMEJERESyYjBCREREsmIwQkRERLJiMEJERESyYjBCREREsmIwQkRERLKSfDh4cygZJDYrK0vmnBAREZGhSq7b+gZ7t4lgJDs7GwAQFBQkc06IiIhIrOzsbHh7e2t93ybmpikqKsKtW7dQsWJFKBQKybablZWFoKAgpKSkOMScNyyv/XO0MrO89o3ltX2CICA7OxuBgYFqk+iWZhM1I05OTqhRo4bZtu/l5WU3X7whWF7752hlZnntG8tr23TViJRgA1YiIiKSFYMRIiIikpVDByPu7u6YOHEi3N3d5c6KRbC89s/Ryszy2jeW13HYRANWIiIisl8OXTNCRERE8mMwQkRERLJiMEJERESyYjBCREREsnLoYOSXX35BcHAwPDw80LZtWxw5ckTuLOkVExODp556ChUrVoSvry969eqFxMREtTSPHz/GyJEjUaVKFVSoUAEvv/wy0tPT1dIkJyeje/fu8PT0hK+vL8aNG4eCggK1NHv27EHLli3h7u6OunXrYtGiReYunl7ffvstFAoFRo8erVxmb+W9efMmBgwYgCpVqqBcuXIICwvDsWPHlO8LgoDPP/8cAQEBKFeuHCIiInDp0iW1bdy/fx/9+/eHl5cXfHx8MGzYMOTk5KilOXXqFJ599ll4eHggKCgI3333nUXKp6qwsBCfffYZQkJCUK5cOdSpUweTJk1Sm8fClsv7zz//4MUXX0RgYCAUCgXWr1+v9r4ly7Z69Wo0bNgQHh4eCAsLw9atWyUvL6C7zPn5+ZgwYQLCwsJQvnx5BAYGYtCgQbh165bNllnfd6zq7bffhkKhwPTp09WW21J5zUZwUCtWrBDc3NyEBQsWCGfPnhVGjBgh+Pj4COnp6XJnTafIyEhh4cKFwpkzZ4SEhAThhRdeEGrWrCnk5OQo07z99ttCUFCQsHPnTuHYsWPC008/LbRr1075fkFBgdCkSRMhIiJCiI+PF7Zu3SpUrVpViI6OVqa5evWq4OnpKXz44YfCuXPnhJ9//llwdnYWtm3bZtHyqjpy5IgQHBwsNG3aVPjggw+Uy+2pvPfv3xdq1aolDBkyRDh8+LBw9epVYfv27cLly5eVab799lvB29tbWL9+vXDy5EnhpZdeEkJCQoRHjx4p03Tt2lVo1qyZcOjQIWHfvn1C3bp1haioKOX7mZmZgp+fn9C/f3/hzJkzwvLly4Vy5coJc+bMsWh5v/nmG6FKlSrC5s2bhaSkJGH16tVChQoVhBkzZthFebdu3Sp88sknwtq1awUAwrp169Tet1TZDhw4IDg7OwvfffedcO7cOeHTTz8VXF1dhdOnT1u0zBkZGUJERISwcuVK4cKFC0JcXJzQpk0boVWrVmrbsKUy6/uOS6xdu1Zo1qyZEBgYKEybNs1my2suDhuMtGnTRhg5cqTy/8LCQiEwMFCIiYmRMVfi3b59WwAg7N27VxCE4oPd1dVVWL16tTLN+fPnBQBCXFycIAjFB4+Tk5OQlpamTDNr1izBy8tLyM3NFQRBEMaPHy80btxYbV+vvvqqEBkZae4iaZSdnS3Uq1dPiI2NFTp27KgMRuytvBMmTBCeeeYZre8XFRUJ/v7+wvfff69clpGRIbi7uwvLly8XBEEQzp07JwAQjh49qkzz119/CQqFQrh586YgCILw66+/CpUqVVKWv2TfDRo0kLpIOnXv3l0YOnSo2rI+ffoI/fv3FwTBvspb+kJlybL169dP6N69u1p+2rZtK7z11luSlrE0XRfnEkeOHBEACNevXxcEwbbLrK28N27cEKpXry6cOXNGqFWrllowYsvllZJDPqbJy8vD8ePHERERoVzm5OSEiIgIxMXFyZgz8TIzMwEAlStXBgAcP34c+fn5amVr2LAhatasqSxbXFwcwsLC4Ofnp0wTGRmJrKwsnD17VplGdRslaeT6fEaOHInu3buXyZO9lXfjxo1o3bo1+vbtC19fX7Ro0QLz5s1Tvp+UlIS0tDS1vHp7e6Nt27Zq5fXx8UHr1q2VaSIiIuDk5ITDhw8r03To0AFubm7KNJGRkUhMTMSDBw/MXUyldu3aYefOnbh48SIA4OTJk9i/fz+6desGwP7Kq8qSZbOW37cmmZmZUCgU8PHxAWB/ZS4qKsLAgQMxbtw4NG7cuMz79lZeYzlkMHL37l0UFhaqXZwAwM/PD2lpaTLlSryioiKMHj0a7du3R5MmTQAAaWlpcHNzUx7YJVTLlpaWprHsJe/pSpOVlYVHjx6ZozharVixAidOnEBMTEyZ9+ytvFevXsWsWbNQr149bN++He+88w5GjRqF33//XS2/un67aWlp8PX1VXvfxcUFlStXFvWZWMLHH3+M1157DQ0bNoSrqytatGiB0aNHo3///mp5sZfyqrJk2bSlkft89/jxY0yYMAFRUVHKieHsrcxTpkyBi4sLRo0apfF9eyuvsWxi1l7SbOTIkThz5gz2798vd1bMJiUlBR988AFiY2Ph4eEhd3bMrqioCK1bt8bkyZMBAC1atMCZM2cwe/ZsDB48WObcSW/VqlVYunQpli1bhsaNGyMhIQGjR49GYGCgXZaXnsjPz0e/fv0gCAJmzZold3bM4vjx45gxYwZOnDgBhUIhd3asmkPWjFStWhXOzs5lelykp6fD399fplyJ895772Hz5s3YvXs3atSooVzu7++PvLw8ZGRkqKVXLZu/v7/Gspe8pyuNl5cXypUrJ3VxtDp+/Dhu376Nli1bwsXFBS4uLti7dy9++uknuLi4wM/Pz67KGxAQgEaNGqktCw0NRXJysjKfJXlTVbq8t2/fVnu/oKAA9+/fF/WZWMK4ceOUtSNhYWEYOHAgxowZo6wFs7fyqrJk2bSlkavsJYHI9evXERsbq6wVAeyrzPv27cPt27dRs2ZN5fnr+vXr+OijjxAcHKzMp72U1xQOGYy4ubmhVatW2Llzp3JZUVERdu7cifDwcBlzpp8gCHjvvfewbt067Nq1CyEhIWrvt2rVCq6urmplS0xMRHJysrJs4eHhOH36tNoBUHJCKLkQhoeHq22jJI2lP5/OnTvj9OnTSEhIUL5at26N/v37K/+2p/K2b9++TFftixcvolatWgCAkJAQ+Pv7q+U1KysLhw8fVitvRkYGjh8/rkyza9cuFBUVoW3btso0//zzD/Lz85VpYmNj0aBBA1SqVMls5Svt4cOHcHJSPw05OzujqKgIgP2VV5Uly2Ytv2/gSSBy6dIl7NixA1WqVFF7357KPHDgQJw6dUrt/BUYGIhx48Zh+/btynzaS3lNIncLWrmsWLFCcHd3FxYtWiScO3dOePPNNwUfHx+1HhfW6J133hG8vb2FPXv2CKmpqcrXw4cPlWnefvttoWbNmsKuXbuEY8eOCeHh4UJ4eLjy/ZKurl26dBESEhKEbdu2CdWqVdPY1XXcuHHC+fPnhV9++UX2rr0lVHvTCIJ9lffIkSOCi4uL8M033wiXLl0Sli5dKnh6egpLlixRpvn2228FHx8fYcOGDcKpU6eEnj17auwO2qJFC+Hw4cPC/v37hXr16ql1FczIyBD8/PyEgQMHCmfOnBFWrFgheHp6Wrxr7+DBg4Xq1asru/auXbtWqFq1qjB+/Hi7KG92drYQHx8vxMfHCwCEH3/8UYiPj1f2HLFU2Q4cOCC4uLgIP/zwg3D+/Hlh4sSJZuv2qavMeXl5wksvvSTUqFFDSEhIUDuHqfYUsaUy6/uOSyvdm8bWymsuDhuMCIIg/Pzzz0LNmjUFNzc3oU2bNsKhQ4fkzpJeADS+Fi5cqEzz6NEj4d133xUqVaokeHp6Cr179xZSU1PVtnPt2jWhW7duQrly5YSqVasKH330kZCfn6+WZvfu3ULz5s0FNzc3oXbt2mr7kFPpYMTeyrtp0yahSZMmgru7u9CwYUNh7ty5au8XFRUJn332meDn5ye4u7sLnTt3FhITE9XS3Lt3T4iKihIqVKggeHl5CW+88YaQnZ2tlubkyZPCM888I7i7uwvVq1cXvv32W7OXrbSsrCzhgw8+EGrWrCl4eHgItWvXFj755BO1C5Mtl3f37t0aj9fBgwdbvGyrVq0S6tevL7i5uQmNGzcWtmzZYvEyJyUlaT2H7d692ybLrO87Lk1TMGJL5TUXhSCoDHVIREREZGEO2WaEiIiIrAeDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKS1f8DCr8jt9wwnWEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tinyllm.plot_loss_curve(trainer.train_steps, trainer.train_losses, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moon isar hioiurowat ,wEthla evtimingde2o  St Bkes  in e ilerr, exid P bandool Ath bac  s hury d y  on\n",
      " f .a erine c  ovandandokus V Hkest2oc fenamcw he onaeragench th9or\n"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "import torch\n",
    "prompt = \"The moon is\"\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "prompt_ids = torch.tensor(prompt_ids, device=model_params.device).unsqueeze(0)\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = trainer.model.generate(prompt_ids, 100)\n",
    "    string = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
